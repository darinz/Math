{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistical Inference\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Pandas](https://img.shields.io/badge/Pandas-1.3+-blue.svg)](https://pandas.pydata.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.4+-orange.svg)](https://matplotlib.org/)\n[![Seaborn](https://img.shields.io/badge/Seaborn-0.11+-blue.svg)](https://seaborn.pydata.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-green.svg)](https://scipy.org/)\n[![Statsmodels](https://img.shields.io/badge/Statsmodels-0.13+-blue.svg)](https://www.statsmodels.org/)\n\nStatistical inference allows us to draw conclusions about populations based on sample data. This chapter covers hypothesis testing, confidence intervals, and p-values - essential tools for making data-driven decisions in AI/ML.\n\n## Table of Contents\n- [Hypothesis Testing Fundamentals](#hypothesis-testing-fundamentals)\n- [One-Sample Tests](#one-sample-tests)\n- [Two-Sample Tests](#two-sample-tests)\n- [Confidence Intervals](#confidence-intervals)\n- [P-Values and Significance](#p-values-and-significance)\n- [Multiple Testing](#multiple-testing)\n- [Practical Applications](#practical-applications)\n\n## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import ttest_1samp, ttest_ind, ttest_rel, chi2_contingency\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\nnp.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hypothesis Testing Fundamentals\n\n### Mathematical Foundation\n\n**Statistical Hypothesis Testing** is a formal procedure for making decisions about population parameters based on sample data. The process involves:\n\n1. **Formulating Hypotheses**: \n   - **Null Hypothesis (H₀)**: A statement about the population parameter that we assume to be true\n   - **Alternative Hypothesis (H₁)**: A statement that contradicts the null hypothesis\n\n2. **Test Statistic**: A function of the sample data that follows a known probability distribution under H₀\n\n3. **Decision Rule**: Based on the test statistic and significance level α, we either reject or fail to reject H₀\n\n**Mathematical Framework:**\nFor a population parameter θ, we test:\n- H₀: θ = θ₀ (null hypothesis)\n- H₁: θ ≠ θ₀ (two-sided alternative) or θ > θ₀, θ < θ₀ (one-sided alternatives)\n\nThe test statistic T is calculated from sample data and compared to critical values or used to compute p-values.\n\n### Null and Alternative Hypotheses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hypothesis_testing_example():\n    \"\"\"Example: Testing if a coin is fair\"\"\"\n    # Null hypothesis: p = 0.5 (fair coin)\n    # Alternative hypothesis: p ≠ 0.5 (biased coin)\n    \n    # Simulate coin flips\n    n_flips = 100\n    p_true = 0.6  # True probability (biased coin)\n    flips = np.random.binomial(1, p_true, n_flips)\n    \n    # Test statistic: number of heads\n    observed_heads = np.sum(flips)\n    observed_proportion = observed_heads / n_flips\n    \n    # Expected under null hypothesis\n    expected_heads = n_flips * 0.5\n    \n    # Z-test statistic\n    z_stat = (observed_heads - expected_heads) / np.sqrt(n_flips * 0.5 * 0.5)\n    \n    # P-value (two-tailed test)\n    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n    \n    return {\n        'observed_heads': observed_heads,\n        'observed_proportion': observed_proportion,\n        'expected_heads': expected_heads,\n        'z_statistic': z_stat,\n        'p_value': p_value\n    }\n\nresults = hypothesis_testing_example()\nprint(\"Coin Fairness Test\")\nfor key, value in results.items():\n    print(f\"{key}: {value:.4f}\")\n\n# Visualize the test\nplt.figure(figsize=(12, 4))\n\n# Observed vs expected\nplt.subplot(1, 3, 1)\ncategories = ['Heads', 'Tails']\nobserved = [results['observed_heads'], 100 - results['observed_heads']]\nexpected = [50, 50]\nx = np.arange(len(categories))\nwidth = 0.35\n\nplt.bar(x - width/2, observed, width, label='Observed', alpha=0.7)\nplt.bar(x + width/2, expected, width, label='Expected', alpha=0.7)\nplt.xlabel('Outcome')\nplt.ylabel('Count')\nplt.title('Observed vs Expected')\nplt.xticks(x, categories)\nplt.legend()\n\n# Z-distribution\nplt.subplot(1, 3, 2)\nx = np.linspace(-4, 4, 1000)\ny = stats.norm.pdf(x, 0, 1)\nplt.plot(x, y, 'b-', linewidth=2)\nplt.fill_between(x, y, where=(x > abs(results['z_statistic'])) | (x < -abs(results['z_statistic'])), \n                 alpha=0.3, color='red', label='Rejection region')\nplt.axvline(results['z_statistic'], color='red', linestyle='--', label=f'z = {results[\"z_statistic\"]:.2f}')\nplt.axvline(-results['z_statistic'], color='red', linestyle='--')\nplt.title('Z-Distribution')\nplt.xlabel('Z-score')\nplt.ylabel('Density')\nplt.legend()\n\n# P-value interpretation\nplt.subplot(1, 3, 3)\nalpha = 0.05\ndecision = \"Reject H₀\" if results['p_value'] < alpha else \"Fail to reject H₀\"\nplt.text(0.5, 0.5, f\"P-value: {results['p_value']:.4f}\\nα = {alpha}\\nDecision: {decision}\", \n         ha='center', va='center', transform=plt.gca().transAxes, fontsize=12,\n         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\nplt.title('Test Decision')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Type I and Type II Errors\n\n**Mathematical Definition:**\n- **Type I Error (α)**: Rejecting H₀ when it's true\n- **Type II Error (β)**: Failing to reject H₀ when it's false\n- **Power (1-β)**: Probability of correctly rejecting H₀ when it's false\n\n**Error Trade-off:**\nAs α decreases, β increases, and vice versa. The relationship is:\n$$\\beta = \\Phi(z_{\\alpha/2} - \\frac{\\delta}{\\sigma/\\sqrt{n}}) + \\Phi(z_{\\alpha/2} + \\frac{\\delta}{\\sigma/\\sqrt{n}})$$\n\nWhere δ is the effect size, σ is the standard deviation, and n is the sample size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def error_types_demonstration():\n    \"\"\"Demonstrate Type I and Type II errors\"\"\"\n    # Simulate multiple hypothesis tests\n    n_tests = 1000\n    alpha = 0.05  # Significance level\n    \n    # True null hypotheses (no effect)\n    true_null = 800\n    # True alternative hypotheses (effect exists)\n    true_alternative = 200\n    \n    # Type I errors (false positives)\n    type_i_errors = np.random.binomial(true_null, alpha)\n    \n    # Type II errors (false negatives) - assume 80% power\n    power = 0.8\n    type_ii_errors = np.random.binomial(true_alternative, 1 - power)\n    \n    # Calculate rates\n    type_i_rate = type_i_errors / true_null\n    type_ii_rate = type_ii_errors / true_alternative\n    \n    return {\n        'type_i_errors': type_i_errors,\n        'type_ii_errors': type_ii_errors,\n        'type_i_rate': type_i_rate,\n        'type_ii_rate': type_ii_rate,\n        'power': power\n    }\n\nerror_results = error_types_demonstration()\nprint(\"Error Types Demonstration\")\nfor key, value in error_results.items():\n    print(f\"{key}: {value:.4f}\")\n\n# Visualize error matrix\nplt.figure(figsize=(10, 4))\n\n# Error matrix\nplt.subplot(1, 2, 1)\nerror_matrix = np.array([\n    [800 - error_results['type_i_errors'], error_results['type_i_errors']],\n    [error_results['type_ii_errors'], 200 - error_results['type_ii_errors']]\n])\n\nsns.heatmap(error_matrix, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Fail to Reject', 'Reject'],\n            yticklabels=['H₀ True', 'H₀ False'])\nplt.title('Error Matrix')\nplt.ylabel('Truth')\nplt.xlabel('Decision')\n\n# Error rates over different alpha levels\nplt.subplot(1, 2, 2)\nalpha_levels = np.linspace(0.01, 0.2, 20)\ntype_i_rates = alpha_levels\ntype_ii_rates = 1 - stats.norm.cdf(stats.norm.ppf(1 - alpha_levels) - 0.5)  # Simplified\n\nplt.plot(alpha_levels, type_i_rates, 'r-', label='Type I Error Rate', linewidth=2)\nplt.plot(alpha_levels, type_ii_rates, 'b-', label='Type II Error Rate', linewidth=2)\nplt.axvline(0.05, color='g', linestyle='--', label='α = 0.05')\nplt.xlabel('Significance Level (α)')\nplt.ylabel('Error Rate')\nplt.title('Error Rates vs Significance Level')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One-Sample Tests\n\n### Z-Test for Population Mean\n\n**Mathematical Foundation:**\nWhen we know the population standard deviation σ, we use the Z-test:\n\n$$Z = \\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}}$$\n\nWhere:\n- $\\bar{X}$ is the sample mean\n- $\\mu_0$ is the hypothesized population mean\n- $\\sigma$ is the population standard deviation\n- $n$ is the sample size\n\n**Assumptions:**\n1. Data is normally distributed (or n > 30 by CLT)\n2. Population standard deviation is known\n3. Independent observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def z_test_manual(data, mu0, sigma, alpha=0.05):\n    \"\"\"\n    Manual implementation of Z-test for population mean\n    \n    Mathematical steps:\n    1. Calculate sample mean: x̄ = (1/n) * Σxᵢ\n    2. Calculate standard error: SE = σ/√n\n    3. Calculate Z-statistic: Z = (x̄ - μ₀) / SE\n    4. Calculate p-value: P(|Z| > |z_observed|)\n    5. Make decision based on α\n    \n    Parameters:\n    data: array-like, sample data\n    mu0: float, hypothesized population mean\n    sigma: float, known population standard deviation\n    alpha: float, significance level\n    \n    Returns:\n    dict: test results\n    \"\"\"\n    n = len(data)\n    sample_mean = np.mean(data)\n    \n    # Step 1: Calculate standard error\n    standard_error = sigma / np.sqrt(n)\n    \n    # Step 2: Calculate Z-statistic\n    z_statistic = (sample_mean - mu0) / standard_error\n    \n    # Step 3: Calculate p-value (two-tailed)\n    p_value = 2 * (1 - stats.norm.cdf(abs(z_statistic)))\n    \n    # Step 4: Calculate critical values\n    z_critical = stats.norm.ppf(1 - alpha/2)\n    \n    # Step 5: Make decision\n    reject_null = abs(z_statistic) > z_critical\n    \n    # Step 6: Calculate confidence interval\n    margin_of_error = z_critical * standard_error\n    ci_lower = sample_mean - margin_of_error\n    ci_upper = sample_mean + margin_of_error\n    \n    return {\n        'sample_mean': sample_mean,\n        'hypothesized_mean': mu0,\n        'standard_error': standard_error,\n        'z_statistic': z_statistic,\n        'p_value': p_value,\n        'z_critical': z_critical,\n        'reject_null': reject_null,\n        'confidence_interval': (ci_lower, ci_upper),\n        'effect_size': abs(sample_mean - mu0) / sigma\n    }\n\n# Example: Test if sample mean differs from hypothesized mean\nnp.random.seed(42)\nsample_data = np.random.normal(105, 15, 50)  # True mean = 105, σ = 15\nmu0 = 100  # Hypothesized mean\nsigma = 15  # Known population standard deviation\n\nz_results = z_test_manual(sample_data, mu0, sigma)\nprint(\"Z-Test Results:\")\nfor key, value in z_results.items():\n    if isinstance(value, tuple):\n        print(f\"{key}: {value[0]:.3f} to {value[1]:.3f}\")\n    else:\n        print(f\"{key}: {value:.3f}\")\n\n# Visualize the test\nplt.figure(figsize=(12, 4))\n\n# Sample distribution\nplt.subplot(1, 3, 1)\nplt.hist(sample_data, bins=15, alpha=0.7, density=True, color='skyblue', edgecolor='black')\nx = np.linspace(min(sample_data), max(sample_data), 100)\nplt.plot(x, stats.norm.pdf(x, np.mean(sample_data), np.std(sample_data, ddof=1)), \n         'r-', linewidth=2, label='Sample distribution')\nplt.axvline(z_results['sample_mean'], color='red', linestyle='--', \n            label=f'Sample mean: {z_results[\"sample_mean\"]:.2f}')\nplt.axvline(mu0, color='green', linestyle='--', \n            label=f'Hypothesized mean: {mu0}')\nplt.xlabel('Values')\nplt.ylabel('Density')\nplt.title('Sample Distribution')\nplt.legend()\n\n# Z-distribution\nplt.subplot(1, 3, 2)\nz_range = np.linspace(-4, 4, 1000)\nz_pdf = stats.norm.pdf(z_range, 0, 1)\nplt.plot(z_range, z_pdf, 'b-', linewidth=2)\nplt.fill_between(z_range, z_pdf, where=(z_range > z_results['z_critical']) | (z_range < -z_results['z_critical']), \n                 alpha=0.3, color='red', label='Rejection region')\nplt.axvline(z_results['z_statistic'], color='red', linestyle='--', \n            label=f'Z = {z_results[\"z_statistic\"]:.2f}')\nplt.axvline(-z_results['z_statistic'], color='red', linestyle='--')\nplt.xlabel('Z-score')\nplt.ylabel('Density')\nplt.title('Z-Distribution')\nplt.legend()\n\n# Confidence interval\nplt.subplot(1, 3, 3)\nci_lower, ci_upper = z_results['confidence_interval']\nplt.errorbar([1], [z_results['sample_mean']], \n             yerr=[[z_results['sample_mean'] - ci_lower], [ci_upper - z_results['sample_mean']]], \n             fmt='o', capsize=5, capthick=2, markersize=8, label='95% CI')\nplt.axhline(mu0, color='green', linestyle='--', label=f'H₀: μ = {mu0}')\nplt.xlim(0.5, 1.5)\nplt.ylabel('Mean')\nplt.title('Confidence Interval')\nplt.legend()\nplt.xticks([])\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T-Test for Population Mean\n\n**Mathematical Foundation:**\nWhen population standard deviation is unknown, we use the t-test:\n\n$$t = \\frac{\\bar{X} - \\mu_0}{s/\\sqrt{n}}$$\n\nWhere:\n- $s$ is the sample standard deviation\n- Degrees of freedom = n - 1\n\n**Key Differences from Z-test:**\n1. Uses sample standard deviation instead of population standard deviation\n2. Follows t-distribution instead of normal distribution\n3. Degrees of freedom affect the shape of the distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def t_test_manual(data, mu0, alpha=0.05):\n    \"\"\"\n    Manual implementation of t-test for population mean\n    \n    Mathematical steps:\n    1. Calculate sample mean: x̄ = (1/n) * Σxᵢ\n    2. Calculate sample standard deviation: s = √[Σ(xᵢ - x̄)² / (n-1)]\n    3. Calculate standard error: SE = s/√n\n    4. Calculate t-statistic: t = (x̄ - μ₀) / SE\n    5. Calculate p-value using t-distribution with df = n-1\n    6. Make decision based on α\n    \n    Parameters:\n    data: array-like, sample data\n    mu0: float, hypothesized population mean\n    alpha: float, significance level\n    \n    Returns:\n    dict: test results\n    \"\"\"\n    n = len(data)\n    df = n - 1  # degrees of freedom\n    \n    # Step 1: Calculate sample statistics\n    sample_mean = np.mean(data)\n    sample_std = np.std(data, ddof=1)  # ddof=1 for sample standard deviation\n    \n    # Step 2: Calculate standard error\n    standard_error = sample_std / np.sqrt(n)\n    \n    # Step 3: Calculate t-statistic\n    t_statistic = (sample_mean - mu0) / standard_error\n    \n    # Step 4: Calculate p-value (two-tailed)\n    p_value = 2 * (1 - stats.t.cdf(abs(t_statistic), df))\n    \n    # Step 5: Calculate critical values\n    t_critical = stats.t.ppf(1 - alpha/2, df)\n    \n    # Step 6: Make decision\n    reject_null = abs(t_statistic) > t_critical\n    \n    # Step 7: Calculate confidence interval\n    margin_of_error = t_critical * standard_error\n    ci_lower = sample_mean - margin_of_error\n    ci_upper = sample_mean + margin_of_error\n    \n    # Step 8: Calculate effect size (Cohen's d)\n    cohens_d = abs(sample_mean - mu0) / sample_std\n    \n    return {\n        'sample_mean': sample_mean,\n        'sample_std': sample_std,\n        'hypothesized_mean': mu0,\n        'standard_error': standard_error,\n        't_statistic': t_statistic,\n        'degrees_of_freedom': df,\n        'p_value': p_value,\n        't_critical': t_critical,\n        'reject_null': reject_null,\n        'confidence_interval': (ci_lower, ci_upper),\n        'cohens_d': cohens_d\n    }\n\n# Example: Test if sample mean differs from hypothesized mean\nnp.random.seed(42)\nsample_data = np.random.normal(105, 15, 30)  # True mean = 105, unknown σ\nmu0 = 100  # Hypothesized mean\n\nt_results = t_test_manual(sample_data, mu0)\nprint(\"T-Test Results:\")\nfor key, value in t_results.items():\n    if isinstance(value, tuple):\n        print(f\"{key}: {value[0]:.3f} to {value[1]:.3f}\")\n    else:\n        print(f\"{key}: {value:.3f}\")\n\n# Compare with scipy implementation\nscipy_t, scipy_p = ttest_1samp(sample_data, mu0)\nprint(f\"\\nSciPy t-test: t = {scipy_t:.3f}, p = {scipy_p:.3f}\")\n\n# Visualize t-distribution vs normal distribution\nplt.figure(figsize=(12, 4))\n\n# Sample distribution\nplt.subplot(1, 3, 1)\nplt.hist(sample_data, bins=10, alpha=0.7, density=True, color='skyblue', edgecolor='black')\nx = np.linspace(min(sample_data), max(sample_data), 100)\nplt.plot(x, stats.norm.pdf(x, np.mean(sample_data), np.std(sample_data, ddof=1)), \n         'r-', linewidth=2, label='Sample distribution')\nplt.axvline(t_results['sample_mean'], color='red', linestyle='--', \n            label=f'Sample mean: {t_results[\"sample_mean\"]:.2f}')\nplt.axvline(mu0, color='green', linestyle='--', \n            label=f'Hypothesized mean: {mu0}')\nplt.xlabel('Values')\nplt.ylabel('Density')\nplt.title('Sample Distribution')\nplt.legend()\n\n# T-distribution vs Normal\nplt.subplot(1, 3, 2)\nx = np.linspace(-4, 4, 1000)\nt_pdf = stats.t.pdf(x, t_results['degrees_of_freedom'])\nnormal_pdf = stats.norm.pdf(x, 0, 1)\nplt.plot(x, t_pdf, 'b-', linewidth=2, label=f't-distribution (df={t_results[\"degrees_of_freedom\"]})')\nplt.plot(x, normal_pdf, 'r--', linewidth=2, label='Normal distribution')\nplt.fill_between(x, t_pdf, where=(x > t_results['t_critical']) | (x < -t_results['t_critical']), \n                 alpha=0.3, color='red', label='Rejection region')\nplt.axvline(t_results['t_statistic'], color='red', linestyle='--', \n            label=f't = {t_results[\"t_statistic\"]:.2f}')\nplt.xlabel('t-score')\nplt.ylabel('Density')\nplt.title('T-Distribution vs Normal')\nplt.legend()\n\n# Effect size interpretation\nplt.subplot(1, 3, 3)\neffect_sizes = [0.2, 0.5, 0.8, 1.2]\ninterpretations = ['Small', 'Medium', 'Large', 'Very Large']\ncolors = ['lightblue', 'skyblue', 'blue', 'darkblue']\n\nfor i, (effect, interpret, color) in enumerate(zip(effect_sizes, interpretations, colors)):\n    plt.bar(i, effect, color=color, alpha=0.7, label=interpret)\n\nplt.bar(4, t_results['cohens_d'], color='red', alpha=0.7, label='Observed')\nplt.axhline(0.2, color='gray', linestyle='--', alpha=0.5, label='Small threshold')\nplt.axhline(0.5, color='gray', linestyle='--', alpha=0.5, label='Medium threshold')\nplt.axhline(0.8, color='gray', linestyle='--', alpha=0.5, label='Large threshold')\n\nplt.xlabel('Effect Size Categories')\nplt.ylabel(\"Cohen's d\")\nplt.title(\"Effect Size (Cohen's d)\")\nplt.xticks(range(5), ['Small', 'Medium', 'Large', 'Very Large', 'Observed'])\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two-Sample Tests\n\n### Independent t-Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def independent_t_test_example():\n    \"\"\"Compare means of two independent groups\"\"\"\n    # Generate two groups\n    n1, n2 = 30, 25\n    mean1, mean2 = 100, 110\n    std1, std2 = 15, 18\n    \n    group1 = np.random.normal(mean1, std1, n1)\n    group2 = np.random.normal(mean2, std2, n2)\n    \n    # Perform independent t-test\n    t_stat, p_value = ttest_ind(group1, group2)\n    \n    # Calculate effect size (Cohen's d)\n    pooled_std = np.sqrt(((n1-1)*np.var(group1, ddof=1) + (n2-1)*np.var(group2, ddof=1)) / (n1+n2-2))\n    cohens_d = (np.mean(group1) - np.mean(group2)) / pooled_std\n    \n    return {\n        'group1_mean': np.mean(group1),\n        'group2_mean': np.mean(group2),\n        'group1_std': np.std(group1, ddof=1),\n        'group2_std': np.std(group2, ddof=1),\n        't_statistic': t_stat,\n        'p_value': p_value,\n        'cohens_d': cohens_d,\n        'group1': group1,\n        'group2': group2\n    }\n\nind_t_results = independent_t_test_example()\nprint(\"Independent t-Test\")\nfor key, value in ind_t_results.items():\n    if key not in ['group1', 'group2']:\n        print(f\"{key}: {value:.4f}\")\n\n# Visualize\nplt.figure(figsize=(15, 5))\n\n# Box plots\nplt.subplot(1, 3, 1)\nplt.boxplot([ind_t_results['group1'], ind_t_results['group2']], labels=['Group 1', 'Group 2'])\nplt.ylabel('Value')\nplt.title('Group Comparison')\n\n# Histograms\nplt.subplot(1, 3, 2)\nplt.hist(ind_t_results['group1'], alpha=0.7, label='Group 1', bins=15)\nplt.hist(ind_t_results['group2'], alpha=0.7, label='Group 2', bins=15)\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Group Distributions')\nplt.legend()\n\n# Effect size interpretation\nplt.subplot(1, 3, 3)\neffect_sizes = [0.2, 0.5, 0.8]\ninterpretations = ['Small', 'Medium', 'Large']\ncolors = ['lightblue', 'orange', 'red']\n\nfor i, (size, interpretation, color) in enumerate(zip(effect_sizes, interpretations, colors)):\n    plt.axvline(size, color=color, linestyle='--', alpha=0.7, label=f'{interpretation} effect')\n    plt.axvline(-size, color=color, linestyle='--', alpha=0.7)\n\nplt.axvline(ind_t_results['cohens_d'], color='black', linewidth=2, label=f\"Cohen's d = {ind_t_results['cohens_d']:.2f}\")\nplt.xlabel(\"Cohen's d\")\nplt.ylabel('Density')\nplt.title(\"Effect Size (Cohen's d)\")\nplt.legend()\nplt.xlim(-2, 2)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paired t-Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def paired_t_test_example():\n    \"\"\"Test for difference in paired observations\"\"\"\n    # Simulate before/after measurements\n    n_pairs = 25\n    true_improvement = 5\n    measurement_error = 3\n    \n    # Generate paired data\n    before = np.random.normal(100, 15, n_pairs)\n    after = before + np.random.normal(true_improvement, measurement_error, n_pairs)\n    \n    # Calculate differences\n    differences = after - before\n    \n    # Perform paired t-test\n    t_stat, p_value = ttest_rel(before, after)\n    \n    # Alternative: test if differences are different from zero\n    t_stat_diff, p_value_diff = ttest_1samp(differences, 0)\n    \n    return {\n        'before_mean': np.mean(before),\n        'after_mean': np.mean(after),\n        'difference_mean': np.mean(differences),\n        't_statistic': t_stat,\n        'p_value': p_value,\n        't_statistic_diff': t_stat_diff,\n        'p_value_diff': p_value_diff,\n        'before': before,\n        'after': after,\n        'differences': differences\n    }\n\npaired_results = paired_t_test_example()\nprint(\"Paired t-Test\")\nfor key, value in paired_results.items():\n    if key not in ['before', 'after', 'differences']:\n        print(f\"{key}: {value:.4f}\")\n\n# Visualize\nplt.figure(figsize=(15, 5))\n\n# Before vs After\nplt.subplot(1, 3, 1)\nplt.scatter(paired_results['before'], paired_results['after'], alpha=0.7)\nplt.plot([80, 120], [80, 120], 'r--', alpha=0.7, label='No change')\nplt.xlabel('Before')\nplt.ylabel('After')\nplt.title('Before vs After')\nplt.legend()\n\n# Differences distribution\nplt.subplot(1, 3, 2)\nplt.hist(paired_results['differences'], bins=15, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.axvline(0, color='red', linestyle='--', linewidth=2, label='No difference')\nplt.axvline(np.mean(paired_results['differences']), color='blue', linestyle='--', linewidth=2, label='Mean difference')\nplt.xlabel('Difference (After - Before)')\nplt.ylabel('Frequency')\nplt.title('Distribution of Differences')\nplt.legend()\n\n# Individual changes\nplt.subplot(1, 3, 3)\nx = np.arange(len(paired_results['before']))\nplt.plot([x, x], [paired_results['before'], paired_results['after']], 'b-', alpha=0.5)\nplt.scatter(x, paired_results['before'], color='red', label='Before', alpha=0.7)\nplt.scatter(x, paired_results['after'], color='green', label='After', alpha=0.7)\nplt.xlabel('Subject')\nplt.ylabel('Value')\nplt.title('Individual Changes')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confidence Intervals\n\n### Confidence Interval Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def confidence_interval_demonstration():\n    \"\"\"Demonstrate confidence interval construction\"\"\"\n    # Generate population\n    population = np.random.normal(100, 15, 10000)\n    true_mean = np.mean(population)\n    \n    # Take multiple samples\n    n_samples = 50\n    sample_size = 30\n    confidence_level = 0.95\n    \n    sample_means = []\n    confidence_intervals = []\n    \n    for _ in range(n_samples):\n        sample = np.random.choice(population, size=sample_size, replace=False)\n        sample_mean = np.mean(sample)\n        sample_std = np.std(sample, ddof=1)\n        \n        # Calculate confidence interval\n        t_value = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1)\n        margin_of_error = t_value * sample_std / np.sqrt(sample_size)\n        \n        sample_means.append(sample_mean)\n        confidence_intervals.append((sample_mean - margin_of_error, sample_mean + margin_of_error))\n    \n    # Count intervals containing true mean\n    intervals_containing_true = sum(1 for ci in confidence_intervals \n                                  if ci[0] <= true_mean <= ci[1])\n    \n    return sample_means, confidence_intervals, true_mean, intervals_containing_true\n\nmeans, intervals, true_mean, count = confidence_interval_demonstration()\n\nprint(f\"Confidence Interval Demonstration\")\nprint(f\"True population mean: {true_mean:.2f}\")\nprint(f\"Intervals containing true mean: {count}/{len(intervals)} ({count/len(intervals)*100:.1f}%)\")\n\n# Visualize confidence intervals\nplt.figure(figsize=(12, 8))\nx_positions = np.arange(len(intervals))\n\n# Plot confidence intervals\nfor i, (lower, upper) in enumerate(intervals):\n    if lower <= true_mean <= upper:\n        plt.plot([i, i], [lower, upper], 'b-', alpha=0.7)\n    else:\n        plt.plot([i, i], [lower, upper], 'r-', alpha=0.7)\n\nplt.axhline(y=true_mean, color='g', linestyle='--', linewidth=2, label='True Mean')\nplt.xlabel('Sample Number')\nplt.ylabel('Value')\nplt.title('Confidence Intervals (95%)')\nplt.legend()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## P-Values and Significance\n\n### P-Value Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def p_value_interpretation():\n    \"\"\"Demonstrate p-value interpretation\"\"\"\n    # Simulate multiple hypothesis tests\n    n_tests = 1000\n    alpha = 0.05\n    \n    # Generate p-values under null hypothesis (uniform distribution)\n    p_values_null = np.random.uniform(0, 1, n_tests)\n    \n    # Generate p-values under alternative hypothesis\n    # (some will be small, some large)\n    p_values_alt = np.concatenate([\n        np.random.beta(0.5, 5, n_tests//2),  # Small p-values\n        np.random.uniform(0, 1, n_tests//2)   # Large p-values\n    ])\n    \n    # Mix null and alternative\n    p_values = np.concatenate([p_values_null[:800], p_values_alt[:200]])\n    \n    return p_values, alpha\n\np_vals, alpha = p_value_interpretation()\n\n# Visualize p-value distribution\nplt.figure(figsize=(15, 5))\n\n# P-value histogram\nplt.subplot(1, 3, 1)\nplt.hist(p_vals, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\nplt.axvline(alpha, color='red', linestyle='--', linewidth=2, label=f'α = {alpha}')\nplt.xlabel('P-value')\nplt.ylabel('Frequency')\nplt.title('P-Value Distribution')\nplt.legend()\n\n# P-value vs significance\nplt.subplot(1, 3, 2)\nsignificant = p_vals < alpha\nplt.scatter(range(len(p_vals)), p_vals, c=significant, cmap='RdYlBu', alpha=0.7)\nplt.axhline(alpha, color='red', linestyle='--', linewidth=2, label=f'α = {alpha}')\nplt.xlabel('Test Number')\nplt.ylabel('P-value')\nplt.title('P-Values vs Significance')\nplt.legend()\n\n# Significance rate\nplt.subplot(1, 3, 3)\nsignificance_rate = np.mean(significant)\nplt.bar(['Significant', 'Not Significant'], \n        [significance_rate, 1-significance_rate], \n        color=['red', 'blue'], alpha=0.7)\nplt.ylabel('Proportion')\nplt.title(f'Significance Rate: {significance_rate:.3f}')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"P-Value Analysis:\")\nprint(f\"Total tests: {len(p_vals)}\")\nprint(f\"Significant tests: {np.sum(significant)}\")\nprint(f\"Significance rate: {np.mean(significant):.3f}\")\nprint(f\"Expected under null: {alpha:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple Testing\n\n### Multiple Testing Correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multiple_testing_correction():\n    \"\"\"Demonstrate multiple testing corrections\"\"\"\n    # Simulate multiple hypothesis tests\n    n_tests = 1000\n    alpha = 0.05\n    \n    # Generate p-values (mostly null, some alternative)\n    p_values = np.concatenate([\n        np.random.uniform(0, 1, 900),  # Null hypotheses\n        np.random.beta(0.5, 5, 100)    # Alternative hypotheses\n    ])\n    \n    # No correction\n    significant_uncorrected = p_values < alpha\n    \n    # Bonferroni correction\n    alpha_bonferroni = alpha / n_tests\n    significant_bonferroni = p_values < alpha_bonferroni\n    \n    # Benjamini-Hochberg (FDR) correction\n    sorted_indices = np.argsort(p_values)\n    sorted_p_values = p_values[sorted_indices]\n    \n    # Calculate critical values\n    critical_values = alpha * np.arange(1, n_tests + 1) / n_tests\n    \n    # Find largest k where p_k <= critical_k\n    bh_significant = np.zeros(n_tests, dtype=bool)\n    for i in range(n_tests):\n        if sorted_p_values[i] <= critical_values[i]:\n            bh_significant[sorted_indices[i]] = True\n    \n    return {\n        'p_values': p_values,\n        'significant_uncorrected': significant_uncorrected,\n        'significant_bonferroni': significant_bonferroni,\n        'significant_bh': bh_significant,\n        'alpha': alpha,\n        'alpha_bonferroni': alpha_bonferroni\n    }\n\nmt_results = multiple_testing_correction()\n\nprint(\"Multiple Testing Correction\")\nprint(f\"Uncorrected significant: {np.sum(mt_results['significant_uncorrected'])}\")\nprint(f\"Bonferroni significant: {np.sum(mt_results['significant_bonferroni'])}\")\nprint(f\"Benjamini-Hochberg significant: {np.sum(mt_results['significant_bh'])}\")\n\n# Visualize\nplt.figure(figsize=(15, 5))\n\n# P-value distribution\nplt.subplot(1, 3, 1)\nplt.hist(mt_results['p_values'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\nplt.axvline(mt_results['alpha'], color='red', linestyle='--', label=f'α = {mt_results[\"alpha\"]}')\nplt.axvline(mt_results['alpha_bonferroni'], color='orange', linestyle='--', label=f'Bonferroni α = {mt_results[\"alpha_bonferroni\"]:.6f}')\nplt.xlabel('P-value')\nplt.ylabel('Frequency')\nplt.title('P-Value Distribution')\nplt.legend()\n\n# Comparison of methods\nplt.subplot(1, 3, 2)\nmethods = ['Uncorrected', 'Bonferroni', 'BH']\ncounts = [np.sum(mt_results['significant_uncorrected']),\n          np.sum(mt_results['significant_bonferroni']),\n          np.sum(mt_results['significant_bh'])]\ncolors = ['red', 'orange', 'green']\n\nplt.bar(methods, counts, color=colors, alpha=0.7)\nplt.ylabel('Number of Significant Tests')\nplt.title('Significant Tests by Method')\n\n# P-value vs rank plot (for BH)\nplt.subplot(1, 3, 3)\nsorted_p = np.sort(mt_results['p_values'])\nranks = np.arange(1, len(sorted_p) + 1)\ncritical_line = mt_results['alpha'] * ranks / len(sorted_p)\n\nplt.plot(ranks, sorted_p, 'b.', alpha=0.7, label='P-values')\nplt.plot(ranks, critical_line, 'r-', linewidth=2, label='Critical line')\nplt.xlabel('Rank')\nplt.ylabel('P-value')\nplt.title('Benjamini-Hochberg Procedure')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Applications\n\n### A/B Testing Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ab_testing_example():\n    \"\"\"Simulate A/B testing scenario\"\"\"\n    # Simulate conversion rates\n    n_a, n_b = 1000, 1000\n    true_rate_a = 0.10  # 10% conversion rate\n    true_rate_b = 0.12  # 12% conversion rate (improvement)\n    \n    # Generate data\n    conversions_a = np.random.binomial(n_a, true_rate_a)\n    conversions_b = np.random.binomial(n_b, true_rate_b)\n    \n    # Calculate conversion rates\n    rate_a = conversions_a / n_a\n    rate_b = conversions_b / n_b\n    \n    # Perform z-test for proportions\n    pooled_rate = (conversions_a + conversions_b) / (n_a + n_b)\n    se = np.sqrt(pooled_rate * (1 - pooled_rate) * (1/n_a + 1/n_b))\n    z_stat = (rate_a - rate_b) / se\n    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n    \n    # Calculate confidence interval for difference\n    se_diff = np.sqrt(rate_a * (1 - rate_a) / n_a + rate_b * (1 - rate_b) / n_b)\n    z_critical = stats.norm.ppf(0.975)\n    margin_of_error = z_critical * se_diff\n    ci_lower = (rate_a - rate_b) - margin_of_error\n    ci_upper = (rate_a - rate_b) + margin_of_error\n    \n    return {\n        'rate_a': rate_a,\n        'rate_b': rate_b,\n        'difference': rate_b - rate_a,\n        'z_statistic': z_stat,\n        'p_value': p_value,\n        'confidence_interval': (ci_lower, ci_upper),\n        'conversions_a': conversions_a,\n        'conversions_b': conversions_b\n    }\n\nab_results = ab_testing_example()\nprint(\"A/B Testing Results\")\nfor key, value in ab_results.items():\n    if key not in ['conversions_a', 'conversions_b']:\n        print(f\"{key}: {value:.4f}\")\n\n# Visualize A/B test\nplt.figure(figsize=(15, 5))\n\n# Conversion rates\nplt.subplot(1, 3, 1)\nrates = [ab_results['rate_a'], ab_results['rate_b']]\nplt.bar(['Version A', 'Version B'], rates, color=['red', 'blue'], alpha=0.7)\nplt.ylabel('Conversion Rate')\nplt.title('Conversion Rates')\nfor i, rate in enumerate(rates):\n    plt.text(i, rate + 0.001, f'{rate:.3f}', ha='center', va='bottom')\n\n# Confidence interval for difference\nplt.subplot(1, 3, 2)\nci_lower, ci_upper = ab_results['confidence_interval']\nplt.errorbar([1], [ab_results['difference']], \n             yerr=[[ab_results['difference'] - ci_lower], [ci_upper - ab_results['difference']]], \n             fmt='o', capsize=5, capthick=2, linewidth=2)\nplt.axhline(0, color='red', linestyle='--', alpha=0.7)\nplt.ylabel('Difference (B - A)')\nplt.title('Confidence Interval for Difference')\nplt.xticks([])\n\n# P-value interpretation\nplt.subplot(1, 3, 3)\nalpha = 0.05\ndecision = \"Reject H₀\" if ab_results['p_value'] < alpha else \"Fail to reject H₀\"\nplt.text(0.5, 0.5, f\"P-value: {ab_results['p_value']:.4f}\\nα = {alpha}\\nDecision: {decision}\", \n         ha='center', va='center', transform=plt.gca().transAxes, fontsize=12,\n         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\nplt.title('Test Decision')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Problems\n\n1. **Hypothesis Testing**: Create a function that performs various hypothesis tests and reports results in a standardized format.\n\n2. **Power Analysis**: Implement power analysis to determine required sample sizes for different effect sizes.\n\n3. **Multiple Testing**: Build a function that applies different multiple testing corrections and compares their results.\n\n4. **Effect Size**: Calculate and interpret different effect size measures (Cohen's d, eta-squared, etc.).\n\n## Further Reading\n\n- \"Statistical Inference\" by George Casella and Roger L. Berger\n- \"The Practice of Statistics\" by David S. Moore\n- \"Statistics in Plain English\" by Timothy C. Urdan\n- \"Multiple Testing Procedures\" by Jason Hsu\n\n## Key Takeaways\n\n- **Hypothesis testing** provides a framework for making decisions about population parameters\n- **P-values** measure evidence against the null hypothesis, not probability of hypothesis being true\n- **Confidence intervals** provide a range of plausible values for population parameters\n- **Multiple testing** requires correction to control false positive rates\n- **Effect sizes** complement p-values by measuring practical significance\n- **Type I and Type II errors** are fundamental concepts in statistical decision making\n\nIn the next chapter, we'll explore regression analysis, including linear and multiple regression techniques."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}