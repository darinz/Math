{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bayesian Statistics\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Pandas](https://img.shields.io/badge/Pandas-1.3+-blue.svg)](https://pandas.pydata.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.4+-orange.svg)](https://matplotlib.org/)\n[![Seaborn](https://img.shields.io/badge/Seaborn-0.11+-blue.svg)](https://seaborn.pydata.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-green.svg)](https://scipy.org/)\n[![PyMC3](https://img.shields.io/badge/PyMC3-3.11+-blue.svg)](https://docs.pymc.io/)\n[![ArviZ](https://img.shields.io/badge/ArviZ-0.12+-orange.svg)](https://python.arviz.org/)\n\nBayesian statistics provides a framework for updating beliefs with data. This chapter covers Bayesian inference, MCMC methods, and their applications in AI/ML.\n\n## Table of Contents\n- [Bayesian Inference Fundamentals](#bayesian-inference-fundamentals)\n- [Conjugate Priors](#conjugate-priors)\n- [MCMC Methods](#mcmc-methods)\n- [Bayesian Regression](#bayesian-regression)\n- [Model Comparison](#model-comparison)\n- [Practical Applications](#practical-applications)\n\n## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import beta, gamma, norm, bernoulli\nimport pymc3 as pm\nimport arviz as az\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\nnp.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian Inference\n\nBayesian inference provides a coherent framework for updating beliefs about parameters based on observed data, combining prior knowledge with new evidence.\n\n### Mathematical Foundation\n\n**Bayes' Theorem:**\n$$P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)}$$\n\nWhere:\n- $P(\\theta | D)$ = **Posterior distribution** (updated belief about θ given data)\n- $P(D | \\theta)$ = **Likelihood function** (probability of data given θ)\n- $P(\\theta)$ = **Prior distribution** (initial belief about θ)\n- $P(D)$ = **Evidence/Marginal likelihood** (normalizing constant)\n\n**Continuous Case:**\n$$f(\\theta | D) = \\frac{f(D | \\theta) f(\\theta)}{\\int f(D | \\theta) f(\\theta) d\\theta}$$\n\n**Log-Posterior:**\n$$\\log f(\\theta | D) = \\log f(D | \\theta) + \\log f(\\theta) - \\log \\int f(D | \\theta) f(\\theta) d\\theta$$\n\n### Prior Distributions\n\n**Conjugate Priors:**\nA prior is **conjugate** to a likelihood if the posterior belongs to the same family as the prior.\n\n**Common Conjugate Pairs:**\n\n**1. Normal-Normal:**\n- Likelihood: $X_i \\sim N(\\mu, \\sigma^2)$ (σ² known)\n- Prior: $\\mu \\sim N(\\mu_0, \\tau_0^2)$\n- Posterior: $\\mu | D \\sim N(\\mu_n, \\tau_n^2)$\n\nWhere:\n$$\\mu_n = \\frac{\\frac{\\mu_0}{\\tau_0^2} + \\frac{n\\bar{x}}{\\sigma^2}}{\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}}$$\n$$\\frac{1}{\\tau_n^2} = \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}$$\n\n**2. Beta-Binomial:**\n- Likelihood: $X \\sim \\text{Binomial}(n, \\theta)$\n- Prior: $\\theta \\sim \\text{Beta}(\\alpha, \\beta)$\n- Posterior: $\\theta | D \\sim \\text{Beta}(\\alpha + x, \\beta + n - x)$\n\n**3. Gamma-Poisson:**\n- Likelihood: $X_i \\sim \\text{Poisson}(\\lambda)$\n- Prior: $\\lambda \\sim \\text{Gamma}(\\alpha, \\beta)$\n- Posterior: $\\lambda | D \\sim \\text{Gamma}(\\alpha + \\sum x_i, \\beta + n)$\n\n**4. Inverse Gamma-Normal:**\n- Likelihood: $X_i \\sim N(\\mu, \\sigma^2)$ (μ known)\n- Prior: $\\sigma^2 \\sim \\text{InvGamma}(\\alpha, \\beta)$\n- Posterior: $\\sigma^2 | D \\sim \\text{InvGamma}(\\alpha + n/2, \\beta + \\frac{1}{2}\\sum(x_i - \\mu)^2)$\n\n**Non-Informative Priors:**\n\n**1. Jeffreys Prior:**\n$$f(\\theta) \\propto \\sqrt{I(\\theta)}$$\n\nWhere $I(\\theta)$ is the Fisher information:\n$$I(\\theta) = -E\\left[\\frac{\\partial^2}{\\partial \\theta^2} \\log f(X | \\theta)\\right]$$\n\n**2. Uniform Prior:**\n$$f(\\theta) \\propto 1$$\n\n**3. Reference Prior:**\nMaximizes the expected Kullback-Leibler divergence between prior and posterior.\n\n### Posterior Analysis\n\n**Posterior Mean (Bayes Estimator):**\n$$\\hat{\\theta}_{Bayes} = E[\\theta | D] = \\int \\theta f(\\theta | D) d\\theta$$\n\n**Posterior Variance:**\n$$\\text{Var}(\\theta | D) = E[(\\theta - \\hat{\\theta}_{Bayes})^2 | D]$$\n\n**Posterior Mode (Maximum A Posteriori):**\n$$\\hat{\\theta}_{MAP} = \\arg\\max_{\\theta} f(\\theta | D)$$\n\n**Credible Intervals:**\nA $(1-\\alpha)$ credible interval satisfies:\n$$P(\\theta_L \\leq \\theta \\leq \\theta_U | D) = 1 - \\alpha$$\n\n**Highest Posterior Density (HPD) Interval:**\nThe shortest interval containing $(1-\\alpha)$ of the posterior probability.\n\n### Predictive Distributions\n\n**Posterior Predictive Distribution:**\n$$f(x_{new} | D) = \\int f(x_{new} | \\theta) f(\\theta | D) d\\theta$$\n\n**Prior Predictive Distribution:**\n$$f(x) = \\int f(x | \\theta) f(\\theta) d\\theta$$\n\n### Model Comparison\n\n**Bayes Factor:**\n$$BF_{12} = \\frac{P(D | M_1)}{P(D | M_2)} = \\frac{\\int f(D | \\theta_1, M_1) f(\\theta_1 | M_1) d\\theta_1}{\\int f(D | \\theta_2, M_2) f(\\theta_2 | M_2) d\\theta_2}$$\n\n**Posterior Model Probabilities:**\n$$P(M_i | D) = \\frac{P(D | M_i) P(M_i)}{\\sum_j P(D | M_j) P(M_j)}$$\n\n### Computational Methods\n\n**1. Markov Chain Monte Carlo (MCMC):**\n- **Metropolis-Hastings Algorithm**\n- **Gibbs Sampling**\n- **Hamiltonian Monte Carlo**\n\n**2. Variational Inference:**\nApproximate the posterior with a simpler distribution.\n\n**3. Laplace Approximation:**\nApproximate the posterior as a normal distribution around the MAP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.optimize import minimize\nimport seaborn as sns\n\ndef normal_normal_conjugate(x, mu0, tau0_sq, sigma_sq):\n    \"\"\"\n    Normal-Normal conjugate pair\n    \n    Mathematical implementation:\n    Prior: μ ~ N(μ₀, τ₀²)\n    Likelihood: Xᵢ ~ N(μ, σ²)\n    Posterior: μ|D ~ N(μₙ, τₙ²)\n    \n    Where:\n    μₙ = (μ₀/τ₀² + nẍ/σ²) / (1/τ₀² + n/σ²)\n    1/τₙ² = 1/τ₀² + n/σ²\n    \n    Parameters:\n    x: array, observed data\n    mu0: float, prior mean\n    tau0_sq: float, prior variance\n    sigma_sq: float, known data variance\n    \n    Returns:\n    tuple: (posterior_mean, posterior_variance)\n    \"\"\"\n    n = len(x)\n    x_bar = np.mean(x)\n    \n    # Posterior parameters\n    tau_n_sq_inv = 1/tau0_sq + n/sigma_sq\n    tau_n_sq = 1/tau_n_sq_inv\n    \n    mu_n = (mu0/tau0_sq + n*x_bar/sigma_sq) / tau_n_sq_inv\n    \n    return mu_n, tau_n_sq\n\ndef beta_binomial_conjugate(x, n, alpha, beta):\n    \"\"\"\n    Beta-Binomial conjugate pair\n    \n    Mathematical implementation:\n    Prior: θ ~ Beta(α, β)\n    Likelihood: X ~ Binomial(n, θ)\n    Posterior: θ|D ~ Beta(α + x, β + n - x)\n    \n    Parameters:\n    x: int, number of successes\n    n: int, number of trials\n    alpha, beta: float, prior parameters\n    \n    Returns:\n    tuple: (posterior_alpha, posterior_beta)\n    \"\"\"\n    alpha_post = alpha + x\n    beta_post = beta + n - x\n    \n    return alpha_post, beta_post\n\ndef gamma_poisson_conjugate(x, alpha, beta):\n    \"\"\"\n    Gamma-Poisson conjugate pair\n    \n    Mathematical implementation:\n    Prior: λ ~ Gamma(α, β)\n    Likelihood: Xᵢ ~ Poisson(λ)\n    Posterior: λ|D ~ Gamma(α + Σxᵢ, β + n)\n    \n    Parameters:\n    x: array, observed data\n    alpha, beta: float, prior parameters\n    \n    Returns:\n    tuple: (posterior_alpha, posterior_beta)\n    \"\"\"\n    n = len(x)\n    sum_x = np.sum(x)\n    \n    alpha_post = alpha + sum_x\n    beta_post = beta + n\n    \n    return alpha_post, beta_post\n\ndef jeffreys_prior_normal():\n    \"\"\"\n    Jeffreys prior for normal distribution with unknown mean\n    \n    Mathematical implementation:\n    f(μ) ∝ 1 (uniform prior)\n    f(σ²) ∝ 1/σ² (scale-invariant prior)\n    \"\"\"\n    return \"f(μ) ∝ 1, f(σ²) ∝ 1/σ²\"\n\ndef posterior_predictive_normal(mu_post, sigma_post_sq, sigma_data_sq):\n    \"\"\"\n    Posterior predictive distribution for normal model\n    \n    Mathematical implementation:\n    X_new|D ~ N(μ_post, σ_post² + σ_data²)\n    \n    Parameters:\n    mu_post: float, posterior mean\n    sigma_post_sq: float, posterior variance\n    sigma_data_sq: float, data variance\n    \n    Returns:\n    tuple: (predictive_mean, predictive_variance)\n    \"\"\"\n    pred_mean = mu_post\n    pred_var = sigma_post_sq + sigma_data_sq\n    \n    return pred_mean, pred_var\n\ndef bayes_factor_normal(x, mu1, sigma1_sq, mu2, sigma2_sq, prior1=0.5, prior2=0.5):\n    \"\"\"\n    Calculate Bayes factor for two normal models\n    \n    Mathematical implementation:\n    BF₁₂ = P(D|M₁) / P(D|M₂)\n    \n    Parameters:\n    x: array, observed data\n    mu1, sigma1_sq: parameters of model 1\n    mu2, sigma2_sq: parameters of model 2\n    prior1, prior2: prior model probabilities\n    \n    Returns:\n    float: Bayes factor\n    \"\"\"\n    n = len(x)\n    \n    # Calculate marginal likelihoods\n    def marginal_likelihood_normal(x, mu, sigma_sq):\n        # Assuming conjugate normal-normal with non-informative prior\n        x_bar = np.mean(x)\n        s_sq = np.var(x, ddof=1)\n        \n        # Marginal likelihood for normal with unknown mean\n        log_ml = -(n/2) * np.log(2*np.pi) - (n/2) * np.log(sigma_sq) - \\\n                 (1/(2*sigma_sq)) * (np.sum((x - x_bar)**2) + n*(x_bar - mu)**2)\n        return np.exp(log_ml)\n    \n    ml1 = marginal_likelihood_normal(x, mu1, sigma1_sq)\n    ml2 = marginal_likelihood_normal(x, mu2, sigma2_sq)\n    \n    bayes_factor = ml1 / ml2\n    return bayes_factor\n\ndef credible_interval_normal(mu_post, sigma_post_sq, alpha=0.05):\n    \"\"\"\n    Calculate credible interval for normal posterior\n    \n    Mathematical implementation:\n    P(μ_L ≤ μ ≤ μ_U | D) = 1 - α\n    \n    Parameters:\n    mu_post: float, posterior mean\n    sigma_post_sq: float, posterior variance\n    alpha: float, significance level\n    \n    Returns:\n    tuple: (lower_bound, upper_bound)\n    \"\"\"\n    z_alpha_2 = stats.norm.ppf(1 - alpha/2)\n    margin = z_alpha_2 * np.sqrt(sigma_post_sq)\n    \n    lower = mu_post - margin\n    upper = mu_post + margin\n    \n    return lower, upper\n\ndef metropolis_hastings_normal(log_posterior, x0, n_samples=10000, proposal_std=0.1):\n    \"\"\"\n    Metropolis-Hastings algorithm for normal posterior\n    \n    Mathematical implementation:\n    1. Propose θ* ~ q(θ*|θₜ)\n    2. Accept with probability min(1, f(θ*|D)/f(θₜ|D) × q(θₜ|θ*)/q(θ*|θₜ))\n    \n    Parameters:\n    log_posterior: function, log posterior density\n    x0: float, initial value\n    n_samples: int, number of samples\n    proposal_std: float, proposal standard deviation\n    \n    Returns:\n    array: MCMC samples\n    \"\"\"\n    samples = np.zeros(n_samples)\n    samples[0] = x0\n    \n    accepted = 0\n    \n    for i in range(1, n_samples):\n        # Propose new value\n        proposal = np.random.normal(samples[i-1], proposal_std)\n        \n        # Calculate acceptance probability\n        log_alpha = log_posterior(proposal) - log_posterior(samples[i-1])\n        alpha = min(1, np.exp(log_alpha))\n        \n        # Accept or reject\n        if np.random.random() < alpha:\n            samples[i] = proposal\n            accepted += 1\n        else:\n            samples[i] = samples[i-1]\n    \n    acceptance_rate = accepted / (n_samples - 1)\n    print(f\"Acceptance rate: {acceptance_rate:.3f}\")\n    \n    return samples\n\ndef laplace_approximation(log_posterior, x0):\n    \"\"\"\n    Laplace approximation to posterior\n    \n    Mathematical implementation:\n    f(θ|D) ≈ N(θ̂, -1/H(θ̂))\n    where θ̂ is the MAP and H(θ̂) is the Hessian at θ̂\n    \n    Parameters:\n    log_posterior: function, log posterior density\n    x0: float, initial guess\n    \n    Returns:\n    tuple: (map_estimate, laplace_variance)\n    \"\"\"\n    # Find MAP\n    result = minimize(lambda x: -log_posterior(x), x0, method='BFGS')\n    map_estimate = result.x[0]\n    \n    # Calculate Hessian (second derivative)\n    h = 1e-6\n    hessian = -(log_posterior(map_estimate + h) - 2*log_posterior(map_estimate) + \n                log_posterior(map_estimate - h)) / (h**2)\n    \n    laplace_variance = 1 / hessian\n    \n    return map_estimate, laplace_variance\n\n# Example: Normal-Normal conjugate analysis\nnp.random.seed(42)\n\n# True parameters\ntrue_mu = 5.0\ntrue_sigma = 2.0\nn_data = 20\n\n# Generate data\ndata = np.random.normal(true_mu, true_sigma, n_data)\n\n# Prior parameters\nmu0 = 0.0\ntau0_sq = 10.0\nsigma_sq = true_sigma**2  # Assume known\n\nprint(\"Bayesian Inference: Normal-Normal Conjugate Analysis\")\nprint(\"=\" * 60)\n\n# Calculate posterior\nmu_post, tau_post_sq = normal_normal_conjugate(data, mu0, tau0_sq, sigma_sq)\n\nprint(f\"Data: n = {n_data}, x̄ = {np.mean(data):.3f}, s² = {np.var(data, ddof=1):.3f}\")\nprint(f\"Prior: μ ~ N({mu0}, {tau0_sq})\")\nprint(f\"Posterior: μ|D ~ N({mu_post:.3f}, {tau_post_sq:.3f})\")\n\n# Compare with frequentist estimate\nfreq_estimate = np.mean(data)\nfreq_var = sigma_sq / n_data\n\nprint(f\"\\nComparison:\")\nprint(f\"Frequentist: μ̂ = {freq_estimate:.3f}, Var(μ̂) = {freq_var:.3f}\")\nprint(f\"Bayesian: μ̂ = {mu_post:.3f}, Var(μ|D) = {tau_post_sq:.3f}\")\n\n# Credible interval\nci_lower, ci_upper = credible_interval_normal(mu_post, tau_post_sq, alpha=0.05)\nprint(f\"95% Credible Interval: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n\n# Posterior predictive distribution\npred_mean, pred_var = posterior_predictive_normal(mu_post, tau_post_sq, sigma_sq)\nprint(f\"Posterior Predictive: X_new|D ~ N({pred_mean:.3f}, {pred_var:.3f})\")\n\n# MCMC sampling\ndef log_posterior_normal(mu):\n    \"\"\"Log posterior for normal model with normal prior\"\"\"\n    # Log likelihood\n    log_likelihood = -0.5 * np.sum((data - mu)**2) / sigma_sq\n    \n    # Log prior\n    log_prior = -0.5 * (mu - mu0)**2 / tau0_sq\n    \n    return log_likelihood + log_prior\n\nmcmc_samples = metropolis_hastings_normal(log_posterior_normal, x0=0.0, n_samples=5000)\nprint(f\"MCMC mean: {np.mean(mcmc_samples):.3f}\")\nprint(f\"MCMC variance: {np.var(mcmc_samples):.3f}\")\n\n# Laplace approximation\nmap_est, laplace_var = laplace_approximation(log_posterior_normal, x0=0.0)\nprint(f\"Laplace approximation: μ̂ = {map_est:.3f}, Var(μ|D) = {laplace_var:.3f}\")\n\n# Visualize Bayesian analysis\nplt.figure(figsize=(15, 10))\n\n# 1. Prior, likelihood, and posterior\nplt.subplot(2, 3, 1)\nmu_range = np.linspace(-2, 12, 1000)\n\n# Prior\nprior = stats.norm.pdf(mu_range, mu0, np.sqrt(tau0_sq))\nplt.plot(mu_range, prior, 'b-', linewidth=2, label='Prior')\n\n# Likelihood (scaled)\nlikelihood = stats.norm.pdf(mu_range, np.mean(data), np.sqrt(sigma_sq/n_data))\nlikelihood = likelihood / np.max(likelihood) * np.max(prior)  # Scale for visualization\nplt.plot(mu_range, likelihood, 'g-', linewidth=2, label='Likelihood (scaled)')\n\n# Posterior\nposterior = stats.norm.pdf(mu_range, mu_post, np.sqrt(tau_post_sq))\nplt.plot(mu_range, posterior, 'r-', linewidth=2, label='Posterior')\n\nplt.axvline(true_mu, color='k', linestyle='--', alpha=0.7, label='True μ')\nplt.xlabel('μ')\nplt.ylabel('Density')\nplt.title('Prior, Likelihood, and Posterior')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# 2. Data histogram with predictive distribution\nplt.subplot(2, 3, 2)\nplt.hist(data, bins=10, alpha=0.7, density=True, label='Data')\n\n# Predictive distribution\nx_pred = np.linspace(min(data) - 2, max(data) + 2, 1000)\npred_pdf = stats.norm.pdf(x_pred, pred_mean, np.sqrt(pred_var))\nplt.plot(x_pred, pred_pdf, 'r-', linewidth=2, label='Predictive')\n\nplt.xlabel('X')\nplt.ylabel('Density')\nplt.title('Data and Predictive Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# 3. MCMC trace plot\nplt.subplot(2, 3, 3)\nplt.plot(mcmc_samples[:1000], alpha=0.7)\nplt.axhline(mu_post, color='r', linestyle='--', label='Analytical Posterior Mean')\nplt.xlabel('Iteration')\nplt.ylabel('μ')\nplt.title('MCMC Trace Plot')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# 4. MCMC histogram\nplt.subplot(2, 3, 4)\nplt.hist(mcmc_samples, bins=50, alpha=0.7, density=True, label='MCMC Samples')\n\n# Analytical posterior\nposterior_mcmc = stats.norm.pdf(mu_range, mu_post, np.sqrt(tau_post_sq))\nplt.plot(mu_range, posterior_mcmc, 'r-', linewidth=2, label='Analytical Posterior')\n\nplt.xlabel('μ')\nplt.ylabel('Density')\nplt.title('MCMC vs Analytical Posterior')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# 5. Credible intervals comparison\nplt.subplot(2, 3, 5)\nintervals = []\nlabels = []\n\n# Frequentist confidence interval\nfreq_ci = stats.norm.interval(0.95, loc=freq_estimate, scale=np.sqrt(freq_var))\nintervals.append(freq_ci)\nlabels.append('Frequentist 95% CI')\n\n# Bayesian credible interval\nintervals.append((ci_lower, ci_upper))\nlabels.append('Bayesian 95% CI')\n\n# Plot intervals\ny_positions = np.arange(len(intervals))\nfor i, (lower, upper) in enumerate(intervals):\n    plt.hlines(y_positions[i], lower, upper, linewidth=3, alpha=0.7)\n    plt.plot([lower, upper], [y_positions[i], y_positions[i]], 'o', markersize=8)\n\nplt.axvline(true_mu, color='k', linestyle='--', alpha=0.7, label='True μ')\nplt.yticks(y_positions, labels)\nplt.xlabel('μ')\nplt.title('Confidence vs Credible Intervals')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# 6. Bayes factor analysis\nplt.subplot(2, 3, 6)\n# Compare two models\nmu1, sigma1_sq = 3.0, 1.0\nmu2, sigma2_sq = 7.0, 1.0\n\nbf = bayes_factor_normal(data, mu1, sigma1_sq, mu2, sigma2_sq)\nlog_bf = np.log(bf)\n\nplt.bar(['Model 1 vs Model 2'], [log_bf], alpha=0.7, color='skyblue')\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.5)\nplt.ylabel('log(Bayes Factor)')\nplt.title('Model Comparison')\nplt.grid(True, alpha=0.3)\n\n# Add interpretation\nif log_bf > 2:\n    interpretation = \"Strong evidence for Model 1\"\nelif log_bf > 1:\n    interpretation = \"Moderate evidence for Model 1\"\nelif log_bf > 0:\n    interpretation = \"Weak evidence for Model 1\"\nelif log_bf > -1:\n    interpretation = \"Weak evidence for Model 2\"\nelif log_bf > -2:\n    interpretation = \"Moderate evidence for Model 2\"\nelse:\n    interpretation = \"Strong evidence for Model 2\"\n\nplt.text(0, log_bf + 0.1, interpretation, ha='center', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n# Example: Beta-Binomial conjugate analysis\nprint(f\"\\nBeta-Binomial Conjugate Analysis\")\nprint(\"=\" * 40)\n\n# Generate binomial data\nn_trials = 50\ntrue_theta = 0.3\nx_successes = np.random.binomial(n_trials, true_theta)\n\n# Prior parameters\nalpha_prior = 2.0\nbeta_prior = 5.0\n\n# Calculate posterior\nalpha_post, beta_post = beta_binomial_conjugate(x_successes, n_trials, alpha_prior, beta_prior)\n\nprint(f\"Data: {x_successes} successes out of {n_trials} trials\")\nprint(f\"Prior: θ ~ Beta({alpha_prior}, {beta_prior})\")\nprint(f\"Posterior: θ|D ~ Beta({alpha_post}, {beta_post})\")\n\n# Posterior statistics\nposterior_mean = alpha_post / (alpha_post + beta_post)\nposterior_var = (alpha_post * beta_post) / ((alpha_post + beta_post)**2 * (alpha_post + beta_post + 1))\n\nprint(f\"Posterior mean: {posterior_mean:.3f}\")\nprint(f\"Posterior variance: {posterior_var:.6f}\")\n\n# Frequentist comparison\nfreq_estimate = x_successes / n_trials\nfreq_var = freq_estimate * (1 - freq_estimate) / n_trials\n\nprint(f\"Frequentist: θ̂ = {freq_estimate:.3f}, Var(θ̂) = {freq_var:.6f}\")\n\n# Credible interval for beta distribution\nci_lower_beta = stats.beta.ppf(0.025, alpha_post, beta_post)\nci_upper_beta = stats.beta.ppf(0.975, alpha_post, beta_post)\nprint(f\"95% Credible Interval: [{ci_lower_beta:.3f}, {ci_upper_beta:.3f}]\")\n\n# Visualize beta-binomial analysis\nplt.figure(figsize=(12, 8))\n\n# Prior, likelihood, and posterior\ntheta_range = np.linspace(0, 1, 1000)\n\n# Prior\nprior_beta = stats.beta.pdf(theta_range, alpha_prior, beta_prior)\nplt.subplot(2, 2, 1)\nplt.plot(theta_range, prior_beta, 'b-', linewidth=2, label='Prior')\nplt.xlabel('θ')\nplt.ylabel('Density')\nplt.title('Beta Prior')\nplt.grid(True, alpha=0.3)\n\n# Likelihood (scaled)\nlikelihood_binom = stats.binom.pmf(x_successes, n_trials, theta_range)\nlikelihood_binom = likelihood_binom / np.max(likelihood_binom) * np.max(prior_beta)\nplt.subplot(2, 2, 2)\nplt.plot(theta_range, likelihood_binom, 'g-', linewidth=2, label='Likelihood (scaled)')\nplt.xlabel('θ')\nplt.ylabel('Density')\nplt.title('Binomial Likelihood')\nplt.grid(True, alpha=0.3)\n\n# Posterior\nposterior_beta = stats.beta.pdf(theta_range, alpha_post, beta_post)\nplt.subplot(2, 2, 3)\nplt.plot(theta_range, posterior_beta, 'r-', linewidth=2, label='Posterior')\nplt.axvline(true_theta, color='k', linestyle='--', alpha=0.7, label='True θ')\nplt.axvline(posterior_mean, color='orange', linestyle='--', alpha=0.7, label='Posterior Mean')\nplt.xlabel('θ')\nplt.ylabel('Density')\nplt.title('Beta Posterior')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Predictive distribution\nplt.subplot(2, 2, 4)\nn_new = 20\npred_successes = np.arange(0, n_new + 1)\npred_probs = stats.betabinom.pmf(pred_successes, n_new, alpha_post, beta_post)\n\nplt.bar(pred_successes, pred_probs, alpha=0.7, color='purple')\nplt.xlabel('Number of Successes')\nplt.ylabel('Probability')\nplt.title(f'Predictive Distribution (n={n_new})')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate mathematical properties\nprint(f\"\\nMathematical Properties Verification:\")\n\n# 1. Conjugate property\nprint(f\"1. Conjugate Property:\")\nprint(f\"   Prior: Beta({alpha_prior}, {beta_prior})\")\nprint(f\"   Likelihood: Binomial({n_trials}, θ)\")\nprint(f\"   Posterior: Beta({alpha_post}, {beta_post})\")\nprint(f\"   Conjugate property holds: {alpha_post == alpha_prior + x_successes and beta_post == beta_prior + n_trials - x_successes}\")\n\n# 2. Posterior mean as weighted average\nprint(f\"\\n2. Posterior Mean as Weighted Average:\")\nprior_mean = alpha_prior / (alpha_prior + beta_prior)\nlikelihood_mean = x_successes / n_trials\nweight_prior = (alpha_prior + beta_prior) / (alpha_prior + beta_prior + n_trials)\nweight_likelihood = n_trials / (alpha_prior + beta_prior + n_trials)\n\nweighted_avg = weight_prior * prior_mean + weight_likelihood * likelihood_mean\nprint(f\"   Prior mean: {prior_mean:.3f}\")\nprint(f\"   Likelihood mean: {likelihood_mean:.3f}\")\nprint(f\"   Weighted average: {weighted_avg:.3f}\")\nprint(f\"   Posterior mean: {posterior_mean:.3f}\")\nprint(f\"   Agreement: {abs(weighted_avg - posterior_mean) < 1e-10}\")\n\n# 3. Effect of sample size\nprint(f\"\\n3. Effect of Sample Size:\")\n# Compare with different sample sizes\nsample_sizes = [10, 50, 100, 500]\nfor n in sample_sizes:\n    x_n = np.random.binomial(n, true_theta)\n    alpha_n, beta_n = beta_binomial_conjugate(x_n, n, alpha_prior, beta_prior)\n    mean_n = alpha_n / (alpha_n + beta_n)\n    var_n = (alpha_n * beta_n) / ((alpha_n + beta_n)**2 * (alpha_n + beta_n + 1))\n    print(f\"   n={n}: θ̂={mean_n:.3f}, Var(θ|D)={var_n:.6f}\")\n\n# 4. Bayes factor interpretation\nprint(f\"\\n4. Bayes Factor Interpretation:\")\nbf_values = [0.01, 0.1, 0.3, 1, 3, 10, 100]\ninterpretations = [\"Very strong evidence for M2\", \"Strong evidence for M2\", \n                   \"Moderate evidence for M2\", \"No preference\", \n                   \"Moderate evidence for M1\", \"Strong evidence for M1\", \n                   \"Very strong evidence for M1\"]\n\nfor bf, interpretation in zip(bf_values, interpretations):\n    print(f\"   BF = {bf}: {interpretation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conjugate Priors\n\n### Common Conjugate Prior Families"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conjugate_prior_examples():\n    \"\"\"Demonstrate common conjugate prior families\"\"\"\n    \n    # 1. Normal-Normal (known variance)\n    mu_0, sigma_0 = 0, 2  # Prior mean and std\n    sigma = 1  # Known data std\n    data = np.random.normal(3, sigma, 10)  # Data\n    \n    # Posterior parameters\n    n = len(data)\n    x_bar = np.mean(data)\n    mu_post = (mu_0/sigma_0**2 + n*x_bar/sigma**2) / (1/sigma_0**2 + n/sigma**2)\n    sigma_post = np.sqrt(1 / (1/sigma_0**2 + n/sigma**2))\n    \n    # 2. Beta-Bernoulli\n    alpha_0, beta_0 = 1, 1  # Prior (uniform)\n    bernoulli_data = np.random.binomial(1, 0.7, 20)  # Data\n    alpha_post = alpha_0 + sum(bernoulli_data)\n    beta_post = beta_0 + len(bernoulli_data) - sum(bernoulli_data)\n    \n    # 3. Gamma-Poisson\n    alpha_0, beta_0 = 2, 1  # Prior\n    poisson_data = np.random.poisson(5, 15)  # Data\n    alpha_post_pois = alpha_0 + sum(poisson_data)\n    beta_post_pois = beta_0 + len(poisson_data)\n    \n    return {\n        'normal': {'prior': (mu_0, sigma_0), 'posterior': (mu_post, sigma_post), 'data': data},\n        'bernoulli': {'prior': (alpha_0, beta_0), 'posterior': (alpha_post, beta_post), 'data': bernoulli_data},\n        'poisson': {'prior': (alpha_0, beta_0), 'posterior': (alpha_post_pois, beta_post_pois), 'data': poisson_data}\n    }\n\nconjugate_examples = conjugate_prior_examples()\n\n# Visualize conjugate prior examples\nplt.figure(figsize=(15, 10))\n\n# Normal-Normal\nplt.subplot(3, 3, 1)\nx = np.linspace(-5, 5, 1000)\nprior_norm = norm.pdf(x, conjugate_examples['normal']['prior'][0], conjugate_examples['normal']['prior'][1])\nposterior_norm = norm.pdf(x, conjugate_examples['normal']['posterior'][0], conjugate_examples['normal']['posterior'][1])\nplt.plot(x, prior_norm, 'b-', linewidth=2, label='Prior')\nplt.plot(x, posterior_norm, 'r-', linewidth=2, label='Posterior')\nplt.xlabel('μ')\nplt.ylabel('Density')\nplt.title('Normal-Normal Conjugate')\nplt.legend()\n\n# Data histogram\nplt.subplot(3, 3, 2)\nplt.hist(conjugate_examples['normal']['data'], bins=10, alpha=0.7, color='green', edgecolor='black')\nplt.xlabel('Data')\nplt.ylabel('Frequency')\nplt.title('Normal Data')\n\n# Beta-Bernoulli\nplt.subplot(3, 3, 3)\ntheta = np.linspace(0, 1, 1000)\nprior_beta = beta.pdf(theta, conjugate_examples['bernoulli']['prior'][0], conjugate_examples['bernoulli']['prior'][1])\nposterior_beta = beta.pdf(theta, conjugate_examples['bernoulli']['posterior'][0], conjugate_examples['bernoulli']['posterior'][1])\nplt.plot(theta, prior_beta, 'b-', linewidth=2, label='Prior')\nplt.plot(theta, posterior_beta, 'r-', linewidth=2, label='Posterior')\nplt.xlabel('θ')\nplt.ylabel('Density')\nplt.title('Beta-Bernoulli Conjugate')\nplt.legend()\n\n# Bernoulli data\nplt.subplot(3, 3, 4)\nplt.hist(conjugate_examples['bernoulli']['data'], bins=[0, 1, 2], alpha=0.7, color='green', edgecolor='black')\nplt.xlabel('Data')\nplt.ylabel('Frequency')\nplt.title('Bernoulli Data')\nplt.xticks([0.5], ['0/1'])\n\n# Gamma-Poisson\nplt.subplot(3, 3, 5)\nx_gamma = np.linspace(0, 10, 1000)\nprior_gamma = gamma.pdf(x_gamma, conjugate_examples['poisson']['prior'][0], scale=1/conjugate_examples['poisson']['prior'][1])\nposterior_gamma = gamma.pdf(x_gamma, conjugate_examples['poisson']['posterior'][0], scale=1/conjugate_examples['poisson']['posterior'][1])\nplt.plot(x_gamma, prior_gamma, 'b-', linewidth=2, label='Prior')\nplt.plot(x_gamma, posterior_gamma, 'r-', linewidth=2, label='Posterior')\nplt.xlabel('λ')\nplt.ylabel('Density')\nplt.title('Gamma-Poisson Conjugate')\nplt.legend()\n\n# Poisson data\nplt.subplot(3, 3, 6)\nplt.hist(conjugate_examples['poisson']['data'], bins=range(0, max(conjugate_examples['poisson']['data'])+2), \n         alpha=0.7, color='green', edgecolor='black')\nplt.xlabel('Data')\nplt.ylabel('Frequency')\nplt.title('Poisson Data')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Conjugate Prior Examples:\")\nprint(f\"Normal-Normal: Prior μ={conjugate_examples['normal']['prior'][0]:.2f}, Posterior μ={conjugate_examples['normal']['posterior'][0]:.2f}\")\nprint(f\"Beta-Bernoulli: Prior α={conjugate_examples['bernoulli']['prior'][0]}, Posterior α={conjugate_examples['bernoulli']['posterior'][0]}\")\nprint(f\"Gamma-Poisson: Prior α={conjugate_examples['poisson']['prior'][0]}, Posterior α={conjugate_examples['poisson']['posterior'][0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MCMC Methods\n\n### Metropolis-Hastings Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def metropolis_hastings(target_dist, proposal_dist, n_samples=10000, initial_state=0):\n    \"\"\"Metropolis-Hastings MCMC algorithm\"\"\"\n    \n    samples = [initial_state]\n    accepted = 0\n    \n    for i in range(n_samples):\n        current_state = samples[-1]\n        \n        # Propose new state\n        proposed_state = proposal_dist(current_state)\n        \n        # Calculate acceptance probability\n        acceptance_ratio = target_dist(proposed_state) / target_dist(current_state)\n        acceptance_prob = min(1, acceptance_ratio)\n        \n        # Accept or reject\n        if np.random.random() < acceptance_prob:\n            samples.append(proposed_state)\n            accepted += 1\n        else:\n            samples.append(current_state)\n    \n    acceptance_rate = accepted / n_samples\n    return np.array(samples), acceptance_rate\n\n# Example: Sample from a mixture of normals\ndef target_distribution(x):\n    \"\"\"Target distribution: mixture of two normals\"\"\"\n    return 0.3 * norm.pdf(x, -2, 1) + 0.7 * norm.pdf(x, 3, 1.5)\n\ndef proposal_distribution(current_state):\n    \"\"\"Proposal distribution: normal centered at current state\"\"\"\n    return np.random.normal(current_state, 1.0)\n\n# Run MCMC\nmcmc_samples, acceptance_rate = metropolis_hastings(target_distribution, proposal_distribution)\n\nprint(\"Metropolis-Hastings MCMC Results\")\nprint(f\"Number of samples: {len(mcmc_samples)}\")\nprint(f\"Acceptance rate: {acceptance_rate:.3f}\")\n\n# Visualize MCMC results\nplt.figure(figsize=(15, 5))\n\n# Target distribution\nplt.subplot(1, 3, 1)\nx = np.linspace(-6, 8, 1000)\ntarget_pdf = target_distribution(x)\nplt.plot(x, target_pdf, 'b-', linewidth=2, label='Target Distribution')\nplt.hist(mcmc_samples, bins=50, density=True, alpha=0.7, color='red', edgecolor='black', label='MCMC Samples')\nplt.xlabel('x')\nplt.ylabel('Density')\nplt.title('Target Distribution vs MCMC Samples')\nplt.legend()\n\n# Trace plot\nplt.subplot(1, 3, 2)\nplt.plot(mcmc_samples[:1000], 'g-', alpha=0.7)\nplt.xlabel('Iteration')\nplt.ylabel('Sample Value')\nplt.title('Trace Plot (First 1000 iterations)')\n\n# Autocorrelation\nplt.subplot(1, 3, 3)\nfrom statsmodels.tsa.stattools import acf\nacf_values = acf(mcmc_samples, nlags=50)\nplt.bar(range(len(acf_values)), acf_values, alpha=0.7, color='purple')\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.title('Autocorrelation Function')\n\nplt.tight_layout()\nplt.show()\n\n# Effective sample size\ndef effective_sample_size(samples):\n    \"\"\"Calculate effective sample size\"\"\"\n    n = len(samples)\n    acf_values = acf(samples, nlags=min(n//2, 1000))\n    # Sum autocorrelations up to first negative value\n    cutoff = np.where(acf_values < 0)[0]\n    if len(cutoff) > 0:\n        cutoff = cutoff[0]\n    else:\n        cutoff = len(acf_values)\n    \n    ess = n / (1 + 2 * np.sum(acf_values[1:cutoff]))\n    return ess\n\ness = effective_sample_size(mcmc_samples)\nprint(f\"Effective sample size: {ess:.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian Regression\n\n### Linear Regression with PyMC3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bayesian_linear_regression():\n    \"\"\"Perform Bayesian linear regression using PyMC3\"\"\"\n    \n    # Generate synthetic data\n    np.random.seed(42)\n    n = 50\n    x = np.random.uniform(0, 10, n)\n    true_slope = 2.5\n    true_intercept = 1.0\n    true_sigma = 1.0\n    y = true_intercept + true_slope * x + np.random.normal(0, true_sigma, n)\n    \n    # Bayesian model with PyMC3\n    with pm.Model() as model:\n        # Priors\n        intercept = pm.Normal('intercept', mu=0, sd=10)\n        slope = pm.Normal('slope', mu=0, sd=10)\n        sigma = pm.HalfNormal('sigma', sd=1)\n        \n        # Likelihood\n        mu = intercept + slope * x\n        likelihood = pm.Normal('likelihood', mu=mu, sd=sigma, observed=y)\n        \n        # Sample from posterior\n        trace = pm.sample(2000, tune=1000, return_inferencedata=False)\n    \n    return model, trace, x, y\n\nmodel, trace, x_reg, y_reg = bayesian_linear_regression()\n\nprint(\"Bayesian Linear Regression Results\")\nprint(f\"True parameters: intercept={1.0}, slope={2.5}, sigma={1.0}\")\n\n# Visualize Bayesian regression results\nplt.figure(figsize=(15, 10))\n\n# Posterior distributions\nplt.subplot(2, 3, 1)\nplt.hist(trace['intercept'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\nplt.axvline(1.0, color='red', linestyle='--', linewidth=2, label='True value')\nplt.xlabel('Intercept')\nplt.ylabel('Frequency')\nplt.title('Posterior Distribution - Intercept')\nplt.legend()\n\nplt.subplot(2, 3, 2)\nplt.hist(trace['slope'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.axvline(2.5, color='red', linestyle='--', linewidth=2, label='True value')\nplt.xlabel('Slope')\nplt.ylabel('Frequency')\nplt.title('Posterior Distribution - Slope')\nplt.legend()\n\nplt.subplot(2, 3, 3)\nplt.hist(trace['sigma'], bins=30, alpha=0.7, color='orange', edgecolor='black')\nplt.axvline(1.0, color='red', linestyle='--', linewidth=2, label='True value')\nplt.xlabel('Sigma')\nplt.ylabel('Frequency')\nplt.title('Posterior Distribution - Sigma')\nplt.legend()\n\n# Regression plot with uncertainty\nplt.subplot(2, 3, 4)\nplt.scatter(x_reg, y_reg, alpha=0.7, color='blue', label='Data')\n\n# Plot regression lines from posterior samples\nx_plot = np.linspace(0, 10, 100)\nfor i in range(0, len(trace['intercept']), 100):  # Plot every 100th sample\n    y_plot = trace['intercept'][i] + trace['slope'][i] * x_plot\n    plt.plot(x_plot, y_plot, 'r-', alpha=0.1)\n\n# Plot mean regression line\nmean_intercept = np.mean(trace['intercept'])\nmean_slope = np.mean(trace['slope'])\ny_mean = mean_intercept + mean_slope * x_plot\nplt.plot(x_plot, y_mean, 'r-', linewidth=3, label='Mean prediction')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Bayesian Regression with Uncertainty')\nplt.legend()\n\n# Credible intervals\nplt.subplot(2, 3, 5)\n# Calculate credible intervals for predictions\ny_predictions = []\nfor x_val in x_plot:\n    y_pred = trace['intercept'] + trace['slope'] * x_val\n    y_predictions.append(y_pred)\n\ny_predictions = np.array(y_predictions)\nlower_ci = np.percentile(y_predictions, 2.5, axis=1)\nupper_ci = np.percentile(y_predictions, 97.5, axis=1)\n\nplt.scatter(x_reg, y_reg, alpha=0.7, color='blue', label='Data')\nplt.plot(x_plot, y_mean, 'r-', linewidth=2, label='Mean prediction')\nplt.fill_between(x_plot, lower_ci, upper_ci, alpha=0.3, color='red', label='95% Credible Interval')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Regression with Credible Intervals')\nplt.legend()\n\n# Trace plots\nplt.subplot(2, 3, 6)\nplt.plot(trace['slope'][:1000], 'g-', alpha=0.7)\nplt.xlabel('Iteration')\nplt.ylabel('Slope')\nplt.title('Trace Plot - Slope')\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(f\"\\nPosterior Summary:\")\nprint(f\"Intercept: {np.mean(trace['intercept']):.3f} ± {np.std(trace['intercept']):.3f}\")\nprint(f\"Slope: {np.mean(trace['slope']):.3f} ± {np.std(trace['slope']):.3f}\")\nprint(f\"Sigma: {np.mean(trace['sigma']):.3f} ± {np.std(trace['sigma']):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Comparison\n\n### Bayes Factors and Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bayesian_model_comparison():\n    \"\"\"Compare Bayesian models using different criteria\"\"\"\n    \n    # Generate data from a quadratic model\n    np.random.seed(42)\n    n = 100\n    x = np.random.uniform(-3, 3, n)\n    y = 1 + 2*x + 0.5*x**2 + np.random.normal(0, 0.5, n)\n    \n    # Model 1: Linear\n    with pm.Model() as linear_model:\n        intercept = pm.Normal('intercept', mu=0, sd=10)\n        slope = pm.Normal('slope', mu=0, sd=10)\n        sigma = pm.HalfNormal('sigma', sd=1)\n        \n        mu = intercept + slope * x\n        likelihood = pm.Normal('likelihood', mu=mu, sd=sigma, observed=y)\n        \n        linear_trace = pm.sample(1000, tune=500, return_inferencedata=False)\n    \n    # Model 2: Quadratic\n    with pm.Model() as quadratic_model:\n        intercept = pm.Normal('intercept', mu=0, sd=10)\n        slope = pm.Normal('slope', mu=0, sd=10)\n        quadratic = pm.Normal('quadratic', mu=0, sd=10)\n        sigma = pm.HalfNormal('sigma', sd=1)\n        \n        mu = intercept + slope * x + quadratic * x**2\n        likelihood = pm.Normal('likelihood', mu=mu, sd=sigma, observed=y)\n        \n        quadratic_trace = pm.sample(1000, tune=500, return_inferencedata=False)\n    \n    return linear_model, quadratic_model, linear_trace, quadratic_trace, x, y\n\nlinear_model, quadratic_model, linear_trace, quadratic_trace, x_comp, y_comp = bayesian_model_comparison()\n\n# Calculate model comparison metrics\ndef calculate_model_metrics(model, trace, x, y):\n    \"\"\"Calculate various model comparison metrics\"\"\"\n    \n    # DIC (Deviance Information Criterion)\n    dic = pm.dic(trace, model)\n    \n    # WAIC (Widely Applicable Information Criterion)\n    waic = pm.waic(trace, model)\n    \n    # LOO-CV (Leave-One-Out Cross-Validation)\n    loo = pm.loo(trace, model)\n    \n    return {'DIC': dic, 'WAIC': waic, 'LOO': loo}\n\nlinear_metrics = calculate_model_metrics(linear_model, linear_trace, x_comp, y_comp)\nquadratic_metrics = calculate_model_metrics(quadratic_model, quadratic_trace, x_comp, y_comp)\n\nprint(\"Bayesian Model Comparison\")\nprint(f\"Linear Model:\")\nprint(f\"  DIC: {linear_metrics['DIC']:.2f}\")\nprint(f\"  WAIC: {linear_metrics['WAIC']['waic']:.2f}\")\nprint(f\"  LOO: {linear_metrics['LOO']['loo']:.2f}\")\n\nprint(f\"\\nQuadratic Model:\")\nprint(f\"  DIC: {quadratic_metrics['DIC']:.2f}\")\nprint(f\"  WAIC: {quadratic_metrics['WAIC']['waic']:.2f}\")\nprint(f\"  LOO: {quadratic_metrics['LOO']['loo']:.2f}\")\n\n# Visualize model comparison\nplt.figure(figsize=(15, 5))\n\n# Data and model fits\nplt.subplot(1, 3, 1)\nplt.scatter(x_comp, y_comp, alpha=0.7, color='blue', label='Data')\n\n# Linear model predictions\nx_plot = np.linspace(-3, 3, 100)\nlinear_pred = np.mean(linear_trace['intercept']) + np.mean(linear_trace['slope']) * x_plot\nplt.plot(x_plot, linear_pred, 'r-', linewidth=2, label='Linear Model')\n\n# Quadratic model predictions\nquad_pred = (np.mean(quadratic_trace['intercept']) + \n             np.mean(quadratic_trace['slope']) * x_plot + \n             np.mean(quadratic_trace['quadratic']) * x_plot**2)\nplt.plot(x_plot, quad_pred, 'g-', linewidth=2, label='Quadratic Model')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Model Fits')\nplt.legend()\n\n# Model comparison metrics\nplt.subplot(1, 3, 2)\nmetrics = ['DIC', 'WAIC', 'LOO']\nlinear_values = [linear_metrics['DIC'], linear_metrics['WAIC']['waic'], linear_metrics['LOO']['loo']]\nquadratic_values = [quadratic_metrics['DIC'], quadratic_metrics['WAIC']['waic'], quadratic_metrics['LOO']['loo']]\n\nx_pos = np.arange(len(metrics))\nwidth = 0.35\n\nplt.bar(x_pos - width/2, linear_values, width, label='Linear Model', alpha=0.7)\nplt.bar(x_pos + width/2, quadratic_values, width, label='Quadratic Model', alpha=0.7)\nplt.xlabel('Metric')\nplt.ylabel('Value')\nplt.title('Model Comparison Metrics')\nplt.xticks(x_pos, metrics)\nplt.legend()\n\n# Residuals comparison\nplt.subplot(1, 3, 3)\nlinear_residuals = y_comp - (np.mean(linear_trace['intercept']) + np.mean(linear_trace['slope']) * x_comp)\nquadratic_residuals = y_comp - (np.mean(quadratic_trace['intercept']) + \n                               np.mean(quadratic_trace['slope']) * x_comp + \n                               np.mean(quadratic_trace['quadratic']) * x_comp**2)\n\nplt.scatter(x_comp, linear_residuals, alpha=0.7, color='red', label='Linear Residuals')\nplt.scatter(x_comp, quadratic_residuals, alpha=0.7, color='green', label='Quadratic Residuals')\nplt.axhline(0, color='black', linestyle='--', alpha=0.7)\nplt.xlabel('x')\nplt.ylabel('Residuals')\nplt.title('Residuals Comparison')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Model selection conclusion\nprint(f\"\\nModel Selection:\")\nprint(f\"Lower values indicate better models.\")\nprint(f\"Linear model preferred by: {sum(1 for i in range(3) if linear_values[i] < quadratic_values[i])}/3 metrics\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Applications\n\n### Bayesian A/B Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bayesian_ab_testing():\n    \"\"\"Perform Bayesian A/B testing\"\"\"\n    \n    # Simulate A/B test data\n    np.random.seed(42)\n    n_a, n_b = 1000, 1000\n    true_rate_a = 0.10  # 10% conversion rate\n    true_rate_b = 0.12  # 12% conversion rate\n    \n    conversions_a = np.random.binomial(n_a, true_rate_a)\n    conversions_b = np.random.binomial(n_b, true_rate_b)\n    \n    # Bayesian model\n    with pm.Model() as ab_model:\n        # Priors for conversion rates\n        rate_a = pm.Beta('rate_a', alpha=1, beta=1)  # Uniform prior\n        rate_b = pm.Beta('rate_b', alpha=1, beta=1)  # Uniform prior\n        \n        # Likelihoods\n        obs_a = pm.Binomial('obs_a', n=n_a, p=rate_a, observed=conversions_a)\n        obs_b = pm.Binomial('obs_b', n=n_b, p=rate_b, observed=conversions_b)\n        \n        # Difference between rates\n        diff = pm.Deterministic('diff', rate_b - rate_a)\n        \n        # Sample from posterior\n        ab_trace = pm.sample(2000, tune=1000, return_inferencedata=False)\n    \n    return ab_trace, conversions_a, conversions_b, n_a, n_b\n\nab_trace, conv_a, conv_b, n_a, n_b = bayesian_ab_testing()\n\nprint(\"Bayesian A/B Testing Results\")\nprint(f\"Group A: {conv_a}/{n_a} conversions ({conv_a/n_a:.3f})\")\nprint(f\"Group B: {conv_b}/{n_b} conversions ({conv_b/n_b:.3f})\")\n\n# Visualize A/B testing results\nplt.figure(figsize=(15, 5))\n\n# Posterior distributions\nplt.subplot(1, 3, 1)\nplt.hist(ab_trace['rate_a'], bins=30, alpha=0.7, color='blue', label='Group A', density=True)\nplt.hist(ab_trace['rate_b'], bins=30, alpha=0.7, color='red', label='Group B', density=True)\nplt.axvline(conv_a/n_a, color='blue', linestyle='--', alpha=0.7)\nplt.axvline(conv_b/n_b, color='red', linestyle='--', alpha=0.7)\nplt.xlabel('Conversion Rate')\nplt.ylabel('Density')\nplt.title('Posterior Distributions')\nplt.legend()\n\n# Difference distribution\nplt.subplot(1, 3, 2)\nplt.hist(ab_trace['diff'], bins=30, alpha=0.7, color='green', edgecolor='black')\nplt.axvline(0, color='red', linestyle='--', alpha=0.7, label='No difference')\nplt.xlabel('Difference (B - A)')\nplt.ylabel('Frequency')\nplt.title('Posterior Distribution of Difference')\nplt.legend()\n\n# Probability of B being better\nprob_b_better = np.mean(ab_trace['diff'] > 0)\nplt.subplot(1, 3, 3)\nplt.bar(['A Better', 'B Better'], [1-prob_b_better, prob_b_better], \n        alpha=0.7, color=['blue', 'red'])\nplt.ylabel('Probability')\nplt.title('Probability of B being Better')\nplt.ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\n# Credible intervals\nrate_a_ci = np.percentile(ab_trace['rate_a'], [2.5, 97.5])\nrate_b_ci = np.percentile(ab_trace['rate_b'], [2.5, 97.5])\ndiff_ci = np.percentile(ab_trace['diff'], [2.5, 97.5])\n\nprint(f\"\\nCredible Intervals (95%):\")\nprint(f\"Rate A: [{rate_a_ci[0]:.4f}, {rate_a_ci[1]:.4f}]\")\nprint(f\"Rate B: [{rate_b_ci[0]:.4f}, {rate_b_ci[1]:.4f}]\")\nprint(f\"Difference: [{diff_ci[0]:.4f}, {diff_ci[1]:.4f}]\")\nprint(f\"Probability B > A: {prob_b_better:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Problems\n\n1. **Bayesian Inference**: Implement Bayesian updating for different likelihood-prior combinations.\n\n2. **MCMC Diagnostics**: Create comprehensive MCMC diagnostic tools for convergence assessment.\n\n3. **Model Comparison**: Build Bayesian model comparison frameworks with multiple criteria.\n\n4. **Hierarchical Models**: Implement hierarchical Bayesian models for grouped data.\n\n## Further Reading\n\n- \"Bayesian Data Analysis\" by Andrew Gelman et al.\n- \"Doing Bayesian Data Analysis\" by John K. Kruschke\n- \"Statistical Rethinking\" by Richard McElreath\n- \"Bayesian Methods for Hackers\" by Cameron Davidson-Pilon\n\n## Key Takeaways\n\n- **Bayesian inference** provides a coherent framework for updating beliefs with data\n- **Conjugate priors** simplify posterior calculations and have analytical solutions\n- **MCMC methods** enable sampling from complex posterior distributions\n- **Bayesian regression** naturally incorporates uncertainty in predictions\n- **Model comparison** uses information criteria like DIC, WAIC, and LOO-CV\n- **Credible intervals** provide intuitive uncertainty quantification\n- **Bayesian methods** are particularly valuable for small datasets and complex models\n- **Real-world applications** include A/B testing, medical diagnosis, and recommendation systems\n\nIn the next chapter, we'll explore experimental design, including randomized controlled trials, factorial designs, and A/B testing methodologies."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}