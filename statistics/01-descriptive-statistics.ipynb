{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Descriptive Statistics\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Pandas](https://img.shields.io/badge/Pandas-1.3+-blue.svg)](https://pandas.pydata.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.4+-orange.svg)](https://matplotlib.org/)\n[![Seaborn](https://img.shields.io/badge/Seaborn-0.11+-blue.svg)](https://seaborn.pydata.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-green.svg)](https://scipy.org/)\n\nDescriptive statistics provide a way to summarize and describe the main features of a dataset. In AI/ML and data science, understanding your data through descriptive statistics is the first crucial step before building any models.\n\n## Table of Contents\n- [Measures of Central Tendency](#measures-of-central-tendency)\n- [Measures of Dispersion](#measures-of-dispersion)\n- [Data Visualization](#data-visualization)\n- [Data Distribution](#data-distribution)\n- [Correlation Analysis](#correlation-analysis)\n- [Practical Applications](#practical-applications)\n\n## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for better plots\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\n# Create sample datasets for demonstration\nnp.random.seed(42)\n\n# Normal distribution\nnormal_data = np.random.normal(100, 15, 1000)\n\n# Skewed distribution\nskewed_data = np.random.exponential(2, 1000)\n\n# Multiple variables for correlation\nx = np.random.normal(0, 1, 1000)\ny = 0.7 * x + np.random.normal(0, 0.5, 1000)\nz = -0.3 * x + 0.4 * y + np.random.normal(0, 0.3, 1000)\n\n# Create DataFrame\ndf = pd.DataFrame({\n    'normal': normal_data,\n    'skewed': skewed_data,\n    'x': x,\n    'y': y,\n    'z': z\n})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measures of Central Tendency\n\nCentral tendency measures describe the center or typical value of a dataset. These measures help us understand where the \"middle\" of our data lies, which is crucial for understanding the distribution and making informed decisions in data analysis.\n\n### Mean (Arithmetic Average)\n\nThe **arithmetic mean** is the most commonly used measure of central tendency. It represents the sum of all values divided by the number of values.\n\n**Mathematical Definition:**\nFor a dataset with n observations: x₁, x₂, ..., xₙ\n\n$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i = \\frac{x_1 + x_2 + ... + x_n}{n}$$\n\n**Properties of the Mean:**\n1. **Linearity**: If we multiply each value by a constant c and add a constant a, the new mean becomes: $\\bar{x}_{new} = c\\bar{x} + a$\n2. **Minimizes Sum of Squared Deviations**: The mean minimizes $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$\n3. **Sensitivity to Outliers**: The mean is heavily influenced by extreme values\n\n**When to Use:**\n- Data is approximately normally distributed\n- No extreme outliers\n- Need a measure that uses all data points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_mean(data):\n    \"\"\"\n    Calculate arithmetic mean of a dataset\n    \n    Mathematical implementation:\n    mean = (sum of all values) / (number of values)\n    \n    Parameters:\n    data: array-like, input data\n    \n    Returns:\n    float: arithmetic mean\n    \"\"\"\n    if len(data) == 0:\n        raise ValueError(\"Cannot calculate mean of empty dataset\")\n    \n    return np.sum(data) / len(data)\n\n# Example calculations with detailed explanation\nmean_normal = calculate_mean(normal_data)\nmean_skewed = calculate_mean(skewed_data)\n\nprint(f\"Mean of normal distribution: {mean_normal:.2f}\")\nprint(f\"Mean of skewed distribution: {mean_skewed:.2f}\")\n\n# Mathematical verification\nprint(f\"Sum of normal data: {np.sum(normal_data):.2f}\")\nprint(f\"Count of normal data: {len(normal_data)}\")\nprint(f\"Calculated mean: {np.sum(normal_data) / len(normal_data):.2f}\")\n\n# Using NumPy (vectorized computation)\nprint(f\"NumPy mean - normal: {np.mean(normal_data):.2f}\")\nprint(f\"NumPy mean - skewed: {np.mean(skewed_data):.2f}\")\n\n# Pandas DataFrame (handles missing values automatically)\nprint(f\"DataFrame mean:\\n{df.mean()}\")\n\n# Demonstrate linearity property\nc, a = 2, 10\ntransformed_data = c * normal_data + a\nprint(f\"Original mean: {np.mean(normal_data):.2f}\")\nprint(f\"Transformed mean: {np.mean(transformed_data):.2f}\")\nprint(f\"Expected: {c * np.mean(normal_data) + a:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Median\n\nThe **median** is the middle value when data is ordered from smallest to largest. It's a robust measure that is not affected by extreme values.\n\n**Mathematical Definition:**\nFor ordered data: x₁ ≤ x₂ ≤ ... ≤ xₙ\n\n$$\\text{Median} = \\begin{cases} \nx_{\\frac{n+1}{2}} & \\text{if } n \\text{ is odd} \\\\\n\\frac{x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1}}{2} & \\text{if } n \\text{ is even}\n\\end{cases}$$\n\n**Properties of the Median:**\n1. **Robustness**: Unaffected by extreme values (outliers)\n2. **Order Preservation**: If all values are multiplied by a positive constant, the median is multiplied by the same constant\n3. **Minimizes Sum of Absolute Deviations**: The median minimizes $\\sum_{i=1}^{n} |x_i - \\text{median}|$\n\n**When to Use:**\n- Data has outliers or is skewed\n- Need a robust measure of central tendency\n- Ordinal data where order matters but differences don't"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_median(data):\n    \"\"\"\n    Calculate median of a dataset\n    \n    Mathematical implementation:\n    - Sort the data\n    - If n is odd: median = middle value\n    - If n is even: median = average of two middle values\n    \n    Parameters:\n    data: array-like, input data\n    \n    Returns:\n    float: median value\n    \"\"\"\n    sorted_data = np.sort(data)\n    n = len(sorted_data)\n    \n    if n == 0:\n        raise ValueError(\"Cannot calculate median of empty dataset\")\n    \n    if n % 2 == 0:\n        # Even number of elements: average of two middle values\n        mid1 = sorted_data[n//2 - 1]\n        mid2 = sorted_data[n//2]\n        return (mid1 + mid2) / 2\n    else:\n        # Odd number of elements: middle value\n        return sorted_data[n//2]\n\n# Example with outliers to demonstrate robustness\ndata_with_outliers = np.append(normal_data, [1000, 2000])\nprint(f\"Mean with outliers: {np.mean(data_with_outliers):.2f}\")\nprint(f\"Median with outliers: {np.median(data_with_outliers):.2f}\")\n\n# Compare mean vs median for skewed data\nprint(f\"Skewed data - Mean: {np.mean(skewed_data):.2f}, Median: {np.median(skewed_data):.2f}\")\n\n# Demonstrate order preservation property\nc = 2\ntransformed_data = c * normal_data\nprint(f\"Original median: {np.median(normal_data):.2f}\")\nprint(f\"Transformed median: {np.median(transformed_data):.2f}\")\nprint(f\"Expected: {c * np.median(normal_data):.2f}\")\n\n# Mathematical verification of median calculation\nsorted_normal = np.sort(normal_data)\nn = len(sorted_normal)\nif n % 2 == 0:\n    median_calc = (sorted_normal[n//2 - 1] + sorted_normal[n//2]) / 2\nelse:\n    median_calc = sorted_normal[n//2]\nprint(f\"Manual median calculation: {median_calc:.2f}\")\nprint(f\"NumPy median: {np.median(normal_data):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mode\n\nThe **mode** is the most frequently occurring value in a dataset. A dataset can have one mode (unimodal), two modes (bimodal), or more modes (multimodal).\n\n**Mathematical Definition:**\nFor discrete data, the mode is the value x that maximizes the frequency function f(x):\n\n$$\\text{Mode} = \\arg\\max_{x} f(x)$$\n\nFor continuous data, the mode is the value that maximizes the probability density function.\n\n**Properties of the Mode:**\n1. **Not Unique**: A dataset can have multiple modes\n2. **Categorical Data**: Works well with nominal and ordinal data\n3. **Peak of Distribution**: Represents the most common value\n\n**When to Use:**\n- Categorical or discrete data\n- Need to identify the most common category\n- Data has clear peaks in distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import mode\n\ndef calculate_mode(data):\n    \"\"\"\n    Calculate mode of a dataset\n    \n    Mathematical implementation:\n    - Count frequency of each unique value\n    - Return value(s) with highest frequency\n    \n    Parameters:\n    data: array-like, input data\n    \n    Returns:\n    tuple: (mode_value, count)\n    \"\"\"\n    return mode(data, keepdims=True)\n\n# Example with categorical data\ncategorical_data = ['A', 'B', 'A', 'C', 'B', 'A', 'D']\nmode_result = calculate_mode(categorical_data)\nprint(f\"Mode: {mode_result.mode[0]} (appears {mode_result.count[0]} times)\")\n\n# Manual calculation for categorical data\nfrom collections import Counter\ncounter = Counter(categorical_data)\nmost_common = counter.most_common(1)[0]\nprint(f\"Manual calculation - Mode: {most_common[0]} (appears {most_common[1]} times)\")\n\n# For continuous data, we can use histogram bins to approximate mode\nhist, bins = np.histogram(normal_data, bins=20)\nmode_bin_index = np.argmax(hist)\nmode_bin_center = (bins[mode_bin_index] + bins[mode_bin_index + 1]) / 2\nprint(f\"Mode bin center: {mode_bin_center:.2f}\")\n\n# Demonstrate multimodal data\nmultimodal_data = np.concatenate([\n    np.random.normal(0, 1, 200),\n    np.random.normal(5, 1, 200),\n    np.random.normal(10, 1, 200)\n])\n\n# Find multiple modes using histogram\nhist_multi, bins_multi = np.histogram(multimodal_data, bins=30)\n# Find local maxima\nfrom scipy.signal import find_peaks\npeaks, _ = find_peaks(hist_multi, height=np.max(hist_multi)*0.5)\nmode_centers = [(bins_multi[i] + bins_multi[i+1])/2 for i in peaks]\nprint(f\"Multiple modes detected at: {[f'{x:.2f}' for x in mode_centers]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Geometric Mean\n\nThe **geometric mean** is useful for data that represents rates of change or multiplicative relationships.\n\n**Mathematical Definition:**\n$$\\text{Geometric Mean} = \\sqrt[n]{x_1 \\times x_2 \\times ... \\times x_n} = \\left(\\prod_{i=1}^{n} x_i\\right)^{\\frac{1}{n}}$$\n\n**Properties:**\n1. **Logarithmic Relationship**: $\\log(\\text{GM}) = \\frac{1}{n}\\sum_{i=1}^{n} \\log(x_i)$\n2. **Multiplicative Data**: Appropriate for growth rates, ratios, and percentages\n3. **Always ≤ Arithmetic Mean**: By the arithmetic mean-geometric mean inequality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def geometric_mean(data):\n    \"\"\"\n    Calculate geometric mean of a dataset\n    \n    Mathematical implementation:\n    GM = (product of all values)^(1/n)\n    \n    Parameters:\n    data: array-like, positive values\n    \n    Returns:\n    float: geometric mean\n    \"\"\"\n    if np.any(data <= 0):\n        raise ValueError(\"Geometric mean requires all positive values\")\n    \n    return np.exp(np.mean(np.log(data)))\n\n# Example with growth rates\ngrowth_rates = [1.05, 1.12, 0.98, 1.08, 1.15]  # 5% growth, 12% growth, etc.\ngm = geometric_mean(growth_rates)\nam = np.mean(growth_rates)\n\nprint(f\"Growth rates: {growth_rates}\")\nprint(f\"Geometric mean: {gm:.4f}\")\nprint(f\"Arithmetic mean: {am:.4f}\")\nprint(f\"GM ≤ AM: {gm <= am}\")\n\n# Demonstrate logarithmic relationship\nlog_gm = np.mean(np.log(growth_rates))\nprint(f\"Log of geometric mean: {log_gm:.4f}\")\nprint(f\"Exp of log mean: {np.exp(log_gm):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Harmonic Mean\n\nThe **harmonic mean** is useful for rates, speeds, and other situations involving reciprocals.\n\n**Mathematical Definition:**\n$$\\text{Harmonic Mean} = \\frac{n}{\\sum_{i=1}^{n} \\frac{1}{x_i}}$$\n\n**Properties:**\n1. **Reciprocal Relationship**: Appropriate for rates and speeds\n2. **Always ≤ Geometric Mean ≤ Arithmetic Mean**: For positive data\n3. **Weighted Version**: $\\text{HM} = \\frac{\\sum w_i}{\\sum \\frac{w_i}{x_i}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def harmonic_mean(data):\n    \"\"\"\n    Calculate harmonic mean of a dataset\n    \n    Mathematical implementation:\n    HM = n / (sum of reciprocals)\n    \n    Parameters:\n    data: array-like, non-zero values\n    \n    Returns:\n    float: harmonic mean\n    \"\"\"\n    if np.any(data == 0):\n        raise ValueError(\"Harmonic mean requires all non-zero values\")\n    \n    return len(data) / np.sum(1 / data)\n\n# Example with speeds\nspeeds = [60, 40, 80]  # km/h for different segments\nhm = harmonic_mean(speeds)\nam = np.mean(speeds)\n\nprint(f\"Speeds: {speeds} km/h\")\nprint(f\"Harmonic mean: {hm:.2f} km/h\")\nprint(f\"Arithmetic mean: {am:.2f} km/h\")\n\n# Demonstrate the relationship: HM ≤ GM ≤ AM\ngm = geometric_mean(speeds)\nprint(f\"Geometric mean: {gm:.2f} km/h\")\nprint(f\"HM ≤ GM ≤ AM: {hm <= gm <= am}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measures of Dispersion\n\nDispersion measures describe how spread out the data is around the central tendency. Understanding dispersion is crucial for assessing the reliability of the central tendency measures and the variability in your data.\n\n### Variance and Standard Deviation\n\n**Variance** measures the average squared deviation from the mean, while **standard deviation** is the square root of variance.\n\n**Mathematical Definition:**\n\n**Population Variance:**\n$$\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} (x_i - \\mu)^2$$\n\n**Sample Variance (Bessel's correction):**\n$$s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2$$\n\n**Standard Deviation:**\n$$\\sigma = \\sqrt{\\sigma^2} \\quad \\text{or} \\quad s = \\sqrt{s^2}$$\n\n**Why n-1 for Sample Variance?**\nThe n-1 correction (Bessel's correction) makes the sample variance an unbiased estimator of the population variance. This is because:\n1. We estimate the population mean μ with the sample mean x̄\n2. This estimation reduces the degrees of freedom by 1\n3. Using n-1 compensates for this reduction\n\n**Properties:**\n1. **Non-negative**: Variance is always ≥ 0\n2. **Scale Dependent**: If we multiply data by c, variance becomes c² times original\n3. **Translation Invariant**: Adding a constant doesn't change variance\n4. **Additive for Independent Variables**: Var(X+Y) = Var(X) + Var(Y) if X,Y independent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_variance(data, sample=True):\n    \"\"\"\n    Calculate variance of a dataset\n    \n    Mathematical implementation:\n    - Calculate mean\n    - Calculate squared deviations from mean\n    - Average the squared deviations\n    - Use n-1 for sample variance (Bessel's correction)\n    \n    Parameters:\n    data: array-like, input data\n    sample: bool, if True use n-1 (sample variance), if False use n (population variance)\n    \n    Returns:\n    float: variance\n    \"\"\"\n    mean_val = np.mean(data)\n    n = len(data)\n    \n    if n == 0:\n        raise ValueError(\"Cannot calculate variance of empty dataset\")\n    \n    squared_deviations = (data - mean_val)**2\n    \n    if sample and n > 1:\n        # Sample variance: use n-1 (Bessel's correction)\n        return np.sum(squared_deviations) / (n - 1)\n    else:\n        # Population variance: use n\n        return np.sum(squared_deviations) / n\n\ndef calculate_std(data, sample=True):\n    \"\"\"\n    Calculate standard deviation of a dataset\n    \n    Mathematical implementation:\n    std = sqrt(variance)\n    \n    Parameters:\n    data: array-like, input data\n    sample: bool, if True use n-1 (sample std), if False use n (population std)\n    \n    Returns:\n    float: standard deviation\n    \"\"\"\n    return np.sqrt(calculate_variance(data, sample))\n\n# Example calculations with detailed explanation\nvariance_normal = calculate_variance(normal_data)\nstd_normal = calculate_std(normal_data)\n\nprint(f\"Variance: {variance_normal:.2f}\")\nprint(f\"Standard Deviation: {std_normal:.2f}\")\nprint(f\"NumPy std: {np.std(normal_data, ddof=1):.2f}\")\n\n# Demonstrate Bessel's correction\nprint(f\"Sample variance (n-1): {np.var(normal_data, ddof=1):.2f}\")\nprint(f\"Population variance (n): {np.var(normal_data, ddof=0):.2f}\")\nprint(f\"Difference: {np.var(normal_data, ddof=1) - np.var(normal_data, ddof=0):.2f}\")\n\n# Mathematical verification\nmean_val = np.mean(normal_data)\nsquared_deviations = (normal_data - mean_val)**2\nmanual_variance = np.sum(squared_deviations) / (len(normal_data) - 1)\nprint(f\"Manual calculation: {manual_variance:.2f}\")\n\n# Demonstrate scale property\nc = 2\nscaled_data = c * normal_data\nprint(f\"Original std: {np.std(normal_data):.2f}\")\nprint(f\"Scaled std: {np.std(scaled_data):.2f}\")\nprint(f\"Expected: {c * np.std(normal_data):.2f}\")\n\n# Demonstrate translation invariance\na = 10\ntranslated_data = normal_data + a\nprint(f\"Original std: {np.std(normal_data):.2f}\")\nprint(f\"Translated std: {np.std(translated_data):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Range and Interquartile Range (IQR)\n\n**Range** is the difference between the maximum and minimum values, while **IQR** is the difference between the 75th and 25th percentiles.\n\n**Mathematical Definition:**\n\n**Range:**\n$$\\text{Range} = x_{max} - x_{min}$$\n\n**IQR:**\n$$\\text{IQR} = Q_3 - Q_1$$\n\nWhere Q₁ (25th percentile) and Q₃ (75th percentile) are defined as:\n- Q₁: Value below which 25% of data falls\n- Q₃: Value below which 75% of data falls\n\n**Properties:**\n1. **Range**: Simple but sensitive to outliers\n2. **IQR**: Robust measure of spread, not affected by outliers\n3. **Percentiles**: Q₁, Q₂ (median), Q₃ provide five-number summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_range(data):\n    \"\"\"\n    Calculate range of a dataset\n    \n    Mathematical implementation:\n    range = max - min\n    \n    Parameters:\n    data: array-like, input data\n    \n    Returns:\n    float: range\n    \"\"\"\n    return np.max(data) - np.min(data)\n\ndef calculate_iqr(data):\n    \"\"\"\n    Calculate interquartile range\n    \n    Mathematical implementation:\n    IQR = Q3 - Q1\n    where Q1 = 25th percentile, Q3 = 75th percentile\n    \n    Parameters:\n    data: array-like, input data\n    \n    Returns:\n    float: interquartile range\n    \"\"\"\n    q1 = np.percentile(data, 25)\n    q3 = np.percentile(data, 75)\n    return q3 - q1\n\n# Example\nprint(f\"Range: {calculate_range(normal_data):.2f}\")\nprint(f\"IQR: {calculate_iqr(normal_data):.2f}\")\n\n# Five-number summary\ndef five_number_summary(data):\n    \"\"\"\n    Calculate five-number summary\n    \n    Mathematical implementation:\n    - Minimum: smallest value\n    - Q1: 25th percentile\n    - Median: 50th percentile\n    - Q3: 75th percentile\n    - Maximum: largest value\n    \n    Parameters:\n    data: array-like, input data\n    \n    Returns:\n    dict: five-number summary\n    \"\"\"\n    return {\n        'min': np.min(data),\n        'q1': np.percentile(data, 25),\n        'median': np.median(data),\n        'q3': np.percentile(data, 75),\n        'max': np.max(data)\n    }\n\nsummary = five_number_summary(normal_data)\nfor key, value in summary.items():\n    print(f\"{key.upper()}: {value:.2f}\")\n\n# Demonstrate robustness of IQR vs Range with outliers\ndata_with_outliers = np.append(normal_data, [1000, 2000])\nprint(f\"Original range: {calculate_range(normal_data):.2f}\")\nprint(f\"Range with outliers: {calculate_range(data_with_outliers):.2f}\")\nprint(f\"Original IQR: {calculate_iqr(normal_data):.2f}\")\nprint(f\"IQR with outliers: {calculate_iqr(data_with_outliers):.2f}\")\n\n# Mathematical verification of percentiles\nsorted_data = np.sort(normal_data)\nn = len(sorted_data)\n\n# Q1 (25th percentile)\nq1_index = 0.25 * (n - 1)\nq1_lower = int(q1_index)\nq1_upper = q1_lower + 1\nq1_weight = q1_index - q1_lower\nq1_manual = (1 - q1_weight) * sorted_data[q1_lower] + q1_weight * sorted_data[q1_upper]\nprint(f\"Manual Q1: {q1_manual:.2f}\")\nprint(f\"NumPy Q1: {np.percentile(normal_data, 25):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Coefficient of Variation (CV)\n\nThe **coefficient of variation** is a standardized measure of dispersion that expresses standard deviation as a percentage of the mean.\n\n**Mathematical Definition:**\n$$\\text{CV} = \\frac{s}{\\bar{x}} \\times 100\\%$$\n\n**Properties:**\n1. **Dimensionless**: Allows comparison across different scales\n2. **Relative Measure**: Shows dispersion relative to the mean\n3. **Useful for**: Comparing variability across different datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def coefficient_of_variation(data):\n    \"\"\"\n    Calculate coefficient of variation\n    \n    Mathematical implementation:\n    CV = (std / mean) * 100%\n    \n    Parameters:\n    data: array-like, input data\n    \n    Returns:\n    float: coefficient of variation as percentage\n    \"\"\"\n    mean_val = np.mean(data)\n    if mean_val == 0:\n        raise ValueError(\"Cannot calculate CV when mean is zero\")\n    \n    std_val = np.std(data, ddof=1)\n    return (std_val / mean_val) * 100\n\n# Example\ncv_normal = coefficient_of_variation(normal_data)\ncv_skewed = coefficient_of_variation(skewed_data)\n\nprint(f\"CV of normal data: {cv_normal:.2f}%\")\nprint(f\"CV of skewed data: {cv_skewed:.2f}%\")\n\n# Compare variability across different scales\nsmall_data = np.random.normal(10, 2, 1000)  # mean=10, std=2\nlarge_data = np.random.normal(1000, 200, 1000)  # mean=1000, std=200\n\ncv_small = coefficient_of_variation(small_data)\ncv_large = coefficient_of_variation(large_data)\n\nprint(f\"Small scale data CV: {cv_small:.2f}%\")\nprint(f\"Large scale data CV: {cv_large:.2f}%\")\nprint(f\"Same relative variability: {abs(cv_small - cv_large) < 0.1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mean Absolute Deviation (MAD)\n\n**Mean absolute deviation** measures the average absolute deviation from the mean.\n\n**Mathematical Definition:**\n$$\\text{MAD} = \\frac{1}{n}\\sum_{i=1}^{n} |x_i - \\bar{x}|$$\n\n**Properties:**\n1. **Robust**: Less sensitive to outliers than variance\n2. **Interpretable**: Same units as original data\n3. **Computationally Simple**: No squaring required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_absolute_deviation(data):\n    \"\"\"\n    Calculate mean absolute deviation\n    \n    Mathematical implementation:\n    MAD = mean of absolute deviations from mean\n    \n    Parameters:\n    data: array-like, input data\n    \n    Returns:\n    float: mean absolute deviation\n    \"\"\"\n    mean_val = np.mean(data)\n    return np.mean(np.abs(data - mean_val))\n\n# Example\nmad_normal = mean_absolute_deviation(normal_data)\nstd_normal = np.std(normal_data, ddof=1)\n\nprint(f\"MAD: {mad_normal:.2f}\")\nprint(f\"Standard deviation: {std_normal:.2f}\")\n\n# Relationship between MAD and standard deviation for normal distribution\n# For normal distribution: MAD ≈ 0.7979 × σ\nexpected_mad = 0.7979 * std_normal\nprint(f\"Expected MAD for normal distribution: {expected_mad:.2f}\")\nprint(f\"Actual MAD: {mad_normal:.2f}\")\nprint(f\"Ratio MAD/σ: {mad_normal/std_normal:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Visualization\n\n### Histograms and Density Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create subplots for comparison\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Histogram\naxes[0, 0].hist(normal_data, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\naxes[0, 0].set_title('Histogram - Normal Distribution')\naxes[0, 0].set_xlabel('Value')\naxes[0, 0].set_ylabel('Frequency')\n\n# Density plot\naxes[0, 1].hist(normal_data, bins=30, density=True, alpha=0.7, color='lightgreen', edgecolor='black')\naxes[0, 1].set_title('Density Plot - Normal Distribution')\naxes[0, 1].set_xlabel('Value')\naxes[0, 1].set_ylabel('Density')\n\n# Skewed distribution\naxes[1, 0].hist(skewed_data, bins=30, alpha=0.7, color='salmon', edgecolor='black')\naxes[1, 0].set_title('Histogram - Skewed Distribution')\naxes[1, 0].set_xlabel('Value')\naxes[1, 0].set_ylabel('Frequency')\n\n# Box plot\naxes[1, 1].boxplot([normal_data, skewed_data], labels=['Normal', 'Skewed'])\naxes[1, 1].set_title('Box Plot Comparison')\naxes[1, 1].set_ylabel('Value')\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Box Plots and Violin Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Box plot\naxes[0].boxplot([normal_data, skewed_data], labels=['Normal', 'Skewed'])\naxes[0].set_title('Box Plot')\naxes[0].set_ylabel('Value')\n\n# Violin plot\naxes[1].violinplot([normal_data, skewed_data], labels=['Normal', 'Skewed'])\naxes[1].set_title('Violin Plot')\naxes[1].set_ylabel('Value')\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Distribution\n\n### Skewness and Kurtosis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_skewness(data):\n    \"\"\"Calculate skewness of a dataset\"\"\"\n    mean_val = np.mean(data)\n    std_val = np.std(data, ddof=1)\n    n = len(data)\n    skewness = (n / ((n-1) * (n-2))) * np.sum(((data - mean_val) / std_val)**3)\n    return skewness\n\ndef calculate_kurtosis(data):\n    \"\"\"Calculate kurtosis of a dataset\"\"\"\n    mean_val = np.mean(data)\n    std_val = np.std(data, ddof=1)\n    n = len(data)\n    kurtosis = (n * (n+1) / ((n-1) * (n-2) * (n-3))) * np.sum(((data - mean_val) / std_val)**4) - (3 * (n-1)**2 / ((n-2) * (n-3)))\n    return kurtosis\n\n# Calculate for our datasets\nprint(f\"Normal distribution - Skewness: {calculate_skewness(normal_data):.3f}\")\nprint(f\"Normal distribution - Kurtosis: {calculate_kurtosis(normal_data):.3f}\")\nprint(f\"Skewed distribution - Skewness: {calculate_skewness(skewed_data):.3f}\")\nprint(f\"Skewed distribution - Kurtosis: {calculate_kurtosis(skewed_data):.3f}\")\n\n# Using SciPy\nprint(f\"SciPy skewness - Normal: {stats.skew(normal_data):.3f}\")\nprint(f\"SciPy kurtosis - Normal: {stats.kurtosis(normal_data):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distribution Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit normal distribution to data\nmu, sigma = stats.norm.fit(normal_data)\nprint(f\"Fitted normal distribution - μ: {mu:.2f}, σ: {sigma:.2f}\")\n\n# Generate fitted curve\nx = np.linspace(normal_data.min(), normal_data.max(), 100)\ny_fitted = stats.norm.pdf(x, mu, sigma)\n\n# Plot fitted distribution\nplt.figure(figsize=(10, 6))\nplt.hist(normal_data, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black', label='Data')\nplt.plot(x, y_fitted, 'r-', linewidth=2, label='Fitted Normal Distribution')\nplt.title('Normal Distribution Fitting')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation Analysis\n\n### Pearson Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_pearson_correlation(x, y):\n    \"\"\"Calculate Pearson correlation coefficient\"\"\"\n    n = len(x)\n    mean_x, mean_y = np.mean(x), np.mean(y)\n    \n    numerator = np.sum((x - mean_x) * (y - mean_y))\n    denominator = np.sqrt(np.sum((x - mean_x)**2) * np.sum((y - mean_y)**2))\n    \n    return numerator / denominator\n\n# Calculate correlations\ncorr_xy = calculate_pearson_correlation(x, y)\ncorr_xz = calculate_pearson_correlation(x, z)\ncorr_yz = calculate_pearson_correlation(y, z)\n\nprint(f\"Correlation X-Y: {corr_xy:.3f}\")\nprint(f\"Correlation X-Z: {corr_xz:.3f}\")\nprint(f\"Correlation Y-Z: {corr_yz:.3f}\")\n\n# Using NumPy\nprint(f\"NumPy correlation X-Y: {np.corrcoef(x, y)[0, 1]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation Matrix and Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlation matrix\ncorrelation_matrix = df[['x', 'y', 'z']].corr()\n\n# Create heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n            square=True, linewidths=0.5)\nplt.title('Correlation Matrix Heatmap')\nplt.show()\n\n# Scatter plot matrix\nsns.pairplot(df[['x', 'y', 'z']], diag_kind='kde')\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Applications\n\n### Data Quality Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_quality_report(df):\n    \"\"\"Generate comprehensive data quality report\"\"\"\n    report = {}\n    \n    for column in df.columns:\n        data = df[column].dropna()\n        \n        report[column] = {\n            'count': len(data),\n            'missing': len(df[column]) - len(data),\n            'missing_pct': (len(df[column]) - len(data)) / len(df[column]) * 100,\n            'mean': np.mean(data),\n            'median': np.median(data),\n            'std': np.std(data, ddof=1),\n            'min': np.min(data),\n            'max': np.max(data),\n            'skewness': stats.skew(data),\n            'kurtosis': stats.kurtosis(data)\n        }\n    \n    return pd.DataFrame(report).T\n\n# Generate report\nquality_report = data_quality_report(df)\nprint(\"Data Quality Report:\")\nprint(quality_report.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Outlier Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_outliers_iqr(data, factor=1.5):\n    \"\"\"Detect outliers using IQR method\"\"\"\n    q1 = np.percentile(data, 25)\n    q3 = np.percentile(data, 75)\n    iqr = q3 - q1\n    \n    lower_bound = q1 - factor * iqr\n    upper_bound = q3 + factor * iqr\n    \n    outliers = data[(data < lower_bound) | (data > upper_bound)]\n    return outliers, lower_bound, upper_bound\n\ndef detect_outliers_zscore(data, threshold=3):\n    \"\"\"Detect outliers using Z-score method\"\"\"\n    z_scores = np.abs(stats.zscore(data))\n    outliers = data[z_scores > threshold]\n    return outliers\n\n# Detect outliers\noutliers_iqr, lb, ub = detect_outliers_iqr(normal_data)\noutliers_zscore = detect_outliers_zscore(normal_data)\n\nprint(f\"IQR outliers: {len(outliers_iqr)} ({len(outliers_iqr)/len(normal_data)*100:.1f}%)\")\nprint(f\"Z-score outliers: {len(outliers_zscore)} ({len(outliers_zscore)/len(normal_data)*100:.1f}%)\")\n\n# Visualize outliers\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.boxplot(normal_data)\nplt.title('Box Plot with Outliers')\nplt.ylabel('Value')\n\nplt.subplot(1, 2, 2)\nz_scores = np.abs(stats.zscore(normal_data))\nplt.scatter(range(len(normal_data)), z_scores, alpha=0.6)\nplt.axhline(y=3, color='r', linestyle='--', label='Threshold (3)')\nplt.title('Z-Score Outlier Detection')\nplt.xlabel('Data Point')\nplt.ylabel('Absolute Z-Score')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics with Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive summary\nprint(\"Comprehensive Summary Statistics:\")\nprint(df.describe())\n\n# Additional statistics\nprint(\"\\nAdditional Statistics:\")\nprint(f\"Skewness:\\n{df.skew()}\")\nprint(f\"Kurtosis:\\n{df.kurtosis()}\")\n\n# Group by statistics (if categorical data available)\n# Example with simulated categories\ndf['category'] = np.random.choice(['A', 'B', 'C'], size=len(df))\nprint(f\"\\nGroup by Statistics:\")\nprint(df.groupby('category')['normal'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Problems\n\n1. **Data Exploration**: Load a real dataset (e.g., from sklearn.datasets) and generate a comprehensive descriptive statistics report.\n\n2. **Outlier Analysis**: Create a function that compares different outlier detection methods and visualizes their results.\n\n3. **Distribution Comparison**: Generate multiple distributions (normal, exponential, uniform) and compare their descriptive statistics.\n\n4. **Correlation Study**: Analyze correlations in a multivariate dataset and create a correlation heatmap with significance levels.\n\n## Further Reading\n\n- \"Statistics in Plain English\" by Timothy C. Urdan\n- \"The Art of Statistics\" by David Spiegelhalter\n- Python Data Science Handbook by Jake VanderPlas\n- NumPy and Pandas documentation for statistical functions\n\n## Key Takeaways\n\n- **Central tendency** measures help understand the typical value in your data\n- **Dispersion** measures indicate how spread out your data is\n- **Visualization** is crucial for understanding data distributions\n- **Correlation** analysis reveals relationships between variables\n- **Data quality** assessment should always precede modeling\n- **Outlier detection** helps identify unusual data points that may need special treatment\n\nIn the next chapter, we'll explore probability fundamentals, which form the foundation for statistical inference and machine learning algorithms."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}