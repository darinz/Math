{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experimental Design\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Pandas](https://img.shields.io/badge/Pandas-1.3+-blue.svg)](https://pandas.pydata.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.4+-orange.svg)](https://matplotlib.org/)\n[![Seaborn](https://img.shields.io/badge/Seaborn-0.11+-blue.svg)](https://seaborn.pydata.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-green.svg)](https://scipy.org/)\n[![Statsmodels](https://img.shields.io/badge/Statsmodels-0.13+-blue.svg)](https://www.statsmodels.org/)\n[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)\n\nExperimental design is crucial for establishing causal relationships and making valid inferences. This chapter covers randomized controlled trials, factorial designs, blocking, and their applications in AI/ML.\n\n## Table of Contents\n- [Randomized Controlled Trials](#randomized-controlled-trials)\n- [Factorial Designs](#factorial-designs)\n- [Blocking and Randomization](#blocking-and-randomization)\n- [Sample Size Determination](#sample-size-determination)\n- [A/B Testing](#ab-testing)\n- [Practical Applications](#practical-applications)\n\n## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import power\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\nnp.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Randomized Controlled Trials\n\n### Mathematical Foundation\n\n**Randomized Controlled Trials (RCTs)** are the gold standard for establishing causal relationships. The key principle is random assignment, which ensures that treatment and control groups are comparable on average across all observed and unobserved characteristics.\n\n**Causal Inference Framework:**\n- **Potential Outcomes**: For each subject i, we define:\n  - $Y_i(1)$: outcome if subject receives treatment\n  - $Y_i(0)$: outcome if subject receives control\n- **Individual Treatment Effect**: $\\tau_i = Y_i(1) - Y_i(0)$\n- **Average Treatment Effect (ATE)**: $\\tau = E[Y_i(1) - Y_i(0)] = E[Y_i(1)] - E[Y_i(0)]$\n\n**Randomization Properties:**\n1. **Unconfoundedness**: $(Y_i(1), Y_i(0)) \\perp T_i$ where $T_i$ is treatment assignment\n2. **Overlap**: $0 < P(T_i = 1) < 1$ for all subjects\n3. **SUTVA**: Stable Unit Treatment Value Assumption (no interference between units)\n\n**Estimation of ATE:**\n$$\\hat{\\tau} = \\bar{Y}_1 - \\bar{Y}_0$$\nwhere $\\bar{Y}_1$ and $\\bar{Y}_0$ are sample means of treated and control groups.\n\n**Standard Error:**\n$$SE(\\hat{\\tau}) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_0^2}{n_0}}$$\nwhere $s_1^2, s_0^2$ are sample variances and $n_1, n_0$ are sample sizes.\n\n### Basic RCT Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_rct(n_treatment=50, n_control=50, treatment_effect=5, noise=2):\n    \"\"\"\n    Simulate a randomized controlled trial\n    \n    Mathematical implementation:\n    1. Generate potential outcomes: Y(0) ~ N(μ₀, σ²), Y(1) = Y(0) + τ\n    2. Randomly assign treatment: T ~ Bernoulli(p) where p = n₁/(n₀ + n₁)\n    3. Observe outcomes: Y = T*Y(1) + (1-T)*Y(0)\n    4. Estimate ATE: τ̂ = Ȳ₁ - Ȳ₀\n    \n    Parameters:\n    n_treatment: int, size of treatment group\n    n_control: int, size of control group\n    treatment_effect: float, true treatment effect\n    noise: float, standard deviation of outcomes\n    \n    Returns:\n    tuple: (DataFrame, true_effect)\n    \"\"\"\n    n_total = n_treatment + n_control\n    \n    # Step 1: Generate potential outcomes\n    # Control potential outcomes: Y(0) ~ N(100, noise²)\n    y0 = np.random.normal(100, noise, n_total)\n    \n    # Treatment potential outcomes: Y(1) = Y(0) + treatment_effect\n    y1 = y0 + treatment_effect\n    \n    # Step 2: Random treatment assignment\n    # Probability of treatment assignment\n    p_treatment = n_treatment / n_total\n    \n    # Random assignment\n    treatment_assignment = np.random.binomial(1, p_treatment, n_total)\n    \n    # Step 3: Observe outcomes (SUTVA assumption)\n    observed_outcomes = treatment_assignment * y1 + (1 - treatment_assignment) * y0\n    \n    # Create DataFrame\n    df_rct = pd.DataFrame({\n        'subject_id': range(n_total),\n        'treatment_assignment': treatment_assignment,\n        'y0_potential': y0,\n        'y1_potential': y1,\n        'observed_outcome': observed_outcomes,\n        'group': ['treatment' if t == 1 else 'control' for t in treatment_assignment]\n    })\n    \n    return df_rct, treatment_effect\n\ndf_rct, true_effect = simulate_rct()\n\nprint(\"Randomized Controlled Trial Simulation\")\nprint(f\"Control group size: {len(df_rct[df_rct['group'] == 'control'])}\")\nprint(f\"Treatment group size: {len(df_rct[df_rct['group'] == 'treatment'])}\")\nprint(f\"True treatment effect: {true_effect}\")\n\n# Verify randomization balance\nprint(f\"\\nRandomization Balance Check:\")\nprint(f\"Treatment assignment probability: {df_rct['treatment_assignment'].mean():.3f}\")\nprint(f\"Expected probability: {len(df_rct[df_rct['group'] == 'treatment']) / len(df_rct):.3f}\")\n\n# Analyze RCT results\ndef analyze_rct(data):\n    \"\"\"\n    Analyze RCT results with detailed statistical calculations\n    \n    Mathematical steps:\n    1. Calculate group means: Ȳ₁, Ȳ₀\n    2. Estimate ATE: τ̂ = Ȳ₁ - Ȳ₀\n    3. Calculate standard error: SE(τ̂) = √(s₁²/n₁ + s₀²/n₀)\n    4. Perform t-test: t = τ̂ / SE(τ̂)\n    5. Calculate confidence interval: τ̂ ± t_{α/2,df} × SE(τ̂)\n    \n    Parameters:\n    data: DataFrame, RCT data\n    \n    Returns:\n    dict: analysis results\n    \"\"\"\n    control_data = data[data['group'] == 'control']['observed_outcome']\n    treatment_data = data[data['group'] == 'treatment']['observed_outcome']\n    \n    n0, n1 = len(control_data), len(treatment_data)\n    \n    # Step 1: Calculate group means\n    control_mean = control_data.mean()\n    treatment_mean = treatment_data.mean()\n    \n    # Step 2: Estimate ATE\n    estimated_effect = treatment_mean - control_mean\n    \n    # Step 3: Calculate standard error\n    # Pooled variance estimator (assuming equal variances)\n    pooled_var = ((n0 - 1) * control_data.var() + (n1 - 1) * treatment_data.var()) / (n0 + n1 - 2)\n    se_diff = np.sqrt(pooled_var * (1/n0 + 1/n1))\n    \n    # Step 4: Perform t-test\n    t_stat = estimated_effect / se_diff\n    df = n0 + n1 - 2  # degrees of freedom\n    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df))\n    \n    # Step 5: Calculate confidence interval\n    t_critical = stats.t.ppf(0.975, df)  # 95% CI\n    ci_lower = estimated_effect - t_critical * se_diff\n    ci_upper = estimated_effect + t_critical * se_diff\n    \n    # Step 6: Calculate effect size (Cohen's d)\n    cohens_d = estimated_effect / np.sqrt(pooled_var)\n    \n    # Step 7: Calculate power (post-hoc)\n    # For a two-sample t-test with equal sample sizes\n    effect_size_for_power = abs(estimated_effect) / np.sqrt(pooled_var)\n    power_achieved = stats.t.cdf(stats.t.ppf(0.975, df) - effect_size_for_power * np.sqrt(n0/2), df)\n    \n    return {\n        'control_mean': control_mean,\n        'treatment_mean': treatment_mean,\n        'estimated_effect': estimated_effect,\n        'standard_error': se_diff,\n        't_statistic': t_stat,\n        'degrees_of_freedom': df,\n        'p_value': p_value,\n        'confidence_interval': (ci_lower, ci_upper),\n        'cohens_d': cohens_d,\n        'power_achieved': power_achieved,\n        'pooled_variance': pooled_var\n    }\n\nrct_results = analyze_rct(df_rct)\n\nprint(f\"\\nRCT Analysis Results:\")\nprint(f\"Control mean: {rct_results['control_mean']:.2f}\")\nprint(f\"Treatment mean: {rct_results['treatment_mean']:.2f}\")\nprint(f\"Estimated ATE: {rct_results['estimated_effect']:.2f}\")\nprint(f\"Standard error: {rct_results['standard_error']:.3f}\")\nprint(f\"t-statistic: {rct_results['t_statistic']:.3f}\")\nprint(f\"Degrees of freedom: {rct_results['degrees_of_freedom']}\")\nprint(f\"p-value: {rct_results['p_value']:.4f}\")\nprint(f\"95% CI: [{rct_results['confidence_interval'][0]:.2f}, {rct_results['confidence_interval'][1]:.2f}]\")\nprint(f\"Cohen's d: {rct_results['cohens_d']:.3f}\")\nprint(f\"Power achieved: {rct_results['power_achieved']:.3f}\")\n\n# Verify potential outcomes framework\nprint(f\"\\nPotential Outcomes Verification:\")\nprint(f\"True ATE: {true_effect}\")\nprint(f\"Estimated ATE: {rct_results['estimated_effect']:.2f}\")\nprint(f\"Bias: {rct_results['estimated_effect'] - true_effect:.3f}\")\n\n# Visualize RCT results\nplt.figure(figsize=(15, 5))\n\n# Box plot\nplt.subplot(1, 3, 1)\nsns.boxplot(data=df_rct, x='group', y='observed_outcome')\nplt.title('RCT Results - Box Plot')\nplt.ylabel('Observed Outcome')\n\n# Histogram\nplt.subplot(1, 3, 2)\ncontrol_data = df_rct[df_rct['group'] == 'control']['observed_outcome']\ntreatment_data = df_rct[df_rct['group'] == 'treatment']['observed_outcome']\n\nplt.hist(control_data, alpha=0.7, label='Control', bins=15, density=True)\nplt.hist(treatment_data, alpha=0.7, label='Treatment', bins=15, density=True)\nplt.xlabel('Observed Outcome')\nplt.ylabel('Density')\nplt.title('RCT Results - Histogram')\nplt.legend()\n\n# Effect size distribution\nplt.subplot(1, 3, 3)\neffect_sizes = []\nfor _ in range(1000):\n    # Bootstrap resampling\n    control_boot = np.random.choice(control_data, size=len(control_data), replace=True)\n    treatment_boot = np.random.choice(treatment_data, size=len(treatment_data), replace=True)\n    effect_sizes.append(treatment_boot.mean() - control_boot.mean())\n\nplt.hist(effect_sizes, bins=30, alpha=0.7, color='green', edgecolor='black', density=True)\nplt.axvline(true_effect, color='red', linestyle='--', linewidth=2, label=f'True effect: {true_effect}')\nplt.axvline(rct_results['estimated_effect'], color='blue', linestyle='--', linewidth=2, \n           label=f'Estimated effect: {rct_results[\"estimated_effect\"]:.2f}')\nplt.xlabel('Treatment Effect')\nplt.ylabel('Density')\nplt.title('Bootstrap Distribution of Treatment Effect')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stratified Randomization\n\n**Mathematical Concept:**\nStratified randomization ensures balance across important covariates by performing separate randomizations within each stratum.\n\n**Stratified ATE Estimation:**\n$$\\hat{\\tau}_{stratified} = \\sum_{s=1}^{S} w_s \\hat{\\tau}_s$$\nwhere $w_s$ is the weight for stratum s (usually proportional to stratum size) and $\\hat{\\tau}_s$ is the estimated treatment effect in stratum s.\n\n**Variance of Stratified Estimator:**\n$$Var(\\hat{\\tau}_{stratified}) = \\sum_{s=1}^{S} w_s^2 Var(\\hat{\\tau}_s)$$\n\n**Benefits:**\n1. **Reduced Variance**: More precise estimates when strata are homogeneous\n2. **Guaranteed Balance**: Ensures treatment groups are balanced on stratifying variables\n3. **Subgroup Analysis**: Enables analysis of treatment effects within strata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stratified_rct(n_per_stratum=25, n_strata=4):\n    \"\"\"\n    Simulate stratified randomized controlled trial\n    \n    Mathematical implementation:\n    1. Define strata (e.g., age groups, severity levels)\n    2. Within each stratum s:\n       - Generate potential outcomes: Yₛ(0) ~ N(μₛ, σ²), Yₛ(1) = Yₛ(0) + τₛ\n       - Randomly assign treatment: T ~ Bernoulli(0.5) within stratum\n    3. Estimate stratified ATE: τ̂ = Σ wₛ τ̂ₛ\n    \n    Parameters:\n    n_per_stratum: int, sample size per stratum per group\n    n_strata: int, number of strata\n    \n    Returns:\n    DataFrame: stratified RCT data\n    \"\"\"\n    strata = []\n    outcomes = []\n    groups = []\n    treatment_assignments = []\n    stratum_effects = []\n    \n    for stratum in range(n_strata):\n        # Different baseline outcomes for each stratum\n        baseline = 90 + stratum * 5\n        \n        # Different treatment effects for each stratum\n        treatment_effect = 3 + stratum * 2\n        \n        # Generate potential outcomes for this stratum\n        y0_stratum = np.random.normal(baseline, 3, n_per_stratum * 2)\n        y1_stratum = y0_stratum + treatment_effect\n        \n        # Random treatment assignment within stratum\n        treatment_assignment = np.random.binomial(1, 0.5, n_per_stratum * 2)\n        \n        # Observe outcomes\n        observed_outcomes = treatment_assignment * y1_stratum + (1 - treatment_assignment) * y0_stratum\n        \n        # Store data\n        strata.extend([stratum] * n_per_stratum * 2)\n        outcomes.extend(observed_outcomes)\n        groups.extend(['treatment' if t == 1 else 'control' for t in treatment_assignment])\n        treatment_assignments.extend(treatment_assignment)\n        stratum_effects.extend([treatment_effect] * n_per_stratum * 2)\n    \n    df_stratified = pd.DataFrame({\n        'stratum': strata,\n        'group': groups,\n        'outcome': outcomes,\n        'treatment_assignment': treatment_assignments,\n        'stratum_effect': stratum_effects\n    })\n    \n    return df_stratified\n\ndf_stratified = stratified_rct()\n\nprint(\"Stratified RCT Simulation\")\nprint(f\"Total sample size: {len(df_stratified)}\")\nprint(f\"Number of strata: {df_stratified['stratum'].nunique()}\")\nprint(f\"Sample size per stratum: {len(df_stratified) // (df_stratified['stratum'].nunique() * 2)}\")\n\n# Analyze stratified RCT\ndef analyze_stratified_rct(data):\n    \"\"\"\n    Analyze stratified RCT results\n    \n    Mathematical steps:\n    1. Calculate stratum-specific treatment effects: τ̂ₛ = Ȳ₁ₛ - Ȳ₀ₛ\n    2. Calculate stratum weights: wₛ = nₛ / N\n    3. Estimate overall ATE: τ̂ = Σ wₛ τ̂ₛ\n    4. Calculate variance: Var(τ̂) = Σ wₛ² Var(τ̂ₛ)\n    \n    Parameters:\n    data: DataFrame, stratified RCT data\n    \n    Returns:\n    dict: analysis results\n    \"\"\"\n    strata = data['stratum'].unique()\n    stratum_effects = []\n    stratum_variances = []\n    stratum_weights = []\n    \n    for stratum in strata:\n        stratum_data = data[data['stratum'] == stratum]\n        control_data = stratum_data[stratum_data['group'] == 'control']['outcome']\n        treatment_data = stratum_data[stratum_data['group'] == 'treatment']['outcome']\n        \n        # Stratum-specific treatment effect\n        stratum_effect = treatment_data.mean() - control_data.mean()\n        stratum_effects.append(stratum_effect)\n        \n        # Stratum-specific variance\n        n0_s, n1_s = len(control_data), len(treatment_data)\n        pooled_var_s = ((n0_s - 1) * control_data.var() + (n1_s - 1) * treatment_data.var()) / (n0_s + n1_s - 2)\n        stratum_var = pooled_var_s * (1/n0_s + 1/n1_s)\n        stratum_variances.append(stratum_var)\n        \n        # Stratum weight\n        stratum_weight = len(stratum_data) / len(data)\n        stratum_weights.append(stratum_weight)\n    \n    # Overall stratified estimate\n    stratified_ate = np.sum(np.array(stratum_weights) * np.array(stratum_effects))\n    \n    # Overall variance\n    stratified_var = np.sum(np.array(stratum_weights)**2 * np.array(stratum_variances))\n    stratified_se = np.sqrt(stratified_var)\n    \n    # Statistical test\n    t_stat = stratified_ate / stratified_se\n    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), len(data) - 2))\n    \n    # Confidence interval\n    t_critical = stats.t.ppf(0.975, len(data) - 2)\n    ci_lower = stratified_ate - t_critical * stratified_se\n    ci_upper = stratified_ate + t_critical * stratified_se\n    \n    return {\n        'stratum_effects': stratum_effects,\n        'stratum_weights': stratum_weights,\n        'stratified_ate': stratified_ate,\n        'stratified_se': stratified_se,\n        't_statistic': t_stat,\n        'p_value': p_value,\n        'confidence_interval': (ci_lower, ci_upper)\n    }\n\nstratified_results = analyze_stratified_rct(df_stratified)\n\nprint(f\"\\nStratified RCT Analysis Results:\")\nfor i, (effect, weight) in enumerate(zip(stratified_results['stratum_effects'], stratified_results['stratum_weights'])):\n    print(f\"Stratum {i}: Effect = {effect:.2f}, Weight = {weight:.3f}\")\nprint(f\"Overall stratified ATE: {stratified_results['stratified_ate']:.2f}\")\nprint(f\"Standard error: {stratified_results['stratified_se']:.3f}\")\nprint(f\"t-statistic: {stratified_results['t_statistic']:.3f}\")\nprint(f\"p-value: {stratified_results['p_value']:.4f}\")\nprint(f\"95% CI: [{stratified_results['confidence_interval'][0]:.2f}, {stratified_results['confidence_interval'][1]:.2f}]\")\n\n# Compare with unstratified analysis\nunstratified_results = analyze_rct(df_stratified)\nprint(f\"\\nComparison:\")\nprint(f\"Unstratified ATE: {unstratified_results['estimated_effect']:.2f}\")\nprint(f\"Stratified ATE: {stratified_results['stratified_ate']:.2f}\")\nprint(f\"Unstratified SE: {unstratified_results['standard_error']:.3f}\")\nprint(f\"Stratified SE: {stratified_results['stratified_se']:.3f}\")\nprint(f\"Efficiency gain: {(unstratified_results['standard_error'] / stratified_results['stratified_se'])**2:.2f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Factorial Designs\n\n### 2x2 Factorial Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def factorial_2x2_design(n_per_cell=30):\n    \"\"\"Simulate 2x2 factorial design\"\"\"\n    \n    # Factors: A (drug) and B (dose)\n    factor_a = ['low', 'high']  # Drug levels\n    factor_b = ['low', 'high']  # Dose levels\n    \n    # True effects\n    baseline = 100\n    effect_a = 5    # Main effect of drug\n    effect_b = 3    # Main effect of dose\n    effect_ab = 2   # Interaction effect\n    \n    data = []\n    \n    for i, a in enumerate(factor_a):\n        for j, b in enumerate(factor_b):\n            # Calculate cell mean\n            cell_mean = baseline\n            if a == 'high':\n                cell_mean += effect_a\n            if b == 'high':\n                cell_mean += effect_b\n            if a == 'high' and b == 'high':\n                cell_mean += effect_ab\n            \n            # Generate outcomes\n            outcomes = np.random.normal(cell_mean, 2, n_per_cell)\n            \n            for outcome in outcomes:\n                data.append({\n                    'factor_a': a,\n                    'factor_b': b,\n                    'outcome': outcome\n                })\n    \n    return pd.DataFrame(data)\n\ndf_factorial = factorial_2x2_design()\n\nprint(\"2x2 Factorial Design Simulation\")\nprint(f\"Total sample size: {len(df_factorial)}\")\nprint(f\"Sample size per cell: {len(df_factorial) // 4}\")\n\n# Analyze factorial design\ndef analyze_factorial_design(data):\n    \"\"\"Analyze factorial design using ANOVA\"\"\"\n    \n    from scipy.stats import f_oneway\n    from itertools import combinations\n    \n    # Cell means\n    cell_means = data.groupby(['factor_a', 'factor_b'])['outcome'].mean()\n    \n    # Main effects\n    main_effect_a = (cell_means.loc[('high', 'low')] + cell_means.loc[('high', 'high')]) / 2 - \\\n                   (cell_means.loc[('low', 'low')] + cell_means.loc[('low', 'high')]) / 2\n    \n    main_effect_b = (cell_means.loc[('low', 'high')] + cell_means.loc[('high', 'high')]) / 2 - \\\n                   (cell_means.loc[('low', 'low')] + cell_means.loc[('high', 'low')]) / 2\n    \n    # Interaction effect\n    interaction = (cell_means.loc[('high', 'high')] - cell_means.loc[('high', 'low')]) - \\\n                 (cell_means.loc[('low', 'high')] - cell_means.loc[('low', 'low')])\n    \n    # ANOVA\n    groups = [group['outcome'].values for name, group in data.groupby(['factor_a', 'factor_b'])]\n    f_stat, p_value = f_oneway(*groups)\n    \n    return {\n        'cell_means': cell_means,\n        'main_effect_a': main_effect_a,\n        'main_effect_b': main_effect_b,\n        'interaction': interaction,\n        'f_statistic': f_stat,\n        'p_value': p_value\n    }\n\nfactorial_results = analyze_factorial_design(df_factorial)\n\nprint(f\"\\nFactorial Design Analysis:\")\nprint(f\"Main effect A (drug): {factorial_results['main_effect_a']:.2f}\")\nprint(f\"Main effect B (dose): {factorial_results['main_effect_b']:.2f}\")\nprint(f\"Interaction effect: {factorial_results['interaction']:.2f}\")\nprint(f\"F-statistic: {factorial_results['f_statistic']:.3f}\")\nprint(f\"p-value: {factorial_results['p_value']:.4f}\")\n\n# Visualize factorial design\nplt.figure(figsize=(15, 5))\n\n# Interaction plot\nplt.subplot(1, 3, 1)\nfor factor_b in ['low', 'high']:\n    subset = df_factorial[df_factorial['factor_b'] == factor_b]\n    means = subset.groupby('factor_a')['outcome'].mean()\n    plt.plot(['low', 'high'], means, 'o-', label=f'Dose {factor_b}', linewidth=2, markersize=8)\n\nplt.xlabel('Drug Level')\nplt.ylabel('Outcome')\nplt.title('Interaction Plot')\nplt.legend()\n\n# Cell means heatmap\nplt.subplot(1, 3, 2)\npivot_table = df_factorial.pivot_table(values='outcome', index='factor_a', columns='factor_b', aggfunc='mean')\nsns.heatmap(pivot_table, annot=True, cmap='coolwarm', center=pivot_table.values.mean(), \n            square=True, linewidths=0.5, fmt='.1f')\nplt.title('Cell Means')\n\n# Main effects\nplt.subplot(1, 3, 3)\neffects = ['Main Effect A', 'Main Effect B', 'Interaction']\neffect_values = [factorial_results['main_effect_a'], \n                factorial_results['main_effect_b'], \n                factorial_results['interaction']]\n\ncolors = ['red' if abs(effect) > 1 else 'blue' for effect in effect_values]\nplt.bar(effects, effect_values, color=colors, alpha=0.7)\nplt.axhline(0, color='black', linestyle='-', alpha=0.7)\nplt.ylabel('Effect Size')\nplt.title('Main Effects and Interaction')\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Blocking and Randomization\n\n### Randomized Block Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def randomized_block_design(n_blocks=6, n_treatments=3):\n    \"\"\"Simulate randomized block design\"\"\"\n    \n    # Generate blocks (e.g., different centers, time periods)\n    blocks = []\n    treatments = []\n    outcomes = []\n    \n    for block in range(n_blocks):\n        # Block-specific baseline\n        baseline = 100 + block * 2\n        \n        # Randomize treatments within each block\n        block_treatments = np.random.permutation(n_treatments)\n        \n        for treatment in range(n_treatments):\n            # Treatment effects\n            treatment_effect = treatment * 3\n            \n            # Generate outcome\n            outcome = baseline + treatment_effect + np.random.normal(0, 2)\n            \n            blocks.append(block)\n            treatments.append(treatment)\n            outcomes.append(outcome)\n    \n    df_block = pd.DataFrame({\n        'block': blocks,\n        'treatment': treatments,\n        'outcome': outcomes\n    })\n    \n    return df_block\n\ndf_block = randomized_block_design()\n\nprint(\"Randomized Block Design Simulation\")\nprint(f\"Number of blocks: {df_block['block'].nunique()}\")\nprint(f\"Number of treatments: {df_block['treatment'].nunique()}\")\nprint(f\"Total sample size: {len(df_block)}\")\n\n# Analyze block design\ndef analyze_block_design(data):\n    \"\"\"Analyze randomized block design\"\"\"\n    \n    # Calculate means\n    overall_mean = data['outcome'].mean()\n    block_means = data.groupby('block')['outcome'].mean()\n    treatment_means = data.groupby('treatment')['outcome'].mean()\n    \n    # Calculate effects\n    block_effects = block_means - overall_mean\n    treatment_effects = treatment_means - overall_mean\n    \n    # Two-way ANOVA (treatments and blocks)\n    from scipy.stats import f_oneway\n    \n    # Treatment groups\n    treatment_groups = [group['outcome'].values for name, group in data.groupby('treatment')]\n    f_treatment, p_treatment = f_oneway(*treatment_groups)\n    \n    # Block groups\n    block_groups = [group['outcome'].values for name, group in data.groupby('block')]\n    f_block, p_block = f_oneway(*block_groups)\n    \n    return {\n        'overall_mean': overall_mean,\n        'block_effects': block_effects,\n        'treatment_effects': treatment_effects,\n        'f_treatment': f_treatment,\n        'p_treatment': p_treatment,\n        'f_block': f_block,\n        'p_block': p_block\n    }\n\nblock_results = analyze_block_design(df_block)\n\nprint(f\"\\nBlock Design Analysis:\")\nprint(f\"Overall mean: {block_results['overall_mean']:.2f}\")\nprint(f\"Treatment F-statistic: {block_results['f_treatment']:.3f}\")\nprint(f\"Treatment p-value: {block_results['p_treatment']:.4f}\")\nprint(f\"Block F-statistic: {block_results['f_block']:.3f}\")\nprint(f\"Block p-value: {block_results['p_block']:.4f}\")\n\n# Visualize block design\nplt.figure(figsize=(15, 5))\n\n# Treatment means\nplt.subplot(1, 3, 1)\ntreatment_means = df_block.groupby('treatment')['outcome'].mean()\nplt.bar(treatment_means.index, treatment_means.values, alpha=0.7, color='skyblue')\nplt.xlabel('Treatment')\nplt.ylabel('Mean Outcome')\nplt.title('Treatment Means')\n\n# Block means\nplt.subplot(1, 3, 2)\nblock_means = df_block.groupby('block')['outcome'].mean()\nplt.bar(block_means.index, block_means.values, alpha=0.7, color='lightgreen')\nplt.xlabel('Block')\nplt.ylabel('Mean Outcome')\nplt.title('Block Means')\n\n# Interaction plot\nplt.subplot(1, 3, 3)\nfor treatment in df_block['treatment'].unique():\n    subset = df_block[df_block['treatment'] == treatment]\n    means = subset.groupby('block')['outcome'].mean()\n    plt.plot(means.index, means.values, 'o-', label=f'Treatment {treatment}', linewidth=2, markersize=6)\n\nplt.xlabel('Block')\nplt.ylabel('Outcome')\nplt.title('Treatment Effects Across Blocks')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Size Determination\n\n### Power Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def power_analysis_example():\n    \"\"\"Demonstrate power analysis for different scenarios\"\"\"\n    \n    # Parameters\n    alpha = 0.05  # Significance level\n    power_levels = [0.8, 0.9, 0.95]  # Desired power levels\n    effect_sizes = [0.2, 0.5, 0.8]   # Cohen's d effect sizes\n    \n    results = []\n    \n    for power in power_levels:\n        for effect_size in effect_sizes:\n            # Calculate required sample size\n            n_per_group = power.tt_ind_solve_power(\n                effect_size=effect_size,\n                alpha=alpha,\n                power=power,\n                ratio=1.0  # Equal group sizes\n            )\n            \n            results.append({\n                'power': power,\n                'effect_size': effect_size,\n                'n_per_group': int(n_per_group),\n                'total_n': int(n_per_group * 2)\n            })\n    \n    return pd.DataFrame(results)\n\npower_results = power_analysis_example()\n\nprint(\"Power Analysis Results\")\nprint(power_results.to_string(index=False))\n\n# Visualize power analysis\nplt.figure(figsize=(15, 5))\n\n# Sample size vs effect size\nplt.subplot(1, 3, 1)\nfor power in power_results['power'].unique():\n    subset = power_results[power_results['power'] == power]\n    plt.plot(subset['effect_size'], subset['n_per_group'], 'o-', \n             label=f'Power = {power}', linewidth=2, markersize=8)\n\nplt.xlabel(\"Cohen's d Effect Size\")\nplt.ylabel('Sample Size per Group')\nplt.title('Sample Size vs Effect Size')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Power curves\nplt.subplot(1, 3, 2)\nsample_sizes = [20, 50, 100, 200]\neffect_sizes = np.linspace(0.1, 1.0, 50)\n\nfor n in sample_sizes:\n    powers = []\n    for effect_size in effect_sizes:\n        power = power.tt_ind_power(effect_size, n, n, alpha=0.05)\n        powers.append(power)\n    plt.plot(effect_sizes, powers, label=f'n = {n}', linewidth=2)\n\nplt.xlabel(\"Cohen's d Effect Size\")\nplt.ylabel('Power')\nplt.title('Power Curves')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Effect size detection\nplt.subplot(1, 3, 3)\nn_values = np.arange(10, 201, 10)\nmin_effect_sizes = []\n\nfor n in n_values:\n    # Find minimum detectable effect size for 80% power\n    min_effect = power.tt_ind_solve_power(\n        effect_size=None,\n        alpha=0.05,\n        power=0.8,\n        nobs1=n\n    )\n    min_effect_sizes.append(min_effect)\n\nplt.plot(n_values, min_effect_sizes, 'b-', linewidth=2)\nplt.xlabel('Sample Size per Group')\nplt.ylabel('Minimum Detectable Effect Size')\nplt.title('Minimum Detectable Effect Size')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Practical example\ndef sample_size_calculation_example():\n    \"\"\"Calculate sample size for a practical example\"\"\"\n    \n    # Scenario: Testing a new drug vs placebo\n    # Expected effect size: 0.5 (medium effect)\n    # Desired power: 0.9\n    # Significance level: 0.05\n    \n    effect_size = 0.5\n    desired_power = 0.9\n    alpha = 0.05\n    \n    n_per_group = power.tt_ind_solve_power(\n        effect_size=effect_size,\n        alpha=alpha,\n        power=desired_power,\n        ratio=1.0\n    )\n    \n    total_n = n_per_group * 2\n    \n    print(f\"Sample Size Calculation Example:\")\n    print(f\"Effect size (Cohen's d): {effect_size}\")\n    print(f\"Desired power: {desired_power}\")\n    print(f\"Significance level: {alpha}\")\n    print(f\"Required sample size per group: {int(n_per_group)}\")\n    print(f\"Total sample size: {int(total_n)}\")\n    \n    return int(n_per_group), int(total_n)\n\nsample_size_example = sample_size_calculation_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A/B Testing\n\n### A/B Test Design and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ab_test_simulation(n_a=1000, n_b=1000, true_rate_a=0.10, true_rate_b=0.12):\n    \"\"\"Simulate A/B test with conversion rates\"\"\"\n    \n    # Generate data\n    conversions_a = np.random.binomial(n_a, true_rate_a)\n    conversions_b = np.random.binomial(n_b, true_rate_b)\n    \n    # Calculate observed rates\n    rate_a = conversions_a / n_a\n    rate_b = conversions_b / n_b\n    \n    # Statistical test\n    from scipy.stats import proportions_ztest\n    z_stat, p_value = proportions_ztest([conversions_a, conversions_b], [n_a, n_b])\n    \n    # Confidence interval for difference\n    pooled_rate = (conversions_a + conversions_b) / (n_a + n_b)\n    se_diff = np.sqrt(pooled_rate * (1 - pooled_rate) * (1/n_a + 1/n_b))\n    diff = rate_b - rate_a\n    ci_lower = diff - 1.96 * se_diff\n    ci_upper = diff + 1.96 * se_diff\n    \n    return {\n        'conversions_a': conversions_a,\n        'conversions_b': conversions_b,\n        'rate_a': rate_a,\n        'rate_b': rate_b,\n        'difference': diff,\n        'z_statistic': z_stat,\n        'p_value': p_value,\n        'confidence_interval': (ci_lower, ci_upper)\n    }\n\nab_results = ab_test_simulation()\n\nprint(\"A/B Test Results\")\nprint(f\"Group A: {ab_results['conversions_a']}/{1000} conversions ({ab_results['rate_a']:.3f})\")\nprint(f\"Group B: {ab_results['conversions_b']}/{1000} conversions ({ab_results['rate_b']:.3f})\")\nprint(f\"Difference: {ab_results['difference']:.4f}\")\nprint(f\"Z-statistic: {ab_results['z_statistic']:.3f}\")\nprint(f\"P-value: {ab_results['p_value']:.4f}\")\nprint(f\"95% CI: [{ab_results['confidence_interval'][0]:.4f}, {ab_results['confidence_interval'][1]:.4f}]\")\n\n# Sequential A/B testing\ndef sequential_ab_test(n_max=2000, true_rate_a=0.10, true_rate_b=0.12, alpha=0.05):\n    \"\"\"Simulate sequential A/B testing\"\"\"\n    \n    conversions_a = []\n    conversions_b = []\n    decisions = []\n    sample_sizes = []\n    \n    for n in range(100, n_max + 1, 100):\n        # Generate data up to current n\n        conv_a = np.random.binomial(n//2, true_rate_a)\n        conv_b = np.random.binomial(n//2, true_rate_b)\n        \n        conversions_a.append(conv_a)\n        conversions_b.append(conv_b)\n        \n        # Statistical test\n        z_stat, p_value = proportions_ztest([conv_a, conv_b], [n//2, n//2])\n        \n        # Decision rule\n        if p_value < alpha:\n            decision = 'reject' if z_stat > 0 else 'reject'\n        else:\n            decision = 'continue'\n        \n        decisions.append(decision)\n        sample_sizes.append(n)\n    \n    return sample_sizes, conversions_a, conversions_b, decisions\n\nsample_sizes, conv_a_seq, conv_b_seq, decisions = sequential_ab_test()\n\n# Visualize sequential testing\nplt.figure(figsize=(15, 5))\n\n# Conversion rates over time\nplt.subplot(1, 3, 1)\nrates_a = [conv_a / (n//2) for conv_a, n in zip(conv_a_seq, sample_sizes)]\nrates_b = [conv_b / (n//2) for conv_b, n in zip(conv_b_seq, sample_sizes)]\n\nplt.plot(sample_sizes, rates_a, 'b-', label='Group A', linewidth=2)\nplt.plot(sample_sizes, rates_b, 'r-', label='Group B', linewidth=2)\nplt.xlabel('Sample Size')\nplt.ylabel('Conversion Rate')\nplt.title('Sequential A/B Test - Conversion Rates')\nplt.legend()\n\n# P-values over time\nplt.subplot(1, 3, 2)\np_values = []\nfor conv_a, conv_b, n in zip(conv_a_seq, conv_b_seq, sample_sizes):\n    z_stat, p_val = proportions_ztest([conv_a, conv_b], [n//2, n//2])\n    p_values.append(p_val)\n\nplt.plot(sample_sizes, p_values, 'g-', linewidth=2)\nplt.axhline(0.05, color='red', linestyle='--', alpha=0.7, label='α = 0.05')\nplt.xlabel('Sample Size')\nplt.ylabel('P-value')\nplt.title('Sequential A/B Test - P-values')\nplt.legend()\n\n# Decision timeline\nplt.subplot(1, 3, 3)\ndecision_colors = {'continue': 'yellow', 'reject': 'red'}\ncolors = [decision_colors[decision] for decision in decisions]\n\nplt.scatter(sample_sizes, [1] * len(sample_sizes), c=colors, alpha=0.7, s=50)\nplt.xlabel('Sample Size')\nplt.ylabel('Decision')\nplt.title('Sequential A/B Test - Decisions')\nplt.yticks([])\n\nplt.tight_layout()\nplt.show()\n\n# A/B test power analysis\ndef ab_test_power_analysis():\n    \"\"\"Power analysis for A/B testing\"\"\"\n    \n    # Parameters\n    baseline_rate = 0.10\n    effect_sizes = [0.01, 0.02, 0.05, 0.10]  # Absolute differences\n    sample_sizes = [500, 1000, 2000, 5000]\n    \n    results = []\n    \n    for effect_size in effect_sizes:\n        for n in sample_sizes:\n            # Calculate power\n            power_val = power.proportion_2ind_power(\n                diff=effect_size,\n                prop2=baseline_rate,\n                nobs1=n//2,\n                alpha=0.05\n            )\n            \n            results.append({\n                'effect_size': effect_size,\n                'sample_size': n,\n                'power': power_val\n            })\n    \n    return pd.DataFrame(results)\n\nab_power_results = ab_test_power_analysis()\n\nprint(\"\\nA/B Test Power Analysis\")\nprint(ab_power_results.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Applications\n\n### Clinical Trial Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clinical_trial_simulation():\n    \"\"\"Simulate a clinical trial with multiple endpoints\"\"\"\n    \n    # Trial parameters\n    n_patients = 200\n    treatment_effect_bp = 5    # Blood pressure reduction (mmHg)\n    treatment_effect_chol = 10 # Cholesterol reduction (mg/dL)\n    \n    # Generate patient data\n    np.random.seed(42)\n    \n    # Baseline characteristics\n    age = np.random.normal(60, 10, n_patients)\n    gender = np.random.binomial(1, 0.5, n_patients)\n    \n    # Randomize to treatment groups\n    treatment = np.random.binomial(1, 0.5, n_patients)\n    \n    # Generate outcomes\n    baseline_bp = 140 + 0.5 * age + np.random.normal(0, 10, n_patients)\n    baseline_chol = 200 + 0.3 * age + np.random.normal(0, 20, n_patients)\n    \n    # Treatment effects\n    bp_reduction = treatment * treatment_effect_bp + np.random.normal(0, 5, n_patients)\n    chol_reduction = treatment * treatment_effect_chol + np.random.normal(0, 15, n_patients)\n    \n    # Final outcomes\n    final_bp = baseline_bp - bp_reduction\n    final_chol = baseline_chol - chol_reduction\n    \n    # Create DataFrame\n    df_trial = pd.DataFrame({\n        'patient_id': range(n_patients),\n        'age': age,\n        'gender': gender,\n        'treatment': treatment,\n        'baseline_bp': baseline_bp,\n        'final_bp': final_bp,\n        'baseline_chol': baseline_chol,\n        'final_chol': final_chol,\n        'bp_change': -bp_reduction,\n        'chol_change': -chol_reduction\n    })\n    \n    return df_trial\n\ndf_trial = clinical_trial_simulation()\n\nprint(\"Clinical Trial Simulation\")\nprint(f\"Number of patients: {len(df_trial)}\")\nprint(f\"Treatment group size: {df_trial['treatment'].sum()}\")\nprint(f\"Control group size: {(1 - df_trial['treatment']).sum()}\")\n\n# Analyze clinical trial\ndef analyze_clinical_trial(data):\n    \"\"\"Analyze clinical trial results\"\"\"\n    \n    # Primary endpoint: Blood pressure\n    control_bp = data[data['treatment'] == 0]['bp_change']\n    treatment_bp = data[data['treatment'] == 1]['bp_change']\n    \n    bp_effect = treatment_bp.mean() - control_bp.mean()\n    bp_t_stat, bp_p_value = stats.ttest_ind(treatment_bp, control_bp)\n    \n    # Secondary endpoint: Cholesterol\n    control_chol = data[data['treatment'] == 0]['chol_change']\n    treatment_chol = data[data['treatment'] == 1]['chol_change']\n    \n    chol_effect = treatment_chol.mean() - control_chol.mean()\n    chol_t_stat, chol_p_value = stats.ttest_ind(treatment_chol, control_chol)\n    \n    # Subgroup analysis\n    male_data = data[data['gender'] == 1]\n    female_data = data[data['gender'] == 0]\n    \n    male_effect = (male_data[male_data['treatment'] == 1]['bp_change'].mean() - \n                  male_data[male_data['treatment'] == 0]['bp_change'].mean())\n    female_effect = (female_data[female_data['treatment'] == 1]['bp_change'].mean() - \n                    female_data[female_data['treatment'] == 0]['bp_change'].mean())\n    \n    return {\n        'bp_effect': bp_effect,\n        'bp_p_value': bp_p_value,\n        'chol_effect': chol_effect,\n        'chol_p_value': chol_p_value,\n        'male_effect': male_effect,\n        'female_effect': female_effect\n    }\n\ntrial_results = analyze_clinical_trial(df_trial)\n\nprint(f\"\\nClinical Trial Results:\")\nprint(f\"Blood pressure effect: {trial_results['bp_effect']:.2f} mmHg (p={trial_results['bp_p_value']:.4f})\")\nprint(f\"Cholesterol effect: {trial_results['chol_effect']:.2f} mg/dL (p={trial_results['chol_p_value']:.4f})\")\nprint(f\"Male subgroup effect: {trial_results['male_effect']:.2f} mmHg\")\nprint(f\"Female subgroup effect: {trial_results['female_effect']:.2f} mmHg\")\n\n# Visualize clinical trial results\nplt.figure(figsize=(15, 10))\n\n# Primary endpoint\nplt.subplot(2, 3, 1)\ncontrol_bp = df_trial[df_trial['treatment'] == 0]['bp_change']\ntreatment_bp = df_trial[df_trial['treatment'] == 1]['bp_change']\nplt.boxplot([control_bp, treatment_bp], labels=['Control', 'Treatment'])\nplt.ylabel('Blood Pressure Change (mmHg)')\nplt.title('Primary Endpoint - Blood Pressure')\n\n# Secondary endpoint\nplt.subplot(2, 3, 2)\ncontrol_chol = df_trial[df_trial['treatment'] == 0]['chol_change']\ntreatment_chol = df_trial[df_trial['treatment'] == 1]['chol_change']\nplt.boxplot([control_chol, treatment_chol], labels=['Control', 'Treatment'])\nplt.ylabel('Cholesterol Change (mg/dL)')\nplt.title('Secondary Endpoint - Cholesterol')\n\n# Subgroup analysis\nplt.subplot(2, 3, 3)\nmale_control = df_trial[(df_trial['treatment'] == 0) & (df_trial['gender'] == 1)]['bp_change']\nmale_treatment = df_trial[(df_trial['treatment'] == 1) & (df_trial['gender'] == 1)]['bp_change']\nfemale_control = df_trial[(df_trial['treatment'] == 0) & (df_trial['gender'] == 0)]['bp_change']\nfemale_treatment = df_trial[(df_trial['treatment'] == 1) & (df_trial['gender'] == 0)]['bp_change']\n\nplt.boxplot([male_control, male_treatment, female_control, female_treatment], \n           labels=['Male\\nControl', 'Male\\nTreatment', 'Female\\nControl', 'Female\\nTreatment'])\nplt.ylabel('Blood Pressure Change (mmHg)')\nplt.title('Subgroup Analysis')\n\n# Baseline characteristics\nplt.subplot(2, 3, 4)\nplt.hist(df_trial[df_trial['treatment'] == 0]['age'], alpha=0.7, label='Control', bins=15)\nplt.hist(df_trial[df_trial['treatment'] == 1]['age'], alpha=0.7, label='Treatment', bins=15)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Age Distribution')\nplt.legend()\n\n# Treatment effects\nplt.subplot(2, 3, 5)\neffects = ['BP Effect', 'Chol Effect', 'Male Effect', 'Female Effect']\neffect_values = [trial_results['bp_effect'], trial_results['chol_effect'], \n                trial_results['male_effect'], trial_results['female_effect']]\ncolors = ['red' if effect < 0 else 'green' for effect in effect_values]\n\nplt.bar(effects, effect_values, color=colors, alpha=0.7)\nplt.axhline(0, color='black', linestyle='-', alpha=0.7)\nplt.ylabel('Treatment Effect')\nplt.title('Treatment Effects')\nplt.xticks(rotation=45)\n\n# P-values\nplt.subplot(2, 3, 6)\nendpoints = ['Blood Pressure', 'Cholesterol']\np_values = [trial_results['bp_p_value'], trial_results['chol_p_value']]\ncolors = ['red' if p < 0.05 else 'blue' for p in p_values]\n\nplt.bar(endpoints, p_values, color=colors, alpha=0.7)\nplt.axhline(0.05, color='red', linestyle='--', alpha=0.7, label='α = 0.05')\nplt.ylabel('P-value')\nplt.title('Statistical Significance')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Problems\n\n1. **RCT Design**: Create functions to design and analyze different types of randomized controlled trials.\n\n2. **Factorial Analysis**: Implement comprehensive factorial design analysis with interaction testing.\n\n3. **Power Analysis**: Build power analysis tools for different experimental designs and effect sizes.\n\n4. **Sequential Testing**: Develop sequential testing frameworks for early stopping in experiments.\n\n## Further Reading\n\n- \"Design and Analysis of Experiments\" by Douglas C. Montgomery\n- \"Statistics for Experimenters\" by Box, Hunter, and Hunter\n- \"Experimental Design\" by Roger E. Kirk\n- \"A/B Testing: The Most Powerful Way to Turn Clicks Into Customers\" by Dan Siroker and Pete Koomen\n\n## Key Takeaways\n\n- **Randomized controlled trials** are the gold standard for establishing causality\n- **Factorial designs** efficiently test multiple factors and their interactions\n- **Blocking** reduces variability and increases statistical power\n- **Sample size determination** ensures adequate power to detect effects\n- **A/B testing** provides practical frameworks for online experiments\n- **Proper randomization** is essential for valid statistical inference\n- **Multiple endpoints** require careful consideration of multiple testing\n- **Subgroup analysis** can reveal important treatment effect heterogeneity\n\nIn the next chapter, we'll explore statistical learning, including cross-validation, model selection, and ensemble methods."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}