{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multivariate Statistics\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Pandas](https://img.shields.io/badge/Pandas-1.3+-blue.svg)](https://pandas.pydata.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.4+-orange.svg)](https://matplotlib.org/)\n[![Seaborn](https://img.shields.io/badge/Seaborn-0.11+-blue.svg)](https://seaborn.pydata.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-green.svg)](https://scipy.org/)\n[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)\n[![FactorAnalyzer](https://img.shields.io/badge/FactorAnalyzer-0.4+-blue.svg)](https://factor-analyzer.readthedocs.io/)\n\nMultivariate statistics deals with the analysis of data with multiple variables. This chapter covers dimensionality reduction, clustering, and multivariate analysis techniques essential for AI/ML.\n\n## Table of Contents\n- [Principal Component Analysis](#principal-component-analysis)\n- [Factor Analysis](#factor-analysis)\n- [Cluster Analysis](#cluster-analysis)\n- [Discriminant Analysis](#discriminant-analysis)\n- [Canonical Correlation](#canonical-correlation)\n- [Multidimensional Scaling](#multidimensional-scaling)\n- [Practical Applications](#practical-applications)\n\n## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score\nfrom sklearn.datasets import make_blobs, make_classification\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\nnp.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Principal Component Analysis\n\n### Basic PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_multivariate_data(n_samples=300, n_features=10):\n    \"\"\"Generate multivariate data with known structure\"\"\"\n    # Generate correlated features\n    np.random.seed(42)\n    \n    # Create correlation matrix\n    corr_matrix = np.eye(n_features)\n    corr_matrix[0:3, 0:3] = 0.8  # First 3 features highly correlated\n    corr_matrix[3:6, 3:6] = 0.7  # Next 3 features moderately correlated\n    corr_matrix[6:8, 6:8] = 0.6  # Features 6-7 correlated\n    \n    # Generate data\n    data = np.random.multivariate_normal(\n        mean=np.zeros(n_features),\n        cov=corr_matrix,\n        size=n_samples\n    )\n    \n    # Add some noise features\n    noise_features = np.random.normal(0, 1, (n_samples, 2))\n    data = np.column_stack([data, noise_features])\n    \n    # Create feature names\n    feature_names = [f'Feature_{i+1}' for i in range(data.shape[1])]\n    \n    return pd.DataFrame(data, columns=feature_names)\n\n# Generate data\ndf_pca = generate_multivariate_data()\nprint(\"Multivariate Data Overview\")\nprint(f\"Shape: {df_pca.shape}\")\nprint(f\"Features: {list(df_pca.columns)}\")\n\n# Correlation matrix\ncorrelation_matrix = df_pca.corr()\nprint(f\"\\nCorrelation Matrix Shape: {correlation_matrix.shape}\")\n\n# Visualize correlation matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n            square=True, linewidths=0.5, fmt='.2f')\nplt.title('Correlation Matrix')\nplt.tight_layout()\nplt.show()\n\n# Perform PCA\ndef perform_pca_analysis(data, n_components=None):\n    \"\"\"Perform comprehensive PCA analysis\"\"\"\n    \n    # Standardize the data\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data)\n    \n    # Perform PCA\n    if n_components is None:\n        pca = PCA()\n    else:\n        pca = PCA(n_components=n_components)\n    \n    pca_result = pca.fit_transform(data_scaled)\n    \n    return pca, pca_result, data_scaled\n\npca, pca_result, data_scaled = perform_pca_analysis(df_pca)\n\nprint(\"PCA Results\")\nprint(f\"Number of components: {pca.n_components_}\")\nprint(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\nprint(f\"Cumulative explained variance: {np.cumsum(pca.explained_variance_ratio_)}\")\n\n# Visualize PCA results\nplt.figure(figsize=(15, 10))\n\n# Scree plot\nplt.subplot(2, 3, 1)\nn_components = len(pca.explained_variance_ratio_)\nplt.bar(range(1, n_components + 1), pca.explained_variance_ratio_, alpha=0.7, color='skyblue')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nplt.title('Scree Plot')\nplt.xticks(range(1, n_components + 1))\n\n# Cumulative explained variance\nplt.subplot(2, 3, 2)\ncumulative_var = np.cumsum(pca.explained_variance_ratio_)\nplt.plot(range(1, n_components + 1), cumulative_var, 'ro-', linewidth=2, markersize=8)\nplt.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='80% threshold')\nplt.axhline(y=0.9, color='orange', linestyle='--', alpha=0.7, label='90% threshold')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Cumulative Explained Variance')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# First two principal components\nplt.subplot(2, 3, 3)\nplt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7, color='lightgreen')\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})')\nplt.title('First Two Principal Components')\n\n# Component loadings\nplt.subplot(2, 3, 4)\nloadings = pca.components_.T\nplt.bar(range(len(df_pca.columns)), loadings[:, 0], alpha=0.7, color='orange')\nplt.xlabel('Features')\nplt.ylabel('Loading')\nplt.title('PC1 Loadings')\nplt.xticks(range(len(df_pca.columns)), df_pca.columns, rotation=45)\n\nplt.subplot(2, 3, 5)\nplt.bar(range(len(df_pca.columns)), loadings[:, 1], alpha=0.7, color='purple')\nplt.xlabel('Features')\nplt.ylabel('Loading')\nplt.title('PC2 Loadings')\nplt.xticks(range(len(df_pca.columns)), df_pca.columns, rotation=45)\n\n# Biplot (simplified)\nplt.subplot(2, 3, 6)\nplt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7, color='lightgreen')\nfor i, feature in enumerate(df_pca.columns):\n    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3, \n              head_width=0.1, head_length=0.1, fc='red', ec='red', alpha=0.7)\n    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, fontsize=8)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Biplot (PC1 vs PC2)')\n\nplt.tight_layout()\nplt.show()\n\n# Determine optimal number of components\ndef find_optimal_components(data, max_components=None):\n    \"\"\"Find optimal number of components using various criteria\"\"\"\n    \n    if max_components is None:\n        max_components = min(data.shape)\n    \n    # Perform PCA with maximum components\n    pca_full = PCA()\n    pca_full.fit(StandardScaler().fit_transform(data))\n    \n    # Calculate metrics\n    explained_var = pca_full.explained_variance_ratio_\n    cumulative_var = np.cumsum(explained_var)\n    \n    # Find components for different thresholds\n    n_80 = np.argmax(cumulative_var >= 0.8) + 1\n    n_90 = np.argmax(cumulative_var >= 0.9) + 1\n    n_95 = np.argmax(cumulative_var >= 0.95) + 1\n    \n    # Kaiser criterion (eigenvalues > 1)\n    eigenvalues = pca_full.explained_variance_\n    n_kaiser = np.sum(eigenvalues > 1)\n    \n    # Elbow method (simplified)\n    second_derivative = np.diff(np.diff(explained_var))\n    n_elbow = np.argmax(second_derivative) + 2\n    \n    return {\n        'n_80': n_80,\n        'n_90': n_90,\n        'n_95': n_95,\n        'n_kaiser': n_kaiser,\n        'n_elbow': n_elbow,\n        'explained_var': explained_var,\n        'cumulative_var': cumulative_var\n    }\n\noptimal_components = find_optimal_components(df_pca)\n\nprint(\"Optimal Number of Components\")\nprint(f\"80% variance: {optimal_components['n_80']} components\")\nprint(f\"90% variance: {optimal_components['n_90']} components\")\nprint(f\"95% variance: {optimal_components['n_95']} components\")\nprint(f\"Kaiser criterion: {optimal_components['n_kaiser']} components\")\nprint(f\"Elbow method: {optimal_components['n_elbow']} components\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Factor Analysis\n\n### Exploratory Factor Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_factor_analysis(data, n_factors=None):\n    \"\"\"Perform factor analysis\"\"\"\n    \n    # Standardize data\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data)\n    \n    # Perform factor analysis\n    if n_factors is None:\n        n_factors = data.shape[1] // 2  # Default to half the features\n    \n    fa = FactorAnalysis(n_components=n_factors, random_state=42)\n    fa_result = fa.fit_transform(data_scaled)\n    \n    return fa, fa_result, data_scaled\n\nfa_model, fa_result, data_scaled_fa = perform_factor_analysis(df_pca, n_factors=4)\n\nprint(\"Factor Analysis Results\")\nprint(f\"Number of factors: {fa_model.n_components_}\")\nprint(f\"Factor loadings shape: {fa_model.components_.shape}\")\n\n# Visualize factor analysis results\nplt.figure(figsize=(15, 10))\n\n# Factor loadings heatmap\nplt.subplot(2, 3, 1)\nloadings_df = pd.DataFrame(fa_model.components_.T, \n                          index=df_pca.columns,\n                          columns=[f'Factor_{i+1}' for i in range(fa_model.n_components_)])\nsns.heatmap(loadings_df, annot=True, cmap='coolwarm', center=0, \n            square=True, linewidths=0.5, fmt='.2f')\nplt.title('Factor Loadings')\n\n# Factor scores\nplt.subplot(2, 3, 2)\nplt.scatter(fa_result[:, 0], fa_result[:, 1], alpha=0.7, color='skyblue')\nplt.xlabel('Factor 1')\nplt.ylabel('Factor 2')\nplt.title('Factor Scores (Factor 1 vs Factor 2)')\n\n# Factor loadings bar plot\nplt.subplot(2, 3, 3)\nloadings_abs = np.abs(fa_model.components_.T)\nplt.bar(range(len(df_pca.columns)), loadings_abs[:, 0], alpha=0.7, color='lightgreen')\nplt.xlabel('Features')\nplt.ylabel('Absolute Loading')\nplt.title('Factor 1 Loadings')\nplt.xticks(range(len(df_pca.columns)), df_pca.columns, rotation=45)\n\nplt.subplot(2, 3, 4)\nplt.bar(range(len(df_pca.columns)), loadings_abs[:, 1], alpha=0.7, color='orange')\nplt.xlabel('Features')\nplt.ylabel('Absolute Loading')\nplt.title('Factor 2 Loadings')\nplt.xticks(range(len(df_pca.columns)), df_pca.columns, rotation=45)\n\n# Factor correlation\nplt.subplot(2, 3, 5)\nfactor_corr = np.corrcoef(fa_result.T)\nsns.heatmap(factor_corr, annot=True, cmap='coolwarm', center=0, \n            square=True, linewidths=0.5, fmt='.2f')\nplt.title('Factor Correlation Matrix')\n\n# Reconstruction error\nreconstructed = fa_model.inverse_transform(fa_result)\nreconstruction_error = np.mean((data_scaled_fa - reconstructed) ** 2)\nplt.subplot(2, 3, 6)\nplt.text(0.5, 0.5, f'Reconstruction Error:\\n{reconstruction_error:.4f}', \n         ha='center', va='center', transform=plt.gca().transAxes, fontsize=12,\n         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\nplt.title('Reconstruction Error')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Reconstruction Error: {reconstruction_error:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster Analysis\n\n### K-Means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_clustering_data(n_samples=300):\n    \"\"\"Generate data for clustering analysis\"\"\"\n    # Generate clusters\n    centers = [[2, 2], [8, 3], [3, 6], [7, 8]]\n    cluster_std = [1.5, 1.2, 1.8, 1.3]\n    \n    X, y_true = make_blobs(n_samples=n_samples, centers=centers, \n                          cluster_std=cluster_std, random_state=42)\n    \n    # Add some noise\n    noise = np.random.normal(0, 0.5, (n_samples, 2))\n    X += noise\n    \n    return X, y_true\n\nX_cluster, y_true = generate_clustering_data()\n\nprint(\"Clustering Data Overview\")\nprint(f\"Data shape: {X_cluster.shape}\")\nprint(f\"True clusters: {len(np.unique(y_true))}\")\n\n# K-Means clustering\ndef perform_kmeans_analysis(X, max_k=10):\n    \"\"\"Perform K-means clustering with different k values\"\"\"\n    \n    inertias = []\n    silhouette_scores = []\n    calinski_scores = []\n    \n    for k in range(2, max_k + 1):\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        kmeans.fit(X)\n        \n        inertias.append(kmeans.inertia_)\n        silhouette_scores.append(silhouette_score(X, kmeans.labels_))\n        calinski_scores.append(calinski_harabasz_score(X, kmeans.labels_))\n    \n    return inertias, silhouette_scores, calinski_scores\n\ninertias, silhouette_scores, calinski_scores = perform_kmeans_analysis(X_cluster)\n\n# Visualize clustering results\nplt.figure(figsize=(15, 10))\n\n# Elbow plot\nplt.subplot(2, 3, 1)\nk_values = range(2, len(inertias) + 2)\nplt.plot(k_values, inertias, 'bo-', linewidth=2, markersize=8)\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.title('Elbow Plot')\nplt.grid(True, alpha=0.3)\n\n# Silhouette score\nplt.subplot(2, 3, 2)\nplt.plot(k_values, silhouette_scores, 'ro-', linewidth=2, markersize=8)\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score')\nplt.grid(True, alpha=0.3)\n\n# Calinski-Harabasz score\nplt.subplot(2, 3, 3)\nplt.plot(k_values, calinski_scores, 'go-', linewidth=2, markersize=8)\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Calinski-Harabasz Score')\nplt.title('Calinski-Harabasz Score')\nplt.grid(True, alpha=0.3)\n\n# Optimal K-means clustering\noptimal_k = k_values[np.argmax(silhouette_scores)]\nkmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\nkmeans_optimal.fit(X_cluster)\n\nplt.subplot(2, 3, 4)\nplt.scatter(X_cluster[:, 0], X_cluster[:, 1], c=kmeans_optimal.labels_, \n           cmap='viridis', alpha=0.7)\nplt.scatter(kmeans_optimal.cluster_centers_[:, 0], kmeans_optimal.cluster_centers_[:, 1], \n           c='red', marker='x', s=200, linewidths=3, label='Centroids')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title(f'K-Means Clustering (k={optimal_k})')\nplt.legend()\n\n# True clusters\nplt.subplot(2, 3, 5)\nplt.scatter(X_cluster[:, 0], X_cluster[:, 1], c=y_true, \n           cmap='viridis', alpha=0.7)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('True Clusters')\n\n# Comparison\nplt.subplot(2, 3, 6)\nfrom sklearn.metrics import adjusted_rand_score\nari_score = adjusted_rand_score(y_true, kmeans_optimal.labels_)\nplt.text(0.5, 0.5, f'Adjusted Rand Index:\\n{ari_score:.3f}', \n         ha='center', va='center', transform=plt.gca().transAxes, fontsize=12,\n         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7))\nplt.title('Clustering Quality')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Optimal number of clusters: {optimal_k}\")\nprint(f\"Adjusted Rand Index: {ari_score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_hierarchical_clustering(X, n_clusters=4):\n    \"\"\"Perform hierarchical clustering\"\"\"\n    \n    # Perform clustering\n    hierarchical = AgglomerativeClustering(n_clusters=n_clusters)\n    hierarchical_labels = hierarchical.fit_predict(X)\n    \n    return hierarchical, hierarchical_labels\n\nhierarchical_model, hierarchical_labels = perform_hierarchical_clustering(X_cluster)\n\n# Visualize hierarchical clustering\nplt.figure(figsize=(15, 5))\n\n# Dendrogram (simplified visualization)\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nplt.subplot(1, 3, 1)\nlinkage_matrix = linkage(X_cluster, method='ward')\ndendrogram(linkage_matrix, truncate_mode='level', p=3)\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.title('Hierarchical Clustering Dendrogram')\n\n# Hierarchical clustering results\nplt.subplot(1, 3, 2)\nplt.scatter(X_cluster[:, 0], X_cluster[:, 1], c=hierarchical_labels, \n           cmap='viridis', alpha=0.7)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Hierarchical Clustering')\n\n# Comparison with K-means\nplt.subplot(1, 3, 3)\nfrom sklearn.metrics import adjusted_rand_score\nhierarchical_ari = adjusted_rand_score(y_true, hierarchical_labels)\nkmeans_ari = adjusted_rand_score(y_true, kmeans_optimal.labels_)\n\nplt.bar(['K-Means', 'Hierarchical'], [kmeans_ari, hierarchical_ari], \n        alpha=0.7, color=['skyblue', 'lightgreen'])\nplt.ylabel('Adjusted Rand Index')\nplt.title('Clustering Method Comparison')\nplt.ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Hierarchical Clustering ARI: {hierarchical_ari:.3f}\")\nprint(f\"K-Means ARI: {kmeans_ari:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DBSCAN Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_dbscan_clustering(X):\n    \"\"\"Perform DBSCAN clustering\"\"\"\n    \n    # Perform DBSCAN\n    dbscan = DBSCAN(eps=1.0, min_samples=5)\n    dbscan_labels = dbscan.fit_predict(X)\n    \n    return dbscan, dbscan_labels\n\ndbscan_model, dbscan_labels = perform_dbscan_clustering(X_cluster)\n\n# Visualize DBSCAN results\nplt.figure(figsize=(15, 5))\n\n# DBSCAN clustering\nplt.subplot(1, 3, 1)\nplt.scatter(X_cluster[:, 0], X_cluster[:, 1], c=dbscan_labels, \n           cmap='viridis', alpha=0.7)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title(f'DBSCAN Clustering (eps=1.0, min_samples=5)')\n\n# Number of clusters found\nn_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\nn_noise = list(dbscan_labels).count(-1)\n\nplt.subplot(1, 3, 2)\ncluster_counts = [list(dbscan_labels).count(i) for i in set(dbscan_labels)]\nplt.bar(range(len(cluster_counts)), cluster_counts, alpha=0.7, color='orange')\nplt.xlabel('Cluster')\nplt.ylabel('Number of Points')\nplt.title(f'Cluster Sizes (Noise: {n_noise})')\n\n# Comparison with other methods\nplt.subplot(1, 3, 3)\ndbscan_ari = adjusted_rand_score(y_true, dbscan_labels)\nmethods = ['K-Means', 'Hierarchical', 'DBSCAN']\nari_scores = [kmeans_ari, hierarchical_ari, dbscan_ari]\ncolors = ['skyblue', 'lightgreen', 'orange']\n\nplt.bar(methods, ari_scores, alpha=0.7, color=colors)\nplt.ylabel('Adjusted Rand Index')\nplt.title('Clustering Method Comparison')\nplt.ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"DBSCAN Results:\")\nprint(f\"Number of clusters: {n_clusters_dbscan}\")\nprint(f\"Number of noise points: {n_noise}\")\nprint(f\"DBSCAN ARI: {dbscan_ari:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discriminant Analysis\n\n### Linear Discriminant Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_classification_data(n_samples=300):\n    \"\"\"Generate data for discriminant analysis\"\"\"\n    \n    # Generate classification data\n    X, y = make_classification(n_samples=n_samples, n_features=10, n_informative=6,\n                             n_redundant=2, n_classes=3, n_clusters_per_class=1,\n                             random_state=42)\n    \n    return X, y\n\nX_disc, y_disc = generate_classification_data()\n\nprint(\"Classification Data Overview\")\nprint(f\"Data shape: {X_disc.shape}\")\nprint(f\"Number of classes: {len(np.unique(y_disc))}\")\n\n# Perform LDA\ndef perform_lda_analysis(X, y):\n    \"\"\"Perform Linear Discriminant Analysis\"\"\"\n    \n    # Standardize data\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Perform LDA\n    lda = LinearDiscriminantAnalysis()\n    lda_result = lda.fit_transform(X_scaled, y)\n    \n    return lda, lda_result, X_scaled\n\nlda_model, lda_result, X_scaled_disc = perform_lda_analysis(X_disc, y_disc)\n\n# Visualize LDA results\nplt.figure(figsize=(15, 10))\n\n# LDA projection\nplt.subplot(2, 3, 1)\nplt.scatter(lda_result[:, 0], lda_result[:, 1], c=y_disc, cmap='viridis', alpha=0.7)\nplt.xlabel('LD1')\nplt.ylabel('LD2')\nplt.title('LDA Projection')\nplt.colorbar()\n\n# Explained variance ratio\nplt.subplot(2, 3, 2)\nexplained_var = lda_model.explained_variance_ratio_\nplt.bar(range(1, len(explained_var) + 1), explained_var, alpha=0.7, color='skyblue')\nplt.xlabel('Linear Discriminant')\nplt.ylabel('Explained Variance Ratio')\nplt.title('LDA Explained Variance')\n\n# Feature importance\nplt.subplot(2, 3, 3)\nfeature_importance = np.abs(lda_model.coef_[0])\nplt.bar(range(len(feature_importance)), feature_importance, alpha=0.7, color='lightgreen')\nplt.xlabel('Features')\nplt.ylabel('Coefficient Magnitude')\nplt.title('LDA Feature Importance')\n\n# Classification accuracy\nfrom sklearn.model_selection import cross_val_score\nlda_scores = cross_val_score(lda_model, X_scaled_disc, y_disc, cv=5)\n\nplt.subplot(2, 3, 4)\nplt.bar(range(1, len(lda_scores) + 1), lda_scores, alpha=0.7, color='orange')\nplt.axhline(lda_scores.mean(), color='red', linestyle='--', label=f'Mean: {lda_scores.mean():.3f}')\nplt.xlabel('Fold')\nplt.ylabel('Accuracy')\nplt.title('LDA Cross-Validation Scores')\nplt.legend()\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred_lda = lda_model.predict(X_scaled_disc)\ncm = confusion_matrix(y_disc, y_pred_lda)\n\nplt.subplot(2, 3, 5)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('LDA Confusion Matrix')\n\n# Comparison with PCA\npca_disc = PCA(n_components=2)\npca_result_disc = pca_disc.fit_transform(X_scaled_disc)\n\nplt.subplot(2, 3, 6)\nplt.scatter(pca_result_disc[:, 0], pca_result_disc[:, 1], c=y_disc, cmap='viridis', alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Projection')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"LDA Results:\")\nprint(f\"Explained variance ratio: {explained_var}\")\nprint(f\"Cross-validation accuracy: {lda_scores.mean():.3f} (+/- {lda_scores.std() * 2:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Canonical Correlation\n\n### Canonical Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_canonical_data(n_samples=200):\n    \"\"\"Generate data for canonical correlation analysis\"\"\"\n    \n    # Generate two sets of variables\n    np.random.seed(42)\n    \n    # First set of variables\n    X1 = np.random.multivariate_normal([0, 0, 0], [[1, 0.7, 0.5], [0.7, 1, 0.6], [0.5, 0.6, 1]], n_samples)\n    \n    # Second set of variables (correlated with first set)\n    X2 = np.random.multivariate_normal([0, 0], [[1, 0.8], [0.8, 1]], n_samples)\n    \n    # Add correlation between sets\n    X2[:, 0] = 0.6 * X1[:, 0] + 0.4 * X2[:, 0]\n    X2[:, 1] = 0.5 * X1[:, 1] + 0.5 * X2[:, 1]\n    \n    return X1, X2\n\nX1_canon, X2_canon = generate_canonical_data()\n\nprint(\"Canonical Correlation Data Overview\")\nprint(f\"X1 shape: {X1_canon.shape}\")\nprint(f\"X2 shape: {X2_canon.shape}\")\n\n# Perform canonical correlation analysis\ndef perform_canonical_correlation(X1, X2):\n    \"\"\"Perform canonical correlation analysis\"\"\"\n    \n    from sklearn.cross_decomposition import CCA\n    \n    # Standardize data\n    scaler1 = StandardScaler()\n    scaler2 = StandardScaler()\n    X1_scaled = scaler1.fit_transform(X1)\n    X2_scaled = scaler2.fit_transform(X2)\n    \n    # Perform CCA\n    cca = CCA(n_components=min(X1.shape[1], X2.shape[1]))\n    cca_result = cca.fit_transform(X1_scaled, X2_scaled)\n    \n    return cca, cca_result, X1_scaled, X2_scaled\n\ncca_model, cca_result, X1_scaled_canon, X2_scaled_canon = perform_canonical_correlation(X1_canon, X2_canon)\n\n# Visualize canonical correlation results\nplt.figure(figsize=(15, 10))\n\n# Canonical variates\nplt.subplot(2, 3, 1)\nplt.scatter(cca_result[0][:, 0], cca_result[1][:, 0], alpha=0.7, color='skyblue')\nplt.xlabel('Canonical Variate 1 (X1)')\nplt.ylabel('Canonical Variate 1 (X2)')\nplt.title('First Canonical Variates')\n\nplt.subplot(2, 3, 2)\nplt.scatter(cca_result[0][:, 1], cca_result[1][:, 1], alpha=0.7, color='lightgreen')\nplt.xlabel('Canonical Variate 2 (X1)')\nplt.ylabel('Canonical Variate 2 (X2)')\nplt.title('Second Canonical Variates')\n\n# Canonical correlations\ncanonical_correlations = np.corrcoef(cca_result[0].T, cca_result[1].T)[:2, 2:]\ndiagonal_correlations = np.diag(canonical_correlations)\n\nplt.subplot(2, 3, 3)\nplt.bar(range(1, len(diagonal_correlations) + 1), diagonal_correlations, alpha=0.7, color='orange')\nplt.xlabel('Canonical Pair')\nplt.ylabel('Canonical Correlation')\nplt.title('Canonical Correlations')\n\n# Loadings for first canonical variate\nplt.subplot(2, 3, 4)\nloadings1 = cca_model.x_weights_[:, 0]\nplt.bar(range(len(loadings1)), loadings1, alpha=0.7, color='purple')\nplt.xlabel('X1 Variables')\nplt.ylabel('Loading')\nplt.title('X1 Loadings (First Canonical Variate)')\n\nplt.subplot(2, 3, 5)\nloadings2 = cca_model.y_weights_[:, 0]\nplt.bar(range(len(loadings2)), loadings2, alpha=0.7, color='brown')\nplt.xlabel('X2 Variables')\nplt.ylabel('Loading')\nplt.title('X2 Loadings (First Canonical Variate)')\n\n# Correlation matrix between original variables\nplt.subplot(2, 3, 6)\ncombined_data = np.column_stack([X1_canon, X2_canon])\ncorr_matrix = np.corrcoef(combined_data.T)\nsns.heatmap(corr_matrix, cmap='coolwarm', center=0, square=True, linewidths=0.5)\nplt.title('Correlation Matrix (X1 + X2)')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Canonical Correlations: {diagonal_correlations}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multivariate Normal Distribution\n\nThe **multivariate normal distribution** is the most important distribution in multivariate statistics, serving as the foundation for many statistical methods.\n\n### Mathematical Definition\n\nA random vector **X** = (X₁, X₂, ..., Xₚ)ᵀ follows a **p-dimensional multivariate normal distribution** if its joint probability density function is:\n\n$$f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2} |\\mathbf{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})\\right)$$\n\nWhere:\n- **μ** = (μ₁, μ₂, ..., μₚ)ᵀ is the **mean vector**\n- **Σ** is the **covariance matrix** (p × p symmetric positive definite)\n- |**Σ**| is the determinant of **Σ**\n\n**Notation:** **X** ~ Nₚ(**μ**, **Σ**)\n\n### Properties of Multivariate Normal Distribution\n\n**1. Marginal Distributions:**\nIf **X** ~ Nₚ(**μ**, **Σ**), then any subset of components follows a multivariate normal distribution:\n- Xᵢ ~ N(μᵢ, σᵢ²) for individual components\n- **X**₍₁,₂₎ ~ N₂(**μ**₍₁,₂₎, **Σ**₍₁,₂₎) for any 2-dimensional subset\n\n**2. Linear Transformations:**\nIf **X** ~ Nₚ(**μ**, **Σ**) and **Y** = **AX** + **b**, then:\n**Y** ~ Nₘ(**Aμ** + **b**, **AΣA**ᵀ)\n\nWhere **A** is an m × p matrix and **b** is an m-dimensional vector.\n\n**3. Independence:**\nFor multivariate normal, uncorrelated components are independent:\n- If Cov(Xᵢ, Xⱼ) = 0 for all i ≠ j, then Xᵢ and Xⱼ are independent\n- This is a unique property of the normal distribution\n\n**4. Conditional Distributions:**\nIf **X** ~ Nₚ(**μ**, **Σ**), partitioned as:\n$$\\mathbf{X} = \\begin{pmatrix} \\mathbf{X}_1 \\\\ \\mathbf{X}_2 \\end{pmatrix}, \\quad \\mathbf{\\mu} = \\begin{pmatrix} \\mathbf{\\mu}_1 \\\\ \\mathbf{\\mu}_2 \\end{pmatrix}, \\quad \\mathbf{\\Sigma} = \\begin{pmatrix} \\mathbf{\\Sigma}_{11} & \\mathbf{\\Sigma}_{12} \\\\ \\mathbf{\\Sigma}_{21} & \\mathbf{\\Sigma}_{22} \\end{pmatrix}$$\n\nThen the conditional distribution is:\n$$\\mathbf{X}_1 | \\mathbf{X}_2 = \\mathbf{x}_2 \\sim N_{p_1}(\\mathbf{\\mu}_{1|2}, \\mathbf{\\Sigma}_{1|2})$$\n\nWhere:\n$$\\mathbf{\\mu}_{1|2} = \\mathbf{\\mu}_1 + \\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}(\\mathbf{x}_2 - \\mathbf{\\mu}_2)$$\n$$\\mathbf{\\Sigma}_{1|2} = \\mathbf{\\Sigma}_{11} - \\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}$$\n\n**5. Maximum Likelihood Estimation:**\nFor a sample **X**₁, **X**₂, ..., **X**ₙ ~ Nₚ(**μ**, **Σ**), the MLEs are:\n$$\\hat{\\mathbf{\\mu}} = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbf{X}_i = \\bar{\\mathbf{X}}$$\n$$\\hat{\\mathbf{\\Sigma}} = \\frac{1}{n}\\sum_{i=1}^{n} (\\mathbf{X}_i - \\bar{\\mathbf{X}})(\\mathbf{X}_i - \\bar{\\mathbf{X}})^T$$\n\n**6. Mahalanobis Distance:**\nThe squared Mahalanobis distance is:\n$$D^2(\\mathbf{x}) = (\\mathbf{x} - \\mathbf{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})$$\n\nThis follows a χ²(p) distribution under the null hypothesis.\n\n### Characteristic Function\n\nThe characteristic function of **X** ~ Nₚ(**μ**, **Σ**) is:\n$$\\phi_{\\mathbf{X}}(\\mathbf{t}) = E[e^{i\\mathbf{t}^T\\mathbf{X}}] = \\exp\\left(i\\mathbf{t}^T\\mathbf{\\mu} - \\frac{1}{2}\\mathbf{t}^T\\mathbf{\\Sigma}\\mathbf{t}\\right)$$\n\n### Moment Generating Function\n\nThe moment generating function is:\n$$M_{\\mathbf{X}}(\\mathbf{t}) = E[e^{\\mathbf{t}^T\\mathbf{X}}] = \\exp\\left(\\mathbf{t}^T\\mathbf{\\mu} + \\frac{1}{2}\\mathbf{t}^T\\mathbf{\\Sigma}\\mathbf{t}\\right)$$\n\n### Central Limit Theorem (Multivariate)\n\nIf **X**₁, **X**₂, ..., **X**ₙ are independent random vectors with E[**X**ᵢ] = **μ** and Cov(**X**ᵢ) = **Σ**, then:\n$$\\sqrt{n}(\\bar{\\mathbf{X}} - \\mathbf{\\mu}) \\xrightarrow{d} N_p(\\mathbf{0}, \\mathbf{\\Sigma})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal, chi2\nfrom scipy.linalg import cholesky, inv\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef multivariate_normal_pdf(x, mu, sigma):\n    \"\"\"\n    Calculate multivariate normal PDF\n    \n    Mathematical implementation:\n    f(x) = (2π)^(-p/2) |Σ|^(-1/2) exp(-0.5(x-μ)ᵀΣ⁻¹(x-μ))\n    \n    Parameters:\n    x: array, point to evaluate\n    mu: array, mean vector\n    sigma: array, covariance matrix\n    \n    Returns:\n    float: PDF value\n    \"\"\"\n    p = len(mu)\n    \n    # Check dimensions\n    if len(x) != p:\n        raise ValueError(\"Dimensions of x and mu must match\")\n    \n    if sigma.shape != (p, p):\n        raise ValueError(\"Sigma must be p×p matrix\")\n    \n    # Calculate determinant\n    det_sigma = np.linalg.det(sigma)\n    if det_sigma <= 0:\n        raise ValueError(\"Sigma must be positive definite\")\n    \n    # Calculate inverse\n    sigma_inv = np.linalg.inv(sigma)\n    \n    # Calculate quadratic form\n    diff = x - mu\n    quadratic_form = diff.T @ sigma_inv @ diff\n    \n    # Calculate PDF\n    pdf = (1 / ((2 * np.pi) ** (p/2) * np.sqrt(det_sigma))) * np.exp(-0.5 * quadratic_form)\n    \n    return pdf\n\ndef generate_multivariate_normal(n, mu, sigma, method='cholesky'):\n    \"\"\"\n    Generate samples from multivariate normal distribution\n    \n    Mathematical implementation:\n    X = μ + LZ where L is Cholesky factor of Σ and Z ~ N(0,I)\n    \n    Parameters:\n    n: int, number of samples\n    mu: array, mean vector\n    sigma: array, covariance matrix\n    method: str, generation method\n    \n    Returns:\n    array: n×p matrix of samples\n    \"\"\"\n    p = len(mu)\n    \n    if method == 'cholesky':\n        # Cholesky decomposition method\n        L = cholesky(sigma, lower=True)\n        Z = np.random.normal(0, 1, (n, p))\n        X = mu + Z @ L.T\n        return X\n    \n    elif method == 'eigenvalue':\n        # Eigenvalue decomposition method\n        eigenvals, eigenvecs = np.linalg.eigh(sigma)\n        D_sqrt = np.diag(np.sqrt(eigenvals))\n        Z = np.random.normal(0, 1, (n, p))\n        X = mu + Z @ D_sqrt @ eigenvecs.T\n        return X\n    \n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\ndef mahalanobis_distance(x, mu, sigma):\n    \"\"\"\n    Calculate Mahalanobis distance\n    \n    Mathematical implementation:\n    D²(x) = (x-μ)ᵀΣ⁻¹(x-μ)\n    \n    Parameters:\n    x: array, point\n    mu: array, mean vector\n    sigma: array, covariance matrix\n    \n    Returns:\n    float: Mahalanobis distance\n    \"\"\"\n    sigma_inv = np.linalg.inv(sigma)\n    diff = x - mu\n    distance_squared = diff.T @ sigma_inv @ diff\n    return np.sqrt(distance_squared)\n\ndef conditional_multivariate_normal(x2, mu, sigma, partition_idx):\n    \"\"\"\n    Calculate conditional distribution parameters\n    \n    Mathematical implementation:\n    μ₁|₂ = μ₁ + Σ₁₂Σ₂₂⁻¹(x₂ - μ₂)\n    Σ₁|₂ = Σ₁₁ - Σ₁₂Σ₂₂⁻¹Σ₂₁\n    \n    Parameters:\n    x2: array, conditioning values\n    mu: array, mean vector\n    sigma: array, covariance matrix\n    partition_idx: list, indices for partition\n    \n    Returns:\n    tuple: (conditional_mean, conditional_covariance)\n    \"\"\"\n    p = len(mu)\n    \n    # Create partition indices\n    idx1 = [i for i in range(p) if i not in partition_idx]\n    idx2 = partition_idx\n    \n    # Extract submatrices\n    mu1 = mu[idx1]\n    mu2 = mu[idx2]\n    sigma11 = sigma[np.ix_(idx1, idx1)]\n    sigma12 = sigma[np.ix_(idx1, idx2)]\n    sigma21 = sigma[np.ix_(idx2, idx1)]\n    sigma22 = sigma[np.ix_(idx2, idx2)]\n    \n    # Calculate conditional parameters\n    sigma22_inv = np.linalg.inv(sigma22)\n    conditional_mean = mu1 + sigma12 @ sigma22_inv @ (x2 - mu2)\n    conditional_covariance = sigma11 - sigma12 @ sigma22_inv @ sigma21\n    \n    return conditional_mean, conditional_covariance\n\ndef multivariate_normal_mle(X):\n    \"\"\"\n    Calculate maximum likelihood estimates for multivariate normal\n    \n    Mathematical implementation:\n    μ̂ = (1/n)ΣᵢXᵢ\n    Σ̂ = (1/n)Σᵢ(Xᵢ-μ̂)(Xᵢ-μ̂)ᵀ\n    \n    Parameters:\n    X: array, n×p data matrix\n    \n    Returns:\n    tuple: (mu_hat, sigma_hat)\n    \"\"\"\n    n, p = X.shape\n    \n    # MLE for mean\n    mu_hat = np.mean(X, axis=0)\n    \n    # MLE for covariance\n    centered_X = X - mu_hat\n    sigma_hat = (centered_X.T @ centered_X) / n\n    \n    return mu_hat, sigma_hat\n\ndef multivariate_normal_log_likelihood(X, mu, sigma):\n    \"\"\"\n    Calculate log-likelihood for multivariate normal\n    \n    Mathematical implementation:\n    log L = -(n/2)log|Σ| - (1/2)Σᵢ(xᵢ-μ)ᵀΣ⁻¹(xᵢ-μ) - (np/2)log(2π)\n    \n    Parameters:\n    X: array, n×p data matrix\n    mu: array, mean vector\n    sigma: array, covariance matrix\n    \n    Returns:\n    float: log-likelihood\n    \"\"\"\n    n, p = X.shape\n    \n    # Calculate determinant and inverse\n    det_sigma = np.linalg.det(sigma)\n    sigma_inv = np.linalg.inv(sigma)\n    \n    # Calculate log-likelihood\n    log_det = np.log(det_sigma)\n    \n    # Calculate quadratic form for all observations\n    centered_X = X - mu\n    quadratic_terms = np.sum(centered_X @ sigma_inv * centered_X, axis=1)\n    total_quadratic = np.sum(quadratic_terms)\n    \n    log_likelihood = -(n/2) * log_det - (1/2) * total_quadratic - (n*p/2) * np.log(2*np.pi)\n    \n    return log_likelihood\n\ndef multivariate_normal_entropy(mu, sigma):\n    \"\"\"\n    Calculate entropy of multivariate normal distribution\n    \n    Mathematical implementation:\n    H(X) = (p/2)log(2πe) + (1/2)log|Σ|\n    \n    Parameters:\n    mu: array, mean vector\n    sigma: array, covariance matrix\n    \n    Returns:\n    float: entropy\n    \"\"\"\n    p = len(mu)\n    det_sigma = np.linalg.det(sigma)\n    \n    entropy = (p/2) * np.log(2*np.pi*np.e) + (1/2) * np.log(det_sigma)\n    return entropy\n\ndef multivariate_normal_kl_divergence(mu1, sigma1, mu2, sigma2):\n    \"\"\"\n    Calculate KL divergence between two multivariate normal distributions\n    \n    Mathematical implementation:\n    KL(N₁||N₂) = (1/2)[tr(Σ₂⁻¹Σ₁) + (μ₂-μ₁)ᵀΣ₂⁻¹(μ₂-μ₁) - p - log(|Σ₁|/|Σ₂|)]\n    \n    Parameters:\n    mu1, sigma1: parameters of first distribution\n    mu2, sigma2: parameters of second distribution\n    \n    Returns:\n    float: KL divergence\n    \"\"\"\n    p = len(mu1)\n    \n    sigma2_inv = np.linalg.inv(sigma2)\n    diff = mu2 - mu1\n    \n    term1 = np.trace(sigma2_inv @ sigma1)\n    term2 = diff.T @ sigma2_inv @ diff\n    term3 = np.log(np.linalg.det(sigma2) / np.linalg.det(sigma1))\n    \n    kl_div = (1/2) * (term1 + term2 - p - term3)\n    return kl_div\n\n# Example: 2-dimensional multivariate normal\nnp.random.seed(42)\n\n# Parameters\nmu = np.array([2.0, 3.0])\nsigma = np.array([[4.0, 1.5],\n                  [1.5, 2.0]])\n\nprint(\"Multivariate Normal Distribution Analysis\")\nprint(\"=\" * 50)\n\n# Generate samples\nn_samples = 1000\nX = generate_multivariate_normal(n_samples, mu, sigma, method='cholesky')\n\nprint(f\"Generated {n_samples} samples from N₂(μ, Σ)\")\nprint(f\"μ = {mu}\")\nprint(f\"Σ = \\n{sigma}\")\n\n# Calculate MLE\nmu_hat, sigma_hat = multivariate_normal_mle(X)\nprint(f\"\\nMaximum Likelihood Estimates:\")\nprint(f\"μ̂ = {mu_hat}\")\nprint(f\"Σ̂ = \\n{sigma_hat}\")\n\n# Calculate log-likelihood\nlog_likelihood = multivariate_normal_log_likelihood(X, mu_hat, sigma_hat)\nprint(f\"Log-likelihood: {log_likelihood:.4f}\")\n\n# Calculate entropy\nentropy = multivariate_normal_entropy(mu, sigma)\nprint(f\"Entropy: {entropy:.4f}\")\n\n# Calculate Mahalanobis distances\nmahal_distances = np.array([mahalanobis_distance(x, mu, sigma) for x in X])\nprint(f\"Mean Mahalanobis distance: {np.mean(mahal_distances):.4f}\")\nprint(f\"Mahalanobis distance variance: {np.var(mahal_distances):.4f}\")\n\n# Test theoretical properties\nprint(f\"\\nTheoretical Properties Verification:\")\n\n# 1. Marginal distributions\nprint(f\"1. Marginal distributions:\")\nprint(f\"   X₁ ~ N({mu[0]}, {sigma[0,0]})\")\nprint(f\"   X₂ ~ N({mu[1]}, {sigma[1,1]})\")\nprint(f\"   Sample means: {np.mean(X, axis=0)}\")\nprint(f\"   Sample variances: {np.var(X, axis=0)}\")\n\n# 2. Correlation\ntheoretical_corr = sigma[0,1] / np.sqrt(sigma[0,0] * sigma[1,1])\nsample_corr = np.corrcoef(X.T)[0,1]\nprint(f\"2. Correlation:\")\nprint(f\"   Theoretical: {theoretical_corr:.4f}\")\nprint(f\"   Sample: {sample_corr:.4f}\")\n\n# 3. Mahalanobis distance distribution\n# Should follow χ²(2) distribution\nchi2_quantiles = chi2.ppf(np.linspace(0.01, 0.99, 100), df=2)\nmahal_squared = mahal_distances**2\nmahal_quantiles = np.percentile(mahal_squared, np.linspace(1, 99, 100))\n\nprint(f\"3. Mahalanobis distance squared ~ χ²(2):\")\nprint(f\"   Sample mean: {np.mean(mahal_squared):.4f} (theoretical: 2.0)\")\nprint(f\"   Sample variance: {np.var(mahal_squared):.4f} (theoretical: 4.0)\")\n\n# Conditional distribution example\nprint(f\"\\nConditional Distribution Example:\")\nx2_condition = 4.0  # Condition on X₂ = 4.0\ncond_mu, cond_sigma = conditional_multivariate_normal(x2_condition, mu, sigma, [1])\nprint(f\"X₁ | X₂ = {x2_condition} ~ N({cond_mu[0]:.4f}, {cond_sigma[0,0]:.4f})\")\n\n# Generate conditional samples\nconditional_samples = np.random.normal(cond_mu[0], np.sqrt(cond_sigma[0,0]), 100)\nprint(f\"Conditional sample mean: {np.mean(conditional_samples):.4f}\")\nprint(f\"Conditional sample variance: {np.var(conditional_samples):.4f}\")\n\n# Visualize multivariate normal\nplt.figure(figsize=(15, 12))\n\n# 1. Scatter plot with contours\nplt.subplot(2, 3, 1)\nplt.scatter(X[:, 0], X[:, 1], alpha=0.6, s=20)\n\n# Create contour plot\nx1_range = np.linspace(mu[0] - 3*np.sqrt(sigma[0,0]), mu[0] + 3*np.sqrt(sigma[0,0]), 100)\nx2_range = np.linspace(mu[1] - 3*np.sqrt(sigma[1,1]), mu[1] + 3*np.sqrt(sigma[1,1]), 100)\nX1, X2 = np.meshgrid(x1_range, x2_range)\nZ = np.zeros_like(X1)\n\nfor i in range(X1.shape[0]):\n    for j in range(X1.shape[1]):\n        x = np.array([X1[i,j], X2[i,j]])\n        Z[i,j] = multivariate_normal_pdf(x, mu, sigma)\n\nplt.contour(X1, X2, Z, levels=10, colors='red', alpha=0.7)\nplt.xlabel('X₁')\nplt.ylabel('X₂')\nplt.title('Multivariate Normal Samples with Contours')\nplt.grid(True, alpha=0.3)\n\n# 2. Marginal distributions\nplt.subplot(2, 3, 2)\nplt.hist(X[:, 0], bins=30, alpha=0.7, density=True, label='X₁')\nplt.hist(X[:, 1], bins=30, alpha=0.7, density=True, label='X₂')\n\n# Theoretical marginal PDFs\nx1_theoretical = np.linspace(mu[0] - 4*np.sqrt(sigma[0,0]), mu[0] + 4*np.sqrt(sigma[0,0]), 100)\nx2_theoretical = np.linspace(mu[1] - 4*np.sqrt(sigma[1,1]), mu[1] + 4*np.sqrt(sigma[1,1]), 100)\n\npdf1 = (1/np.sqrt(2*np.pi*sigma[0,0])) * np.exp(-0.5*(x1_theoretical - mu[0])**2/sigma[0,0])\npdf2 = (1/np.sqrt(2*np.pi*sigma[1,1])) * np.exp(-0.5*(x2_theoretical - mu[1])**2/sigma[1,1])\n\nplt.plot(x1_theoretical, pdf1, 'r-', linewidth=2, label='X₁ PDF')\nplt.plot(x2_theoretical, pdf2, 'g-', linewidth=2, label='X₂ PDF')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.title('Marginal Distributions')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# 3. Mahalanobis distance distribution\nplt.subplot(2, 3, 3)\nplt.hist(mahal_squared, bins=30, alpha=0.7, density=True, label='Sample')\n\n# Theoretical χ²(2) distribution\nchi2_x = np.linspace(0, np.max(mahal_squared), 100)\nchi2_pdf = chi2.pdf(chi2_x, df=2)\nplt.plot(chi2_x, chi2_pdf, 'r-', linewidth=2, label='χ²(2) PDF')\n\nplt.xlabel('Mahalanobis Distance²')\nplt.ylabel('Density')\nplt.title('Mahalanobis Distance Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# 4. Q-Q plot for Mahalanobis distances\nplt.subplot(2, 3, 4)\nfrom scipy.stats import probplot\nprobplot(mahal_squared, dist=chi2, sparams=(2,), plot=plt)\nplt.title('Q-Q Plot: Mahalanobis Distance² vs χ²(2)')\nplt.grid(True, alpha=0.3)\n\n# 5. Correlation structure\nplt.subplot(2, 3, 5)\ncorrelation_matrix = np.corrcoef(X.T)\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n            xticklabels=['X₁', 'X₂'], yticklabels=['X₁', 'X₂'])\nplt.title('Sample Correlation Matrix')\n\n# 6. 3D surface plot of PDF\nax = plt.subplot(2, 3, 6, projection='3d')\nsurf = ax.plot_surface(X1, X2, Z, cmap='viridis', alpha=0.8)\nax.set_xlabel('X₁')\nax.set_ylabel('X₂')\nax.set_zlabel('PDF')\nax.set_title('Multivariate Normal PDF Surface')\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate linear transformation properties\nprint(f\"\\nLinear Transformation Properties:\")\n\n# Define transformation: Y = AX + b\nA = np.array([[1.5, 0.5],\n              [0.5, 1.0]])\nb = np.array([1.0, 2.0])\n\n# Theoretical transformed parameters\nmu_transformed = A @ mu + b\nsigma_transformed = A @ sigma @ A.T\n\nprint(f\"Transformation: Y = AX + b\")\nprint(f\"A = \\n{A}\")\nprint(f\"b = {b}\")\nprint(f\"Theoretical μ_Y = {mu_transformed}\")\nprint(f\"Theoretical Σ_Y = \\n{sigma_transformed}\")\n\n# Apply transformation to samples\nY = X @ A.T + b\n\n# Calculate sample parameters\nmu_Y_hat, sigma_Y_hat = multivariate_normal_mle(Y)\nprint(f\"Sample μ_Y = {mu_Y_hat}\")\nprint(f\"Sample Σ_Y = \\n{sigma_Y_hat}\")\n\n# Verify transformation property\ntransformation_error_mu = np.linalg.norm(mu_transformed - mu_Y_hat)\ntransformation_error_sigma = np.linalg.norm(sigma_transformed - sigma_Y_hat)\nprint(f\"Transformation property verification:\")\nprint(f\"  μ error: {transformation_error_mu:.6f}\")\nprint(f\"  Σ error: {transformation_error_sigma:.6f}\")\n\n# KL divergence example\nprint(f\"\\nKL Divergence Example:\")\n# Create two different multivariate normal distributions\nmu2 = np.array([3.0, 4.0])\nsigma2 = np.array([[3.0, 1.0],\n                   [1.0, 2.5]])\n\nkl_div_12 = multivariate_normal_kl_divergence(mu, sigma, mu2, sigma2)\nkl_div_21 = multivariate_normal_kl_divergence(mu2, sigma2, mu, sigma)\n\nprint(f\"KL(N₁||N₂) = {kl_div_12:.4f}\")\nprint(f\"KL(N₂||N₁) = {kl_div_21:.4f}\")\nprint(f\"KL divergence is asymmetric: {abs(kl_div_12 - kl_div_21) > 1e-10}\")\n\n# Entropy comparison\nentropy1 = multivariate_normal_entropy(mu, sigma)\nentropy2 = multivariate_normal_entropy(mu2, sigma2)\n\nprint(f\"Entropy N₁: {entropy1:.4f}\")\nprint(f\"Entropy N₂: {entropy2:.4f}\")\nprint(f\"Entropy difference: {abs(entropy1 - entropy2):.4f}\")\n\n# Visualize transformation\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X[:, 0], X[:, 1], alpha=0.6, s=20, label='Original X')\nplt.scatter(Y[:, 0], Y[:, 1], alpha=0.6, s=20, label='Transformed Y')\nplt.xlabel('X₁ / Y₁')\nplt.ylabel('X₂ / Y₂')\nplt.title('Linear Transformation')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\n# Show how transformation affects the distribution\nplt.hist(X[:, 0], bins=30, alpha=0.7, density=True, label='X₁')\nplt.hist(Y[:, 0], bins=30, alpha=0.7, density=True, label='Y₁')\n\n# Theoretical PDFs\nx1_pdf = (1/np.sqrt(2*np.pi*sigma[0,0])) * np.exp(-0.5*(x1_theoretical - mu[0])**2/sigma[0,0])\ny1_theoretical = np.linspace(mu_transformed[0] - 4*np.sqrt(sigma_transformed[0,0]), \n                            mu_transformed[0] + 4*np.sqrt(sigma_transformed[0,0]), 100)\ny1_pdf = (1/np.sqrt(2*np.pi*sigma_transformed[0,0])) * np.exp(-0.5*(y1_theoretical - mu_transformed[0])**2/sigma_transformed[0,0])\n\nplt.plot(x1_theoretical, x1_pdf, 'r-', linewidth=2, label='X₁ PDF')\nplt.plot(y1_theoretical, y1_pdf, 'g-', linewidth=2, label='Y₁ PDF')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.title('Marginal Distribution Transformation')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate independence property\nprint(f\"\\nIndependence Property:\")\n# Create uncorrelated multivariate normal\nsigma_uncorr = np.array([[4.0, 0.0],\n                         [0.0, 2.0]])\n\nX_uncorr = generate_multivariate_normal(n_samples, mu, sigma_uncorr)\ncorr_uncorr = np.corrcoef(X_uncorr.T)[0,1]\n\nprint(f\"Uncorrelated case:\")\nprint(f\"  Theoretical correlation: 0.0\")\nprint(f\"  Sample correlation: {corr_uncorr:.6f}\")\nprint(f\"  Components independent: {abs(corr_uncorr) < 0.1}\")\n\n# Test independence by checking if joint PDF equals product of marginals\ndef test_independence(X, mu, sigma):\n    \"\"\"Test independence by comparing joint and marginal PDFs\"\"\"\n    n_test = 100\n    test_points = generate_multivariate_normal(n_test, mu, sigma)\n    \n    joint_pdf_values = np.array([multivariate_normal_pdf(x, mu, sigma) for x in test_points])\n    \n    # Calculate product of marginal PDFs\n    marginal_pdf_values = np.ones(n_test)\n    for i, x in enumerate(test_points):\n        pdf1 = (1/np.sqrt(2*np.pi*sigma[0,0])) * np.exp(-0.5*(x[0] - mu[0])**2/sigma[0,0])\n        pdf2 = (1/np.sqrt(2*np.pi*sigma[1,1])) * np.exp(-0.5*(x[1] - mu[1])**2/sigma[1,1])\n        marginal_pdf_values[i] = pdf1 * pdf2\n    \n    # Calculate correlation between joint and marginal PDFs\n    independence_corr = np.corrcoef(joint_pdf_values, marginal_pdf_values)[0,1]\n    return independence_corr\n\nindependence_corr_uncorr = test_independence(X_uncorr, mu, sigma_uncorr)\nindependence_corr_corr = test_independence(X, mu, sigma)\n\nprint(f\"Independence test (correlation between joint and marginal PDFs):\")\nprint(f\"  Uncorrelated case: {independence_corr_uncorr:.6f} (should be ≈ 1.0)\")\nprint(f\"  Correlated case: {independence_corr_corr:.6f} (should be < 1.0)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Applications\n\n### Customer Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_customer_data(n_customers=500):\n    \"\"\"Generate customer data for segmentation\"\"\"\n    \n    np.random.seed(42)\n    \n    # Generate customer features\n    age = np.random.normal(35, 10, n_customers)\n    income = np.random.normal(50000, 20000, n_customers)\n    spending = np.random.normal(2000, 800, n_customers)\n    frequency = np.random.poisson(5, n_customers)\n    satisfaction = np.random.uniform(1, 10, n_customers)\n    \n    # Create customer segments\n    segment_1 = (age < 30) & (income > 60000)\n    segment_2 = (age > 45) & (spending > 2500)\n    segment_3 = (frequency > 7) & (satisfaction > 8)\n    \n    segments = np.zeros(n_customers)\n    segments[segment_1] = 1\n    segments[segment_2] = 2\n    segments[segment_3] = 3\n    \n    # Create DataFrame\n    customer_data = pd.DataFrame({\n        'age': age,\n        'income': income,\n        'spending': spending,\n        'frequency': frequency,\n        'satisfaction': satisfaction,\n        'segment': segments\n    })\n    \n    return customer_data\n\ncustomer_df = generate_customer_data()\n\nprint(\"Customer Segmentation Analysis\")\nprint(f\"Data shape: {customer_df.shape}\")\nprint(f\"Number of segments: {len(customer_df['segment'].unique())}\")\n\n# Perform customer segmentation\nfeatures = ['age', 'income', 'spending', 'frequency', 'satisfaction']\nX_customers = customer_df[features].values\ny_customers = customer_df['segment'].values\n\n# Standardize features\nscaler_customers = StandardScaler()\nX_customers_scaled = scaler_customers.fit_transform(X_customers)\n\n# Perform clustering\nkmeans_customers = KMeans(n_clusters=4, random_state=42)\nkmeans_customers.fit(X_customers_scaled)\ncustomer_clusters = kmeans_customers.labels_\n\n# Visualize customer segmentation\nplt.figure(figsize=(15, 10))\n\n# Age vs Income\nplt.subplot(2, 3, 1)\nplt.scatter(customer_df['age'], customer_df['income'], c=customer_clusters, \n           cmap='viridis', alpha=0.7)\nplt.xlabel('Age')\nplt.ylabel('Income')\nplt.title('Age vs Income (Clusters)')\n\n# Spending vs Frequency\nplt.subplot(2, 3, 2)\nplt.scatter(customer_df['spending'], customer_df['frequency'], c=customer_clusters, \n           cmap='viridis', alpha=0.7)\nplt.xlabel('Spending')\nplt.ylabel('Frequency')\nplt.title('Spending vs Frequency (Clusters)')\n\n# PCA projection\npca_customers = PCA(n_components=2)\npca_customers_result = pca_customers.fit_transform(X_customers_scaled)\n\nplt.subplot(2, 3, 3)\nplt.scatter(pca_customers_result[:, 0], pca_customers_result[:, 1], c=customer_clusters, \n           cmap='viridis', alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Projection (Clusters)')\n\n# Cluster characteristics\nplt.subplot(2, 3, 4)\ncluster_means = []\nfor i in range(4):\n    cluster_mask = customer_clusters == i\n    cluster_means.append(customer_df[features][cluster_mask].mean())\n\ncluster_means_df = pd.DataFrame(cluster_means, columns=features)\nsns.heatmap(cluster_means_df.T, annot=True, cmap='coolwarm', center=0, \n            square=True, linewidths=0.5, fmt='.1f')\nplt.title('Cluster Characteristics')\n\n# Feature importance in clustering\nplt.subplot(2, 3, 5)\nfeature_importance = np.abs(kmeans_customers.cluster_centers_).mean(axis=0)\nplt.bar(features, feature_importance, alpha=0.7, color='orange')\nplt.xlabel('Features')\nplt.ylabel('Average Cluster Center Magnitude')\nplt.title('Feature Importance in Clustering')\nplt.xticks(rotation=45)\n\n# Cluster sizes\nplt.subplot(2, 3, 6)\ncluster_sizes = [np.sum(customer_clusters == i) for i in range(4)]\nplt.bar(range(4), cluster_sizes, alpha=0.7, color='purple')\nplt.xlabel('Cluster')\nplt.ylabel('Number of Customers')\nplt.title('Cluster Sizes')\n\nplt.tight_layout()\nplt.show()\n\n# Customer segment profiles\nprint(\"\\nCustomer Segment Profiles:\")\nfor i in range(4):\n    cluster_mask = customer_clusters == i\n    cluster_data = customer_df[cluster_mask]\n    print(f\"\\nSegment {i+1} (n={len(cluster_data)}):\")\n    print(f\"  Average age: {cluster_data['age'].mean():.1f}\")\n    print(f\"  Average income: ${cluster_data['income'].mean():,.0f}\")\n    print(f\"  Average spending: ${cluster_data['spending'].mean():,.0f}\")\n    print(f\"  Average frequency: {cluster_data['frequency'].mean():.1f}\")\n    print(f\"  Average satisfaction: {cluster_data['satisfaction'].mean():.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Problems\n\n1. **Dimensionality Reduction**: Implement PCA with automatic component selection and visualization.\n\n2. **Clustering Evaluation**: Create comprehensive clustering evaluation frameworks with multiple metrics.\n\n3. **Feature Engineering**: Build automated feature engineering pipelines using multivariate techniques.\n\n4. **Anomaly Detection**: Develop multivariate anomaly detection methods using clustering and dimensionality reduction.\n\n## Further Reading\n\n- \"Multivariate Data Analysis\" by Hair, Black, Babin, and Anderson\n- \"Applied Multivariate Statistical Analysis\" by Johnson and Wichern\n- \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n- \"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman\n\n## Key Takeaways\n\n- **PCA** reduces dimensionality while preserving variance\n- **Factor Analysis** identifies latent variables underlying observed features\n- **Clustering** groups similar observations without predefined labels\n- **Discriminant Analysis** finds optimal projections for classification\n- **Canonical Correlation** analyzes relationships between two sets of variables\n- **Multivariate techniques** are essential for high-dimensional data analysis\n- **Feature engineering** benefits greatly from multivariate statistical methods\n- **Real-world applications** include customer segmentation, feature selection, and data exploration\n\nIn the next chapter, we'll explore Bayesian statistics, including Bayesian inference, MCMC methods, and their applications in machine learning."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}