{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Topics\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Pandas](https://img.shields.io/badge/Pandas-1.3+-blue.svg)](https://pandas.pydata.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.4+-orange.svg)](https://matplotlib.org/)\n[![Seaborn](https://img.shields.io/badge/Seaborn-0.11+-blue.svg)](https://seaborn.pydata.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-green.svg)](https://scipy.org/)\n[![Statsmodels](https://img.shields.io/badge/Statsmodels-0.13+-blue.svg)](https://www.statsmodels.org/)\n[![Lifelines](https://img.shields.io/badge/Lifelines-0.27+-orange.svg)](https://lifelines.readthedocs.io/)\n\n# Chapter 10: Advanced Topics\n\n## Overview\n\nThis chapter explores advanced statistical methods that are essential for specialized data science and AI/ML applications. Topics include non-parametric methods, survival analysis, mixed models, causal inference, robust statistics, and their practical uses.\n\n## Learning Objectives\n- Apply non-parametric statistical tests\n- Analyze time-to-event data with survival analysis\n- Understand and use mixed-effects models\n- Grasp causal inference techniques\n- Use robust statistics for outlier-prone data\n\n## Prerequisites\n- Familiarity with hypothesis testing, regression, and Python libraries (scipy, statsmodels, lifelines)\n\n## Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols, mixedlm\nfrom lifelines import KaplanMeierFitter, CoxPHFitter\nsns.set(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 1. Non-Parametric Methods\n\nNon-parametric tests do not assume a specific data distribution. They are especially useful when data are ordinal, not normally distributed, or when sample sizes are small. These methods rely on the ranks or order of the data rather than the actual values.\n\n### 1.1 Wilcoxon Rank-Sum Test (Mann-Whitney U)\n\n**Mathematical Concept:**\nThe Mann-Whitney U test compares two independent samples to assess whether their population distributions differ. It tests the null hypothesis that the probability of an observation from one group exceeding an observation from the other group is 0.5.\n\n- **Null Hypothesis (H₀):** The distributions of both groups are equal.\n- **Alternative Hypothesis (H₁):** The distributions are not equal.\n\n**Test Statistic:**\n1. Combine all observations and rank them.\n2. Calculate the sum of ranks for each group.\n3. Compute the U statistic for each group:\n   $$ U_1 = n_1 n_2 + \\frac{n_1(n_1+1)}{2} - R_1 $$\n   $$ U_2 = n_1 n_2 + \\frac{n_2(n_2+1)}{2} - R_2 $$\n   Where $R_1$ and $R_2$ are the sums of ranks for groups 1 and 2, and $n_1$, $n_2$ are their sizes.\n4. The smaller U is used for significance testing.\n\n**Python Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare two independent samples\nnp.random.seed(42)\ngroup1 = np.random.normal(0, 1, 30)\ngroup2 = np.random.normal(0.5, 1, 30)\nstat, p = stats.mannwhitneyu(group1, group2)\nprint(f\"Mann-Whitney U: stat={stat:.2f}, p-value={p:.4f}\")\n\n# Manual calculation of ranks and U statistic\nfrom scipy.stats import rankdata\ndata = np.concatenate([group1, group2])\nranks = rankdata(data)\nR1 = np.sum(ranks[:30])\nR2 = np.sum(ranks[30:])\nU1 = 30*30 + 30*31/2 - R1\nU2 = 30*30 + 30*31/2 - R2\nprint(f\"Manual U1: {U1:.2f}, U2: {U2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Kruskal-Wallis Test\n\n**Mathematical Concept:**\nThe Kruskal-Wallis test generalizes the Mann-Whitney U test to more than two groups. It tests whether samples originate from the same distribution.\n\n- **Null Hypothesis (H₀):** All groups have the same distribution.\n- **Alternative Hypothesis (H₁):** At least one group differs.\n\n**Test Statistic:**\n1. Combine all data and rank them.\n2. Calculate the sum of ranks $R_j$ for each group $j$.\n3. Compute:\n   $$ H = \\frac{12}{N(N+1)} \\sum_{j=1}^k \\frac{R_j^2}{n_j} - 3(N+1) $$\n   Where $N$ is the total number of observations, $n_j$ is the size of group $j$.\n4. Under H₀, $H$ follows a chi-square distribution with $k-1$ degrees of freedom.\n\n**Python Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare more than two groups\nsample1 = np.random.normal(0, 1, 30)\nsample2 = np.random.normal(0.5, 1, 30)\nsample3 = np.random.normal(1, 1, 30)\nstat, p = stats.kruskal(sample1, sample2, sample3)\nprint(f\"Kruskal-Wallis: stat={stat:.2f}, p-value={p:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Permutation Test\n\n**Mathematical Concept:**\nA permutation test assesses the significance of an observed statistic by comparing it to the distribution of statistics computed from data with randomly permuted group labels.\n\n- **Null Hypothesis (H₀):** The group labels are exchangeable (no effect).\n- **Alternative Hypothesis (H₁):** The group labels are not exchangeable (effect exists).\n\n**Python Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def permutation_test(x, y, n_permutations=10000):\n    observed = np.abs(np.mean(x) - np.mean(y))\n    combined = np.concatenate([x, y])\n    count = 0\n    for _ in range(n_permutations):\n        np.random.shuffle(combined)\n        new_x = combined[:len(x)]\n        new_y = combined[len(x):]\n        if np.abs(np.mean(new_x) - np.mean(new_y)) >= observed:\n            count += 1\n    return count / n_permutations\n\np_perm = permutation_test(group1, group2)\nprint(f\"Permutation test p-value: {p_perm:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n- Non-parametric tests are robust to outliers and non-normality, but may have less power than parametric tests when assumptions are met.\n- Always visualize your data and check assumptions before choosing a test.\n\n---\n\n## 2. Survival Analysis\n\nSurvival analysis models time-to-event data, which is common in medical studies, reliability engineering, and customer churn analysis. The key feature is handling censored data—cases where the event of interest has not occurred for some subjects during the observation period.\n\n### 2.1 Kaplan-Meier Estimator\n\n**Mathematical Concept:**\nThe Kaplan-Meier estimator is a non-parametric statistic used to estimate the survival function $S(t)$ from lifetime data.\n\n- **Survival Function:**\n  $$ S(t) = P(T > t) $$\n  where $T$ is the time until the event (e.g., death, failure).\n\n- **Estimator:**\n  $$ \\hat{S}(t) = \\prod_{t_i \\leq t} \\left(1 - \\frac{d_i}{n_i}\\right) $$\n  where $t_i$ are event times, $d_i$ is the number of events at $t_i$, and $n_i$ is the number at risk just before $t_i$.\n\n- **Censoring:**\n  Censored data are those for which the event has not occurred by the end of the study or loss to follow-up. The Kaplan-Meier estimator properly accounts for right-censored data.\n\n**Python Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lifelines import KaplanMeierFitter\nnp.random.seed(42)\ntimes = np.random.exponential(10, 100)\nevent_observed = np.random.binomial(1, 0.8, 100)  # 80% events, 20% censored\nkmf = KaplanMeierFitter()\nkmf.fit(times, event_observed)\n\n# Plot the survival function\nkmf.plot_survival_function()\nplt.title('Kaplan-Meier Survival Curve')\nplt.xlabel('Time')\nplt.ylabel('Survival Probability')\nplt.show()\n\n# Print survival probabilities at specific times\ntime_points = [5, 10, 15]\nfor t in time_points:\n    print(f\"Estimated survival at t={t}: {kmf.survival_function_at_times(t).values[0]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Cox Proportional Hazards Model\n\n**Mathematical Concept:**\nThe Cox model is a semi-parametric regression model for the hazard function:\n\n- **Hazard Function:**\n  $$ h(t|X) = h_0(t) \\exp(\\beta^T X) $$\n  where $h_0(t)$ is the baseline hazard, $X$ are covariates, and $\\beta$ are coefficients.\n\n- **Interpretation:**\n  The exponentiated coefficients $\\exp(\\beta_j)$ represent the hazard ratio for a one-unit increase in $X_j$.\n\n- **Partial Likelihood:**\n  The Cox model estimates $\\beta$ by maximizing the partial likelihood, which does not require specifying $h_0(t)$.\n\n**Python Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lifelines import CoxPHFitter\ndf = pd.DataFrame({'time': times, 'event': event_observed, 'age': np.random.randint(30, 70, 100)})\ncph = CoxPHFitter()\ncph.fit(df, duration_col='time', event_col='event')\ncph.print_summary()  # Shows coefficients, hazard ratios, and p-values\n\n# Predict survival for a new subject\nnew_subject = pd.DataFrame({'age': [50]})\npred_surv = cph.predict_survival_function(new_subject, times=np.arange(0, 30, 1))\npred_surv.plot()\nplt.title('Predicted Survival Curve for Age 50')\nplt.xlabel('Time')\nplt.ylabel('Survival Probability')\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n- The Kaplan-Meier estimator provides a non-parametric estimate of survival over time, useful for visualizing and comparing groups.\n- The Cox model allows for covariate adjustment and quantifies the effect of predictors on the hazard rate.\n- Always check the proportional hazards assumption when using the Cox model (see lifelines documentation for diagnostics).\n\n---\n\n## 3. Mixed Models (Hierarchical/Random Effects)\n\nMixed models, also known as hierarchical or multilevel models, are used when data are grouped or clustered (e.g., repeated measures, students within schools). They account for both fixed effects (parameters associated with the entire population) and random effects (parameters that vary by group or cluster).\n\n### Mathematical Concept\n- **Model Structure:**\n  $$ y_{ij} = \\beta_0 + u_j + \\epsilon_{ij} $$\n  where:\n  - $y_{ij}$: observation $i$ in group $j$\n  - $\\beta_0$: overall intercept (fixed effect)\n  - $u_j \\sim N(0, \\sigma_u^2)$: random effect for group $j$\n  - $\\epsilon_{ij} \\sim N(0, \\sigma^2)$: residual error\n\n- **Interpretation:**\n  - Fixed effects estimate the average relationship across all groups.\n  - Random effects capture group-specific deviations from the average.\n\n- **Variance Components:**\n  - $\\sigma_u^2$: variance between groups\n  - $\\sigma^2$: variance within groups\n\n- **Intraclass Correlation Coefficient (ICC):**\n  $$ ICC = \\frac{\\sigma_u^2}{\\sigma_u^2 + \\sigma^2} $$\n  Measures the proportion of total variance attributable to group-level differences.\n\n### Python Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate data for 10 groups, 10 observations each\nnp.random.seed(42)\ngroups = np.repeat(np.arange(10), 10)\n# Random effect for each group\ngroup_effects = np.random.normal(0, 2, 10)\nvalues = 2 * groups + group_effects[groups] + np.random.normal(0, 2, 100)\ndf = pd.DataFrame({'group': groups, 'value': values})\n\n# Fit mixed model: value ~ 1 + (1 | group)\nfrom statsmodels.formula.api import mixedlm\nmodel = mixedlm('value ~ 1', df, groups=df['group'])\nresult = model.fit()\nprint(result.summary())\n\n# Extract variance components\nvar_group = result.cov_re.iloc[0, 0]\nvar_resid = result.scale\nicc = var_group / (var_group + var_resid)\nprint(f\"Intraclass Correlation Coefficient (ICC): {icc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n- The fixed effect (Intercept) estimates the overall mean.\n- The random effect variance shows how much groups differ from each other.\n- ICC quantifies the degree of similarity within groups.\n- Mixed models are essential for repeated measures, longitudinal data, and nested data structures in ML/AI experiments.\n\n---\n\n## 4. Causal Inference\n\nCausal inference aims to estimate the effect of an intervention or treatment, going beyond correlation to answer \"what if\" questions. This is crucial in AI/ML for understanding the impact of features, policies, or actions.\n\n### 4.1 Propensity Score Matching (PSM)\n\n**Mathematical Concept:**\n- The propensity score $e(X)$ is the probability of receiving treatment given covariates $X$:\n  $$ e(X) = P(T=1|X) $$\n- Matching treated and control units with similar propensity scores simulates a randomized experiment, reducing confounding bias.\n\n**Steps:**\n1. Estimate propensity scores (e.g., logistic regression).\n2. Match treated and control units with similar scores.\n3. Compare outcomes between matched groups to estimate the average treatment effect (ATE).\n\n**Python Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import NearestNeighbors\n\n# Simulate data\nnp.random.seed(42)\nN = 200\nX = np.random.normal(0, 1, (N, 2))\nT = np.random.binomial(1, 1/(1 + np.exp(-X[:,0] + 0.5*X[:,1])))\nY = 2*T + X[:,0] + np.random.normal(0, 1, N)\n\n# Estimate propensity scores\nmodel = LogisticRegression()\nmodel.fit(X, T)\npropensity = model.predict_proba(X)[:,1]\n\n# Match each treated unit to nearest control by propensity score\ntreated_idx = np.where(T == 1)[0]\ncontrol_idx = np.where(T == 0)[0]\nmatcher = NearestNeighbors(n_neighbors=1).fit(propensity[control_idx].reshape(-1,1))\n_, indices = matcher.kneighbors(propensity[treated_idx].reshape(-1,1))\nmatched_controls = control_idx[indices.flatten()]\n\n# Estimate ATE\nate = np.mean(Y[treated_idx] - Y[matched_controls])\nprint(f\"Estimated ATE by PSM: {ate:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Instrumental Variables (IV)\n\n**Mathematical Concept:**\n- IV methods address unmeasured confounding by using an instrument $Z$ that affects treatment $T$ but not the outcome $Y$ directly (except through $T$).\n- The classic two-stage least squares (2SLS) procedure:\n  1. Regress $T$ on $Z$ to get predicted treatment $\\hat{T}$.\n  2. Regress $Y$ on $\\hat{T}$ to estimate the causal effect.\n\n**Assumptions:**\n- Relevance: $Z$ is correlated with $T$.\n- Exclusion: $Z$ affects $Y$ only through $T$.\n- Independence: $Z$ is independent of unmeasured confounders.\n\n**Python Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n# Simulate data\nnp.random.seed(42)\nN = 200\nZ = np.random.binomial(1, 0.5, N)\nT = 0.8*Z + np.random.normal(0, 1, N)\nY = 2*T + np.random.normal(0, 1, N)\n\n# First stage: T ~ Z\nT_hat = sm.OLS(T, sm.add_constant(Z)).fit().predict(sm.add_constant(Z))\n# Second stage: Y ~ T_hat\niv_result = sm.OLS(Y, sm.add_constant(T_hat)).fit()\nprint(iv_result.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Example: Causal Impact (with DoWhy)\n\n**DoWhy** is a Python library for causal inference that provides a unified interface for modeling, identification, estimation, and refutation of causal effects.\n\n**Python Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install: pip install dowhy\nimport dowhy\n# See DoWhy documentation for detailed examples:\n# https://microsoft.github.io/dowhy/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n- Causal inference methods help estimate the effect of interventions, policies, or features in observational data.\n- Always check assumptions (e.g., no unmeasured confounding for PSM, valid instrument for IV).\n- Use graphical models (causal diagrams) to clarify assumptions and guide analysis.\n\n---\n\n## 5. Robust Statistics\n\nRobust statistics are designed to be less sensitive to outliers and violations of model assumptions (such as normality). They provide more reliable estimates when data contain anomalies or are not well-behaved.\n\n### 5.1 Median and Median Absolute Deviation (MAD)\n\n**Mathematical Concept:**\n- **Median:** The value separating the higher half from the lower half of a data sample. It is a robust measure of central tendency.\n- **Median Absolute Deviation (MAD):**\n  $$ \\text{MAD} = \\text{median}(|x_i - \\text{median}(x)|) $$\n  MAD is a robust measure of statistical dispersion.\n- **Scaling:** For normal data, $\\text{MAD} \\times 1.4826$ estimates the standard deviation.\n\n**Python Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Median and Median Absolute Deviation\nx = np.random.normal(0, 1, 100)\nx[::10] += 10  # Add outliers\nmedian = np.median(x)\nmad = stats.median_abs_deviation(x)\nprint(f\"Median: {median:.2f}, MAD: {mad:.2f}\")\n\n# Compare to mean and standard deviation\nmean = np.mean(x)\nstd = np.std(x)\nprint(f\"Mean: {mean:.2f}, Std: {std:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Robust Regression\n\n**Mathematical Concept:**\n- **Ordinary Least Squares (OLS):** Minimizes the sum of squared residuals, sensitive to outliers.\n- **Robust Regression (e.g., M-estimators):** Minimizes a function less sensitive to large residuals (e.g., Huber loss).\n- **Huber Loss:**\n  $$ L(\\delta) = \\begin{cases} \\frac{1}{2}\\delta^2 & \\text{if } |\\delta| \\leq \\epsilon \\\\ \\epsilon(|\\delta| - \\frac{1}{2}\\epsilon) & \\text{if } |\\delta| > \\epsilon \\end{cases} $$\n\n**Python Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\nnp.random.seed(42)\nX = np.random.randn(100, 1)\ny = 2 * X.flatten() + np.random.normal(0, 1, 100)\ny[::10] += 10  # Add outliers\nX = sm.add_constant(X)\n\n# OLS regression\nols_model = sm.OLS(y, X).fit()\nprint(\"OLS coefficients:\", ols_model.params)\n\n# Robust regression (Huber loss)\nrobust_model = sm.RLM(y, X)\nrobust_results = robust_model.fit()\nprint(\"Robust coefficients:\", robust_results.params)\n\n# Compare fits visually\nimport matplotlib.pyplot as plt\nplt.scatter(X[:,1], y, alpha=0.6, label='Data with outliers')\nplt.plot(X[:,1], ols_model.predict(X), color='red', label='OLS fit')\nplt.plot(X[:,1], robust_results.predict(X), color='green', label='Robust fit')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.title('OLS vs Robust Regression')\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n- The median and MAD provide robust alternatives to the mean and standard deviation.\n- Robust regression methods (like RLM) yield more reliable parameter estimates in the presence of outliers.\n- Always compare robust and classical methods to assess the influence of outliers in your data.\n\n---\n\n## 6. Practice Problems\n\n1. **Non-Parametric Testing:**\n   - Simulate two groups of data, one normal and one skewed. Use the Mann-Whitney U test to compare their medians. Interpret the result and visualize the distributions.\n   - For three or more groups with different variances, apply the Kruskal-Wallis test. Discuss when you would prefer this over ANOVA.\n\n2. **Survival Analysis:**\n   - Generate synthetic time-to-event data with censoring. Fit a Kaplan-Meier curve and interpret the survival probability at a given time. Compare survival between two groups (e.g., treatment vs. control) using the log-rank test.\n   - Fit a Cox proportional hazards model with at least one covariate. Interpret the hazard ratio and check the proportional hazards assumption (e.g., using Schoenfeld residuals in lifelines).\n\n3. **Mixed Models:**\n   - Simulate repeated measures data (e.g., students tested multiple times). Fit a mixed-effects model and interpret both fixed and random effects. Calculate and interpret the intraclass correlation coefficient (ICC).\n   - Compare the results of a mixed model to a standard linear regression. Discuss the consequences of ignoring group structure.\n\n4. **Causal Inference:**\n   - Simulate observational data with a confounder. Estimate the average treatment effect (ATE) using propensity score matching. Visualize the distribution of propensity scores before and after matching.\n   - Use an instrumental variable approach to estimate a causal effect in the presence of unmeasured confounding. Clearly state and justify the instrument's validity.\n\n5. **Robust Statistics:**\n   - Create a dataset with outliers. Compare the mean and standard deviation to the median and MAD. Discuss which is more appropriate and why.\n   - Fit both OLS and robust regression models to data with outliers. Visualize and interpret the differences in fitted lines and coefficients.\n\n**Guidance:**\n- For each problem, write out the mathematical formulation, perform the analysis in Python, and interpret the results in the context of real-world data science or AI/ML applications.\n- Consider edge cases (e.g., small sample sizes, high censoring, strong confounding, extreme outliers) and discuss how robust or advanced methods address them.\n\n## 7. Further Reading\n\n- **\"Survival Analysis: A Self-Learning Text\" by Kleinbaum & Klein**  \n  A comprehensive and accessible introduction to survival analysis, including practical examples and exercises.\n\n- **\"Causal Inference in Statistics: A Primer\" by Pearl, Glymour, & Jewell**  \n  An essential guide to the logic and mathematics of causal inference, with clear explanations and real-world examples.\n\n- **\"Robust Statistics\" by Huber & Ronchetti**  \n  The definitive reference on robust statistical methods, covering theory, algorithms, and applications.\n\n- **DoWhy Documentation** ([https://microsoft.github.io/dowhy/](https://microsoft.github.io/dowhy/))  \n  Official documentation for the DoWhy Python library, with tutorials and case studies on causal inference in Python.\n\n- **lifelines Documentation** ([https://lifelines.readthedocs.io/](https://lifelines.readthedocs.io/))  \n  User guide and API reference for the lifelines library, including advanced survival analysis techniques and diagnostics.\n\n- **\"Applied Longitudinal Analysis\" by Fitzmaurice, Laird, & Ware**  \n  A practical resource for mixed models and longitudinal data analysis, with a focus on applications in health and social sciences.\n\n- **\"The Book of Why\" by Judea Pearl**  \n  An accessible and thought-provoking exploration of causality, counterfactuals, and the future of AI.\n\n- **\"Modern Applied Statistics with S\" by Venables & Ripley**  \n  A classic text covering a wide range of advanced statistical methods, with practical code examples (much of which translates to Python).\n\n**Tip:**\n- When exploring advanced topics, always consult both theoretical and applied resources. Try to implement methods from scratch in Python to deepen your understanding.\n\n---\n\n## 8. Key Takeaways\n- Non-parametric methods are powerful for non-normal or ordinal data\n- Survival analysis is essential for time-to-event data\n- Mixed models handle hierarchical and repeated measures data\n- Causal inference requires careful design and specialized methods\n- Robust statistics protect against outliers and model violations\n\n---\n\n**Congratulations!** You have completed the comprehensive statistics guide for AI/ML and data science."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}