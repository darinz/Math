{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regression Analysis\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Pandas](https://img.shields.io/badge/Pandas-1.3+-blue.svg)](https://pandas.pydata.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.4+-orange.svg)](https://matplotlib.org/)\n[![Seaborn](https://img.shields.io/badge/Seaborn-0.11+-blue.svg)](https://seaborn.pydata.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-green.svg)](https://scipy.org/)\n[![Statsmodels](https://img.shields.io/badge/Statsmodels-0.13+-blue.svg)](https://www.statsmodels.org/)\n[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)\n\nRegression analysis is a fundamental statistical technique for modeling relationships between variables. It's essential for prediction, understanding causal relationships, and making data-driven decisions in AI/ML.\n\n## Table of Contents\n- [Simple Linear Regression](#simple-linear-regression)\n- [Multiple Linear Regression](#multiple-linear-regression)\n- [Model Diagnostics](#model-diagnostics)\n- [Variable Selection](#variable-selection)\n- [Polynomial Regression](#polynomial-regression)\n- [Logistic Regression](#logistic-regression)\n- [Practical Applications](#practical-applications)\n\n## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.metrics import r2_score, mean_squared_error, classification_report\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\nnp.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Linear Regression\n\nSimple linear regression models the relationship between a dependent variable (Y) and a single independent variable (X) using a linear function.\n\n### Mathematical Foundation\n\n**Model Specification:**\n$$Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\quad i = 1, 2, \\ldots, n$$\n\nWhere:\n- $Y_i$ is the dependent variable for observation i\n- $X_i$ is the independent variable for observation i\n- $\\beta_0$ is the intercept (y-intercept)\n- $\\beta_1$ is the slope coefficient\n- $\\epsilon_i$ is the error term (residual)\n\n**Assumptions:**\n1. **Linearity**: The relationship between X and Y is linear\n2. **Independence**: Observations are independent\n3. **Homoscedasticity**: Error variance is constant\n4. **Normality**: Errors are normally distributed\n5. **No multicollinearity**: Not applicable for simple regression\n\n**Least Squares Estimation:**\nThe goal is to minimize the sum of squared residuals:\n$$\\min_{\\beta_0, \\beta_1} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2$$\n\n**Normal Equations:**\n$$\\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2 = 0$$\n$$\\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2 = 0$$\n\n**Solution:**\n$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}$$\n$$\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1} \\bar{X}$$\n\n**Derivation of Slope Coefficient:**\nStarting with the normal equation for β₁:\n$$\\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i) X_i = 0$$\n$$\\sum_{i=1}^{n} Y_i X_i - \\beta_0 \\sum_{i=1}^{n} X_i - \\beta_1 \\sum_{i=1}^{n} X_i^2 = 0$$\n\nSubstituting $\\beta_0 = \\bar{Y} - \\beta_1 \\bar{X}$:\n$$\\sum_{i=1}^{n} Y_i X_i - (\\bar{Y} - \\beta_1 \\bar{X}) \\sum_{i=1}^{n} X_i - \\beta_1 \\sum_{i=1}^{n} X_i^2 = 0$$\n$$\\sum_{i=1}^{n} Y_i X_i - \\bar{Y} \\sum_{i=1}^{n} X_i + \\beta_1 \\bar{X} \\sum_{i=1}^{n} X_i - \\beta_1 \\sum_{i=1}^{n} X_i^2 = 0$$\n$$\\sum_{i=1}^{n} Y_i X_i - n \\bar{Y} \\bar{X} = \\beta_1 (\\sum_{i=1}^{n} X_i^2 - n \\bar{X}^2)$$\n$$\\beta_1 = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_linear_regression(X, y):\n    \"\"\"\n    Perform simple linear regression using least squares\n    \n    Mathematical implementation:\n    β₁ = Cov(X,Y) / Var(X)\n    β₀ = Ȳ - β₁X̄\n    \n    Parameters:\n    X: array-like, independent variable\n    y: array-like, dependent variable\n    \n    Returns:\n    dict: regression results\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    \n    n = len(X)\n    \n    # Calculate means\n    X_mean = np.mean(X)\n    y_mean = np.mean(y)\n    \n    # Calculate slope (β₁)\n    numerator = np.sum((X - X_mean) * (y - y_mean))\n    denominator = np.sum((X - X_mean) ** 2)\n    \n    if denominator == 0:\n        raise ValueError(\"X has zero variance\")\n    \n    beta_1 = numerator / denominator\n    \n    # Calculate intercept (β₀)\n    beta_0 = y_mean - beta_1 * X_mean\n    \n    # Calculate predicted values\n    y_pred = beta_0 + beta_1 * X\n    \n    # Calculate residuals\n    residuals = y - y_pred\n    \n    # Calculate R-squared\n    ss_res = np.sum(residuals ** 2)\n    ss_tot = np.sum((y - y_mean) ** 2)\n    r_squared = 1 - (ss_res / ss_tot)\n    \n    # Calculate standard errors\n    mse = ss_res / (n - 2)  # Mean squared error\n    se_beta_1 = np.sqrt(mse / np.sum((X - X_mean) ** 2))\n    se_beta_0 = np.sqrt(mse * (1/n + X_mean**2 / np.sum((X - X_mean) ** 2)))\n    \n    return {\n        'intercept': beta_0,\n        'slope': beta_1,\n        'r_squared': r_squared,\n        'residuals': residuals,\n        'y_pred': y_pred,\n        'se_intercept': se_beta_0,\n        'se_slope': se_beta_1,\n        'mse': mse,\n        'n': n\n    }\n\ndef calculate_correlation(X, y):\n    \"\"\"\n    Calculate correlation coefficient\n    \n    Mathematical implementation:\n    r = Cov(X,Y) / (σ_X × σ_Y)\n    \n    Parameters:\n    X: array-like, independent variable\n    y: array-like, dependent variable\n    \n    Returns:\n    float: correlation coefficient\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Calculate means\n    X_mean = np.mean(X)\n    y_mean = np.mean(y)\n    \n    # Calculate correlation\n    numerator = np.sum((X - X_mean) * (y - y_mean))\n    denominator = np.sqrt(np.sum((X - X_mean) ** 2) * np.sum((y - y_mean) ** 2))\n    \n    if denominator == 0:\n        return 0\n    \n    return numerator / denominator\n\n# Generate sample data\nn_samples = 100\nX = np.random.uniform(0, 10, n_samples)\ntrue_slope = 2.5\ntrue_intercept = 1.0\nnoise_std = 1.5\n\n# True relationship: Y = 1.0 + 2.5*X + ε\ny_true = true_intercept + true_slope * X\ny = y_true + np.random.normal(0, noise_std, n_samples)\n\n# Perform regression\nresults = simple_linear_regression(X, y)\ncorrelation = calculate_correlation(X, y)\n\nprint(\"Simple Linear Regression Results:\")\nprint(f\"True intercept: {true_intercept:.2f}\")\nprint(f\"Estimated intercept: {results['intercept']:.4f}\")\nprint(f\"True slope: {true_slope:.2f}\")\nprint(f\"Estimated slope: {results['slope']:.4f}\")\nprint(f\"R-squared: {results['r_squared']:.4f}\")\nprint(f\"Correlation coefficient: {correlation:.4f}\")\nprint(f\"Standard error (intercept): {results['se_intercept']:.4f}\")\nprint(f\"Standard error (slope): {results['se_slope']:.4f}\")\n\n# Verify mathematical relationship: R² = r² for simple linear regression\nprint(f\"R² = r²: {abs(results['r_squared'] - correlation**2) < 1e-10}\")\n\n# Visualize the regression\nplt.figure(figsize=(15, 10))\n\n# Plot 1: Scatter plot with regression line\nplt.subplot(2, 3, 1)\nplt.scatter(X, y, alpha=0.6, color='blue', label='Data points')\nplt.plot(X, y_true, 'r--', linewidth=2, label='True relationship')\nplt.plot(X, results['y_pred'], 'g-', linewidth=2, label='Fitted line')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Simple Linear Regression')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Residuals vs X\nplt.subplot(2, 3, 2)\nplt.scatter(X, results['residuals'], alpha=0.6, color='orange')\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel('X')\nplt.ylabel('Residuals')\nplt.title('Residuals vs X')\nplt.grid(True, alpha=0.3)\n\n# Plot 3: Residuals vs Predicted\nplt.subplot(2, 3, 3)\nplt.scatter(results['y_pred'], results['residuals'], alpha=0.6, color='green')\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel('Predicted Y')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Predicted')\nplt.grid(True, alpha=0.3)\n\n# Plot 4: Q-Q plot of residuals\nplt.subplot(2, 3, 4)\nfrom scipy.stats import probplot\nprobplot(results['residuals'], dist=\"norm\", plot=plt)\nplt.title('Q-Q Plot of Residuals')\nplt.grid(True, alpha=0.3)\n\n# Plot 5: Histogram of residuals\nplt.subplot(2, 3, 5)\nplt.hist(results['residuals'], bins=15, alpha=0.7, color='purple', edgecolor='black')\nplt.xlabel('Residuals')\nplt.ylabel('Frequency')\nplt.title('Histogram of Residuals')\nplt.grid(True, alpha=0.3)\n\n# Plot 6: Leverage plot (studentized residuals)\nplt.subplot(2, 3, 6)\n# Calculate leverage\nX_with_const = np.column_stack([np.ones(len(X)), X])\nH = X_with_const @ np.linalg.inv(X_with_const.T @ X_with_const) @ X_with_const.T\nleverage = np.diag(H)\n\n# Calculate studentized residuals\nmse = results['mse']\nstudentized_residuals = results['residuals'] / np.sqrt(mse * (1 - leverage))\n\nplt.scatter(leverage, studentized_residuals, alpha=0.6, color='brown')\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel('Leverage')\nplt.ylabel('Studentized Residuals')\nplt.title('Leverage Plot')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate the mathematical relationship between correlation and slope\nprint(f\"\\nMathematical Relationship:\")\nprint(f\"Correlation coefficient (r): {correlation:.4f}\")\nprint(f\"Slope coefficient (β₁): {results['slope']:.4f}\")\nprint(f\"Standard deviation of X: {np.std(X):.4f}\")\nprint(f\"Standard deviation of Y: {np.std(y):.4f}\")\nprint(f\"r × (σ_Y / σ_X) = {correlation * np.std(y) / np.std(X):.4f}\")\nprint(f\"β₁ = r × (σ_Y / σ_X): {abs(results['slope'] - correlation * np.std(y) / np.std(X)) < 1e-10}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Statistical Inference in Regression\n\n**Hypothesis Testing for Slope:**\n- **Null Hypothesis**: H₀: β₁ = 0 (no linear relationship)\n- **Alternative Hypothesis**: H₁: β₁ ≠ 0 (linear relationship exists)\n\n**Test Statistic:**\n$$t = \\frac{\\hat{\\beta_1} - 0}{\\text{SE}(\\hat{\\beta_1})} \\sim t_{n-2}$$\n\n**Confidence Interval for Slope:**\n$$\\hat{\\beta_1} \\pm t_{\\alpha/2, n-2} \\times \\text{SE}(\\hat{\\beta_1})$$\n\n**Prediction Interval:**\nFor a new observation X₀:\n$$\\hat{Y}_0 \\pm t_{\\alpha/2, n-2} \\times \\sqrt{\\text{MSE} \\left(1 + \\frac{1}{n} + \\frac{(X_0 - \\bar{X})^2}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}\\right)}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def regression_inference(X, y, alpha=0.05):\n    \"\"\"\n    Perform statistical inference for regression parameters\n    \n    Mathematical implementation:\n    t = β₁ / SE(β₁)\n    CI = β₁ ± t_{α/2, n-2} × SE(β₁)\n    \n    Parameters:\n    X: array-like, independent variable\n    y: array-like, dependent variable\n    alpha: float, significance level\n    \n    Returns:\n    dict: inference results\n    \"\"\"\n    results = simple_linear_regression(X, y)\n    \n    # Degrees of freedom\n    df = results['n'] - 2\n    \n    # T-statistic for slope\n    t_stat_slope = results['slope'] / results['se_slope']\n    \n    # P-value for slope (two-tailed)\n    from scipy.stats import t\n    p_value_slope = 2 * (1 - t.cdf(abs(t_stat_slope), df))\n    \n    # Critical value\n    t_critical = t.ppf(1 - alpha/2, df)\n    \n    # Confidence intervals\n    ci_slope_lower = results['slope'] - t_critical * results['se_slope']\n    ci_slope_upper = results['slope'] + t_critical * results['se_slope']\n    \n    ci_intercept_lower = results['intercept'] - t_critical * results['se_intercept']\n    ci_intercept_upper = results['intercept'] + t_critical * results['se_intercept']\n    \n    return {\n        't_statistic_slope': t_stat_slope,\n        'p_value_slope': p_value_slope,\n        'ci_slope': (ci_slope_lower, ci_slope_upper),\n        'ci_intercept': (ci_intercept_lower, ci_intercept_upper),\n        't_critical': t_critical,\n        'degrees_of_freedom': df,\n        **results\n    }\n\ndef prediction_interval(X, y, X_new, alpha=0.05):\n    \"\"\"\n    Calculate prediction interval for new observations\n    \n    Mathematical implementation:\n    PI = Ŷ₀ ± t_{α/2, n-2} × √(MSE × (1 + 1/n + (X₀-X̄)²/SSX))\n    \n    Parameters:\n    X: array-like, independent variable\n    y: array-like, dependent variable\n    X_new: array-like, new X values\n    alpha: float, significance level\n    \n    Returns:\n    tuple: (predictions, lower_bounds, upper_bounds)\n    \"\"\"\n    results = simple_linear_regression(X, y)\n    \n    # Degrees of freedom\n    df = results['n'] - 2\n    \n    # Critical value\n    from scipy.stats import t\n    t_critical = t.ppf(1 - alpha/2, df)\n    \n    # Calculate predictions\n    y_pred_new = results['intercept'] + results['slope'] * X_new\n    \n    # Calculate prediction intervals\n    X_mean = np.mean(X)\n    ssx = np.sum((X - X_mean) ** 2)\n    \n    # Standard error of prediction\n    se_pred = np.sqrt(results['mse'] * (1 + 1/results['n'] + (X_new - X_mean)**2 / ssx))\n    \n    # Prediction intervals\n    lower_bounds = y_pred_new - t_critical * se_pred\n    upper_bounds = y_pred_new + t_critical * se_pred\n    \n    return y_pred_new, lower_bounds, upper_bounds\n\n# Perform inference\ninference_results = regression_inference(X, y, alpha=0.05)\n\nprint(\"Statistical Inference Results:\")\nprint(f\"T-statistic for slope: {inference_results['t_statistic_slope']:.4f}\")\nprint(f\"P-value for slope: {inference_results['p_value_slope']:.4f}\")\nprint(f\"95% CI for slope: ({inference_results['ci_slope'][0]:.4f}, {inference_results['ci_slope'][1]:.4f})\")\nprint(f\"95% CI for intercept: ({inference_results['ci_intercept'][0]:.4f}, {inference_results['ci_intercept'][1]:.4f})\")\nprint(f\"Degrees of freedom: {inference_results['degrees_of_freedom']}\")\n\n# Test if slope is significantly different from zero\nalpha = 0.05\nif inference_results['p_value_slope'] < alpha:\n    print(f\"Reject H₀: Slope is significantly different from zero (p < {alpha})\")\nelse:\n    print(f\"Fail to reject H₀: No evidence that slope differs from zero (p ≥ {alpha})\")\n\n# Calculate prediction intervals\nX_new = np.linspace(0, 10, 50)\ny_pred_new, lower_bounds, upper_bounds = prediction_interval(X, y, X_new, alpha=0.05)\n\n# Visualize inference results\nplt.figure(figsize=(15, 5))\n\n# Plot 1: Regression with confidence and prediction intervals\nplt.subplot(1, 3, 1)\nplt.scatter(X, y, alpha=0.6, color='blue', label='Data points')\nplt.plot(X_new, y_pred_new, 'g-', linewidth=2, label='Regression line')\nplt.fill_between(X_new, lower_bounds, upper_bounds, alpha=0.3, color='red', label='95% Prediction interval')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Regression with Prediction Intervals')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 2: T-distribution with critical region\nplt.subplot(1, 3, 2)\nfrom scipy.stats import t\nt_range = np.linspace(-4, 4, 1000)\nt_pdf = t.pdf(t_range, inference_results['degrees_of_freedom'])\n\nplt.plot(t_range, t_pdf, 'b-', linewidth=2, label=f't-distribution (df={inference_results[\"degrees_of_freedom\"]})')\nplt.axvline(inference_results['t_statistic_slope'], color='red', linestyle='--', \n            label=f't = {inference_results[\"t_statistic_slope\"]:.3f}')\nplt.axvline(inference_results['t_critical'], color='orange', linestyle=':', \n            label=f'Critical value = {inference_results[\"t_critical\"]:.3f}')\nplt.axvline(-inference_results['t_critical'], color='orange', linestyle=':', \n            label=f'Critical value = -{inference_results[\"t_critical\"]:.3f}')\nplt.fill_between(t_range, t_pdf, where=(t_range > inference_results['t_critical']) | (t_range < -inference_results['t_critical']), \n                 alpha=0.3, color='red', label='Rejection region')\nplt.xlabel('t')\nplt.ylabel('Probability Density')\nplt.title('T-Distribution')\nplt.legend()\n\n# Plot 3: Confidence intervals\nplt.subplot(1, 3, 3)\nparameters = ['Intercept', 'Slope']\nestimates = [inference_results['intercept'], inference_results['slope']]\nci_lower = [inference_results['ci_intercept'][0], inference_results['ci_slope'][0]]\nci_upper = [inference_results['ci_intercept'][1], inference_results['ci_slope'][1]]\n\nx_pos = np.arange(len(parameters))\nplt.errorbar(x_pos, estimates, yerr=[estimates[i] - ci_lower[i] for i in range(len(parameters))], \n             fmt='o', capsize=5, capthick=2, linewidth=2, label='95% CI')\nplt.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='H₀: β = 0')\nplt.xticks(x_pos, parameters)\nplt.ylabel('Parameter Estimate')\nplt.title('Parameter Estimates with 95% CI')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate the relationship between R² and correlation\nprint(f\"\\nRelationship between R² and correlation:\")\nprint(f\"R²: {inference_results['r_squared']:.4f}\")\nprint(f\"Correlation coefficient: {correlation:.4f}\")\nprint(f\"Correlation squared: {correlation**2:.4f}\")\nprint(f\"R² = r²: {abs(inference_results['r_squared'] - correlation**2) < 1e-10}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Diagnostics\n\n**Residual Analysis:**\n1. **Normality**: Residuals should be normally distributed\n2. **Independence**: Residuals should be independent\n3. **Homoscedasticity**: Residual variance should be constant\n4. **Linearity**: Relationship should be linear\n\n**Influence Diagnostics:**\n- **Leverage**: Measures how far an observation is from the center of X\n- **Cook's Distance**: Measures the influence of each observation\n- **DFFITS**: Measures the influence on fitted values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def regression_diagnostics(X, y):\n    \"\"\"\n    Perform comprehensive regression diagnostics\n    \n    Parameters:\n    X: array-like, independent variable\n    y: array-like, dependent variable\n    \n    Returns:\n    dict: diagnostic results\n    \"\"\"\n    results = simple_linear_regression(X, y)\n    \n    # Calculate leverage\n    X_with_const = np.column_stack([np.ones(len(X)), X])\n    H = X_with_const @ np.linalg.inv(X_with_const.T @ X_with_const) @ X_with_const.T\n    leverage = np.diag(H)\n    \n    # Calculate studentized residuals\n    mse = results['mse']\n    studentized_residuals = results['residuals'] / np.sqrt(mse * (1 - leverage))\n    \n    # Calculate Cook's distance\n    p = 2  # Number of parameters\n    cook_distance = (studentized_residuals**2 / p) * (leverage / (1 - leverage))\n    \n    # Calculate DFFITS\n    dffits = studentized_residuals * np.sqrt(leverage / (1 - leverage))\n    \n    # Test for normality (Shapiro-Wilk)\n    from scipy.stats import shapiro\n    shapiro_stat, shapiro_p = shapiro(results['residuals'])\n    \n    # Test for homoscedasticity (Breusch-Pagan)\n    # Using a simplified version\n    squared_residuals = results['residuals']**2\n    X_with_const_resid = np.column_stack([np.ones(len(X)), X])\n    try:\n        from scipy.stats import f\n        # Calculate R² for squared residuals\n        res_results = simple_linear_regression(X, squared_residuals)\n        bp_stat = res_results['r_squared'] * len(X)\n        bp_p = 1 - chi2.cdf(bp_stat, 1)  # 1 degree of freedom\n    except:\n        bp_stat, bp_p = np.nan, np.nan\n    \n    return {\n        'leverage': leverage,\n        'studentized_residuals': studentized_residuals,\n        'cook_distance': cook_distance,\n        'dffits': dffits,\n        'shapiro_stat': shapiro_stat,\n        'shapiro_p': shapiro_p,\n        'bp_stat': bp_stat,\n        'bp_p': bp_p,\n        **results\n    }\n\n# Perform diagnostics\ndiagnostics = regression_diagnostics(X, y)\n\nprint(\"Regression Diagnostics:\")\nprint(f\"Shapiro-Wilk test for normality:\")\nprint(f\"  Statistic: {diagnostics['shapiro_stat']:.4f}\")\nprint(f\"  P-value: {diagnostics['shapiro_p']:.4f}\")\nprint(f\"  Normality assumption: {'Rejected' if diagnostics['shapiro_p'] < 0.05 else 'Not rejected'}\")\n\nprint(f\"\\nBreusch-Pagan test for homoscedasticity:\")\nprint(f\"  Statistic: {diagnostics['bp_stat']:.4f}\")\nprint(f\"  P-value: {diagnostics['bp_p']:.4f}\")\nprint(f\"  Homoscedasticity assumption: {'Rejected' if diagnostics['bp_p'] < 0.05 else 'Not rejected'}\")\n\n# Identify influential observations\nhigh_leverage = diagnostics['leverage'] > 2 * (2 + 1) / len(X)  # 2(p+1)/n\nhigh_cook = diagnostics['cook_distance'] > 4 / len(X)  # 4/n\nhigh_dffits = abs(diagnostics['dffits']) > 2 * np.sqrt(2 / len(X))  # 2√(2/n)\n\nprint(f\"\\nInfluential Observations:\")\nprint(f\"High leverage: {np.sum(high_leverage)} observations\")\nprint(f\"High Cook's distance: {np.sum(high_cook)} observations\")\nprint(f\"High DFFITS: {np.sum(high_dffits)} observations\")\n\n# Visualize diagnostics\nplt.figure(figsize=(15, 10))\n\n# Plot 1: Residuals vs Fitted\nplt.subplot(2, 3, 1)\nplt.scatter(diagnostics['y_pred'], diagnostics['residuals'], alpha=0.6, color='blue')\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Fitted')\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Q-Q plot\nplt.subplot(2, 3, 2)\nprobplot(diagnostics['residuals'], dist=\"norm\", plot=plt)\nplt.title('Q-Q Plot of Residuals')\nplt.grid(True, alpha=0.3)\n\n# Plot 3: Leverage plot\nplt.subplot(2, 3, 3)\nplt.scatter(diagnostics['leverage'], diagnostics['studentized_residuals'], alpha=0.6, color='green')\nplt.axhline(y=0, color='red', linestyle='--')\nplt.axhline(y=2, color='orange', linestyle=':', alpha=0.7)\nplt.axhline(y=-2, color='orange', linestyle=':', alpha=0.7)\nplt.xlabel('Leverage')\nplt.ylabel('Studentized Residuals')\nplt.title('Leverage Plot')\nplt.grid(True, alpha=0.3)\n\n# Plot 4: Cook's distance\nplt.subplot(2, 3, 4)\nplt.plot(diagnostics['cook_distance'], 'o-', alpha=0.7, color='purple')\nplt.axhline(y=4/len(X), color='red', linestyle='--', label='4/n threshold')\nplt.xlabel('Observation')\nplt.ylabel(\"Cook's Distance\")\nplt.title(\"Cook's Distance\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 5: DFFITS\nplt.subplot(2, 3, 5)\nplt.plot(diagnostics['dffits'], 'o-', alpha=0.7, color='brown')\nthreshold = 2 * np.sqrt(2 / len(X))\nplt.axhline(y=threshold, color='red', linestyle='--', label=f'±{threshold:.3f} threshold')\nplt.axhline(y=-threshold, color='red', linestyle='--')\nplt.xlabel('Observation')\nplt.ylabel('DFFITS')\nplt.title('DFFITS')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 6: Scale-location plot\nplt.subplot(2, 3, 6)\nsqrt_abs_residuals = np.sqrt(np.abs(diagnostics['residuals']))\nplt.scatter(diagnostics['y_pred'], sqrt_abs_residuals, alpha=0.6, color='orange')\nplt.xlabel('Fitted Values')\nplt.ylabel('√|Residuals|')\nplt.title('Scale-Location Plot')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary of diagnostic findings\nprint(f\"\\nDiagnostic Summary:\")\nprint(f\"✓ Linearity: Check residuals vs fitted plot\")\nprint(f\"✓ Normality: {'✓' if diagnostics['shapiro_p'] >= 0.05 else '✗'} (Shapiro-Wilk p = {diagnostics['shapiro_p']:.4f})\")\nprint(f\"✓ Homoscedasticity: {'✓' if diagnostics['bp_p'] >= 0.05 else '✗'} (Breusch-Pagan p = {diagnostics['bp_p']:.4f})\")\nprint(f\"✓ Independence: Check for patterns in residuals vs fitted\")\nprint(f\"✓ No influential observations: {'✓' if np.sum(high_cook) == 0 else '✗'} ({np.sum(high_cook)} influential)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple Linear Regression\n\n### Multiple Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_multiple_regression_data(n=200):\n    \"\"\"Generate data for multiple regression\"\"\"\n    # Generate features\n    x1 = np.random.normal(0, 1, n)\n    x2 = np.random.normal(0, 1, n)\n    x3 = np.random.normal(0, 1, n)\n    \n    # True coefficients\n    true_coeffs = [2.0, -1.5, 0.8, 1.2]  # intercept, x1, x2, x3\n    \n    # Generate target with noise\n    noise = np.random.normal(0, 0.5, n)\n    y = true_coeffs[0] + true_coeffs[1] * x1 + true_coeffs[2] * x2 + true_coeffs[3] * x3 + noise\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'x1': x1,\n        'x2': x2,\n        'x3': x3,\n        'y': y\n    })\n    \n    return df, true_coeffs\n\ndf, true_coeffs = generate_multiple_regression_data()\n\n# Multiple regression with scikit-learn\nX = df[['x1', 'x2', 'x3']]\ny = df['y']\n\nmodel_multi = LinearRegression()\nmodel_multi.fit(X, y)\ny_pred_multi = model_multi.predict(X)\n\nprint(\"Multiple Linear Regression Results\")\nprint(f\"Intercept: {model_multi.intercept_:.3f}\")\nfor i, (feature, coef) in enumerate(zip(['x1', 'x2', 'x3'], model_multi.coef_)):\n    print(f\"{feature} coefficient: {coef:.3f} (true: {true_coeffs[i+1]:.3f})\")\n\nprint(f\"R-squared: {r2_score(y, y_pred_multi):.3f}\")\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y, y_pred_multi)):.3f}\")\n\n# Statsmodels multiple regression\nX_sm = sm.add_constant(X)\nsm_model_multi = sm.OLS(y, X_sm).fit()\n\nprint(\"\\nStatsmodels Multiple Regression\")\nprint(sm_model_multi.summary())\n\n# Visualize multiple regression\nplt.figure(figsize=(15, 5))\n\n# Actual vs Predicted\nplt.subplot(1, 3, 1)\nplt.scatter(y, y_pred_multi, alpha=0.7, color='skyblue')\nplt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)\nplt.xlabel('Actual Y')\nplt.ylabel('Predicted Y')\nplt.title('Actual vs Predicted')\n\n# Residuals\nplt.subplot(1, 3, 2)\nresiduals_multi = y - y_pred_multi\nplt.scatter(y_pred_multi, residuals_multi, alpha=0.7, color='lightgreen')\nplt.axhline(0, color='red', linestyle='--', alpha=0.7)\nplt.xlabel('Predicted Y')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\n\n# Coefficient comparison\nplt.subplot(1, 3, 3)\nfeatures = ['x1', 'x2', 'x3']\nestimated_coeffs = model_multi.coef_\ntrue_coeffs_features = true_coeffs[1:]\n\nx_pos = np.arange(len(features))\nwidth = 0.35\n\nplt.bar(x_pos - width/2, estimated_coeffs, width, label='Estimated', alpha=0.7)\nplt.bar(x_pos + width/2, true_coeffs_features, width, label='True', alpha=0.7)\nplt.xlabel('Features')\nplt.ylabel('Coefficients')\nplt.title('Coefficient Comparison')\nplt.xticks(x_pos, features)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Diagnostics\n\n### Assumption Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def regression_diagnostics(model, X, y, y_pred):\n    \"\"\"Perform comprehensive regression diagnostics\"\"\"\n    residuals = y - y_pred\n    \n    # 1. Linearity\n    linearity_test = sm.stats.diagnostic.linear_harvey_collier(model)\n    \n    # 2. Normality of residuals\n    normality_test = stats.shapiro(residuals)\n    \n    # 3. Homoscedasticity\n    homoscedasticity_test = sm.stats.diagnostic.het_breuschpagan(residuals, X)\n    \n    # 4. Independence (Durbin-Watson)\n    dw_stat = sm.stats.durbin_watson(residuals)\n    \n    # 5. Multicollinearity (VIF)\n    vif_data = []\n    for i in range(X.shape[1]):\n        vif = sm.stats.outliers_influence.variance_inflation_factor(X.values, i)\n        vif_data.append(vif)\n    \n    return {\n        'linearity_pvalue': linearity_test[1],\n        'normality_pvalue': normality_test[1],\n        'homoscedasticity_pvalue': homoscedasticity_test[1],\n        'durbin_watson': dw_stat,\n        'vif': vif_data\n    }\n\n# Perform diagnostics\nX_diag = sm.add_constant(X)\ndiagnostics = regression_diagnostics(sm_model_multi, X_diag, y, y_pred_multi)\n\nprint(\"Regression Diagnostics\")\nprint(f\"Linearity test p-value: {diagnostics['linearity_pvalue']:.3f}\")\nprint(f\"Normality test p-value: {diagnostics['normality_pvalue']:.3f}\")\nprint(f\"Homoscedasticity test p-value: {diagnostics['homoscedasticity_pvalue']:.3f}\")\nprint(f\"Durbin-Watson statistic: {diagnostics['durbin_watson']:.3f}\")\n\nprint(\"\\nVIF Values:\")\nfor i, feature in enumerate(['x1', 'x2', 'x3']):\n    print(f\"{feature}: {diagnostics['vif'][i+1]:.3f}\")\n\n# Visualize diagnostics\nplt.figure(figsize=(15, 10))\n\n# Residuals vs Fitted\nplt.subplot(2, 3, 1)\nplt.scatter(y_pred_multi, residuals_multi, alpha=0.7, color='skyblue')\nplt.axhline(0, color='red', linestyle='--', alpha=0.7)\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Fitted')\n\n# Q-Q plot\nplt.subplot(2, 3, 2)\nstats.probplot(residuals_multi, dist=\"norm\", plot=plt)\nplt.title('Q-Q Plot of Residuals')\n\n# Scale-Location plot\nplt.subplot(2, 3, 3)\nresiduals_abs = np.abs(residuals_multi)\nplt.scatter(y_pred_multi, residuals_abs, alpha=0.7, color='lightgreen')\nplt.xlabel('Fitted Values')\nplt.ylabel('|Residuals|')\nplt.title('Scale-Location Plot')\n\n# Residuals vs Leverage\nplt.subplot(2, 3, 4)\ninfluence = sm_model_multi.get_influence()\nleverage = influence.hat_matrix_diag\nplt.scatter(leverage, residuals_multi, alpha=0.7, color='orange')\nplt.xlabel('Leverage')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Leverage')\n\n# Cook's Distance\nplt.subplot(2, 3, 5)\ncooks_distance = influence.cooks_distance[0]\nplt.bar(range(len(cooks_distance)), cooks_distance, alpha=0.7, color='purple')\nplt.xlabel('Observation')\nplt.ylabel(\"Cook's Distance\")\nplt.title(\"Cook's Distance\")\n\n# Histogram of residuals\nplt.subplot(2, 3, 6)\nplt.hist(residuals_multi, bins=20, alpha=0.7, color='pink', edgecolor='black')\nplt.xlabel('Residuals')\nplt.ylabel('Frequency')\nplt.title('Residuals Distribution')\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Outlier Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_outliers(model, X, y, y_pred):\n    \"\"\"Detect outliers using various methods\"\"\"\n    residuals = y - y_pred\n    \n    # 1. Standardized residuals\n    std_residuals = residuals / np.std(residuals)\n    outliers_std = np.abs(std_residuals) > 3\n    \n    # 2. Studentized residuals\n    influence = model.get_influence()\n    studentized_residuals = influence.resid_studentized_external\n    outliers_studentized = np.abs(studentized_residuals) > 3\n    \n    # 3. Leverage\n    leverage = influence.hat_matrix_diag\n    leverage_threshold = 2 * (X.shape[1] + 1) / X.shape[0]\n    outliers_leverage = leverage > leverage_threshold\n    \n    # 4. Cook's Distance\n    cooks_distance = influence.cooks_distance[0]\n    cooks_threshold = 4 / X.shape[0]\n    outliers_cooks = cooks_distance > cooks_threshold\n    \n    return {\n        'std_residuals': std_residuals,\n        'studentized_residuals': studentized_residuals,\n        'leverage': leverage,\n        'cooks_distance': cooks_distance,\n        'outliers_std': outliers_std,\n        'outliers_studentized': outliers_studentized,\n        'outliers_leverage': outliers_leverage,\n        'outliers_cooks': outliers_cooks\n    }\n\noutlier_results = detect_outliers(sm_model_multi, X_diag, y, y_pred_multi)\n\nprint(\"Outlier Detection Results\")\nprint(f\"Outliers (standardized residuals): {np.sum(outlier_results['outliers_std'])}\")\nprint(f\"Outliers (studentized residuals): {np.sum(outlier_results['outliers_studentized'])}\")\nprint(f\"High leverage points: {np.sum(outlier_results['outliers_leverage'])}\")\nprint(f\"High Cook's distance: {np.sum(outlier_results['outliers_cooks'])}\")\n\n# Visualize outliers\nplt.figure(figsize=(15, 5))\n\n# Standardized residuals\nplt.subplot(1, 3, 1)\nplt.scatter(range(len(outlier_results['std_residuals'])), outlier_results['std_residuals'], \n           alpha=0.7, color='skyblue')\nplt.axhline(3, color='red', linestyle='--', alpha=0.7)\nplt.axhline(-3, color='red', linestyle='--', alpha=0.7)\nplt.xlabel('Observation')\nplt.ylabel('Standardized Residuals')\nplt.title('Standardized Residuals')\n\n# Leverage\nplt.subplot(1, 3, 2)\nplt.scatter(range(len(outlier_results['leverage'])), outlier_results['leverage'], \n           alpha=0.7, color='lightgreen')\nleverage_threshold = 2 * (X_diag.shape[1] + 1) / X_diag.shape[0]\nplt.axhline(leverage_threshold, color='red', linestyle='--', alpha=0.7)\nplt.xlabel('Observation')\nplt.ylabel('Leverage')\nplt.title('Leverage')\n\n# Cook's Distance\nplt.subplot(1, 3, 3)\nplt.bar(range(len(outlier_results['cooks_distance'])), outlier_results['cooks_distance'], \n        alpha=0.7, color='orange')\ncooks_threshold = 4 / X_diag.shape[0]\nplt.axhline(cooks_threshold, color='red', linestyle='--', alpha=0.7)\nplt.xlabel('Observation')\nplt.ylabel(\"Cook's Distance\")\nplt.title(\"Cook's Distance\")\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variable Selection\n\n### Stepwise Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stepwise_selection(X, y, direction='forward'):\n    \"\"\"Perform stepwise variable selection\"\"\"\n    features = list(X.columns)\n    selected_features = []\n    remaining_features = features.copy()\n    \n    if direction == 'forward':\n        while remaining_features:\n            best_feature = None\n            best_score = -np.inf\n            \n            for feature in remaining_features:\n                current_features = selected_features + [feature]\n                X_current = X[current_features]\n                X_current = sm.add_constant(X_current)\n                \n                model = sm.OLS(y, X_current).fit()\n                score = model.aic  # Lower AIC is better\n                \n                if score > best_score:\n                    best_score = score\n                    best_feature = feature\n            \n            if best_feature:\n                selected_features.append(best_feature)\n                remaining_features.remove(best_feature)\n                print(f\"Added {best_feature}, AIC: {best_score:.3f}\")\n            else:\n                break\n    \n    return selected_features\n\n# Generate more complex data for variable selection\ndef generate_complex_data(n=300):\n    \"\"\"Generate data with some irrelevant features\"\"\"\n    np.random.seed(42)\n    \n    # Relevant features\n    x1 = np.random.normal(0, 1, n)\n    x2 = np.random.normal(0, 1, n)\n    x3 = np.random.normal(0, 1, n)\n    \n    # Irrelevant features\n    x4 = np.random.normal(0, 1, n)\n    x5 = np.random.normal(0, 1, n)\n    x6 = np.random.normal(0, 1, n)\n    \n    # Target\n    y = 2.0 + 1.5 * x1 - 0.8 * x2 + 0.5 * x3 + np.random.normal(0, 0.5, n)\n    \n    df = pd.DataFrame({\n        'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4, 'x5': x5, 'x6': x6, 'y': y\n    })\n    \n    return df\n\ndf_complex = generate_complex_data()\nX_complex = df_complex[['x1', 'x2', 'x3', 'x4', 'x5', 'x6']]\ny_complex = df_complex['y']\n\nprint(\"Stepwise Variable Selection\")\nselected_features = stepwise_selection(X_complex, y_complex, direction='forward')\nprint(f\"Selected features: {selected_features}\")\n\n# Compare models\nX_full = sm.add_constant(X_complex)\nX_selected = sm.add_constant(X_complex[selected_features])\n\nmodel_full = sm.OLS(y_complex, X_full).fit()\nmodel_selected = sm.OLS(y_complex, X_selected).fit()\n\nprint(f\"\\nModel Comparison:\")\nprint(f\"Full model AIC: {model_full.aic:.3f}\")\nprint(f\"Selected model AIC: {model_selected.aic:.3f}\")\nprint(f\"Full model R²: {model_full.rsquared:.3f}\")\nprint(f\"Selected model R²: {model_selected.rsquared:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.preprocessing import StandardScaler\n\ndef regularization_comparison(X, y):\n    \"\"\"Compare different regularization methods\"\"\"\n    # Scale features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n    \n    # Models\n    models = {\n        'Linear': LinearRegression(),\n        'Ridge': Ridge(alpha=1.0),\n        'Lasso': Lasso(alpha=0.1),\n        'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)\n    }\n    \n    results = {}\n    \n    for name, model in models.items():\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        \n        results[name] = {\n            'r2': r2_score(y_test, y_pred),\n            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n            'coefficients': model.coef_ if hasattr(model, 'coef_') else None\n        }\n    \n    return results, models\n\nreg_results, reg_models = regularization_comparison(X_complex, y_complex)\n\nprint(\"Regularization Comparison\")\nfor name, result in reg_results.items():\n    print(f\"{name}: R² = {result['r2']:.3f}, RMSE = {result['rmse']:.3f}\")\n\n# Visualize coefficient shrinkage\nplt.figure(figsize=(12, 4))\n\n# Coefficient comparison\nplt.subplot(1, 2, 1)\nfeatures = X_complex.columns\nx_pos = np.arange(len(features))\nwidth = 0.2\n\nfor i, (name, result) in enumerate(reg_results.items()):\n    if result['coefficients'] is not None:\n        plt.bar(x_pos + i*width, result['coefficients'], width, label=name, alpha=0.7)\n\nplt.xlabel('Features')\nplt.ylabel('Coefficients')\nplt.title('Coefficient Comparison')\nplt.xticks(x_pos + width*1.5, features, rotation=45)\nplt.legend()\n\n# Performance comparison\nplt.subplot(1, 2, 2)\nmodels = list(reg_results.keys())\nr2_scores = [reg_results[name]['r2'] for name in models]\nrmse_scores = [reg_results[name]['rmse'] for name in models]\n\nx_pos = np.arange(len(models))\nplt.bar(x_pos - 0.2, r2_scores, 0.4, label='R²', alpha=0.7)\nplt.bar(x_pos + 0.2, rmse_scores, 0.4, label='RMSE', alpha=0.7)\nplt.xlabel('Models')\nplt.ylabel('Score')\nplt.title('Model Performance')\nplt.xticks(x_pos, models)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Polynomial Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def polynomial_regression_example():\n    \"\"\"Demonstrate polynomial regression\"\"\"\n    # Generate non-linear data\n    x = np.random.uniform(-3, 3, 100)\n    y = 2 + 3*x - 0.5*x**2 + 0.1*x**3 + np.random.normal(0, 0.5, 100)\n    \n    # Fit polynomial models of different degrees\n    degrees = [1, 2, 3, 4, 5]\n    models = {}\n    scores = {}\n    \n    for degree in degrees:\n        poly = PolynomialFeatures(degree=degree)\n        X_poly = poly.fit_transform(x.reshape(-1, 1))\n        \n        model = LinearRegression()\n        model.fit(X_poly, y)\n        y_pred = model.predict(X_poly)\n        \n        models[degree] = model\n        scores[degree] = {\n            'r2': r2_score(y, y_pred),\n            'rmse': np.sqrt(mean_squared_error(y, y_pred))\n        }\n    \n    return x, y, models, scores, degrees\n\nx_poly, y_poly, poly_models, poly_scores, degrees = polynomial_regression_example()\n\nprint(\"Polynomial Regression Results\")\nfor degree in degrees:\n    print(f\"Degree {degree}: R² = {poly_scores[degree]['r2']:.3f}, RMSE = {poly_scores[degree]['rmse']:.3f}\")\n\n# Visualize polynomial fits\nplt.figure(figsize=(15, 5))\n\n# Data and fits\nplt.subplot(1, 3, 1)\nplt.scatter(x_poly, y_poly, alpha=0.7, color='skyblue', label='Data')\n\nx_plot = np.linspace(-3, 3, 100)\nfor degree in [1, 2, 3]:\n    poly = PolynomialFeatures(degree=degree)\n    X_plot = poly.fit_transform(x_plot.reshape(-1, 1))\n    y_plot = poly_models[degree].predict(X_plot)\n    plt.plot(x_plot, y_plot, linewidth=2, label=f'Degree {degree}')\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Polynomial Fits')\nplt.legend()\n\n# R-squared vs degree\nplt.subplot(1, 3, 2)\nr2_values = [poly_scores[d]['r2'] for d in degrees]\nplt.plot(degrees, r2_values, 'bo-', linewidth=2, markersize=8)\nplt.xlabel('Polynomial Degree')\nplt.ylabel('R²')\nplt.title('R² vs Polynomial Degree')\nplt.grid(True, alpha=0.3)\n\n# RMSE vs degree\nplt.subplot(1, 3, 3)\nrmse_values = [poly_scores[d]['rmse'] for d in degrees]\nplt.plot(degrees, rmse_values, 'ro-', linewidth=2, markersize=8)\nplt.xlabel('Polynomial Degree')\nplt.ylabel('RMSE')\nplt.title('RMSE vs Polynomial Degree')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def logistic_regression_example():\n    \"\"\"Demonstrate logistic regression\"\"\"\n    # Generate binary classification data\n    np.random.seed(42)\n    n = 200\n    \n    # Features\n    x1 = np.random.normal(0, 1, n)\n    x2 = np.random.normal(0, 1, n)\n    \n    # True coefficients\n    true_coeffs = [0.5, 1.2, -0.8]  # intercept, x1, x2\n    \n    # Generate probabilities\n    logits = true_coeffs[0] + true_coeffs[1] * x1 + true_coeffs[2] * x2\n    probabilities = 1 / (1 + np.exp(-logits))\n    \n    # Generate binary outcomes\n    y = np.random.binomial(1, probabilities)\n    \n    # Create DataFrame\n    df_logistic = pd.DataFrame({\n        'x1': x1,\n        'x2': x2,\n        'y': y\n    })\n    \n    return df_logistic, true_coeffs\n\ndf_logistic, true_coeffs = logistic_regression_example()\n\n# Fit logistic regression\nX_logistic = df_logistic[['x1', 'x2']]\ny_logistic = df_logistic['y']\n\n# Scikit-learn\nmodel_logistic = LogisticRegression(random_state=42)\nmodel_logistic.fit(X_logistic, y_logistic)\ny_pred_logistic = model_logistic.predict(X_logistic)\ny_prob_logistic = model_logistic.predict_proba(X_logistic)[:, 1]\n\n# Statsmodels\nX_logistic_sm = sm.add_constant(X_logistic)\nsm_model_logistic = sm.Logit(y_logistic, X_logistic_sm).fit()\n\nprint(\"Logistic Regression Results\")\nprint(f\"Scikit-learn coefficients: {model_logistic.intercept_[0]:.3f}, {model_logistic.coef_[0]}\")\nprint(f\"True coefficients: {true_coeffs}\")\nprint(f\"Accuracy: {np.mean(y_pred_logistic == y_logistic):.3f}\")\n\nprint(\"\\nStatsmodels Results:\")\nprint(sm_model_logistic.summary())\n\n# Visualize logistic regression\nplt.figure(figsize=(15, 5))\n\n# Data with decision boundary\nplt.subplot(1, 3, 1)\nscatter = plt.scatter(X_logistic['x1'], X_logistic['x2'], c=y_logistic, cmap='RdYlBu', alpha=0.7)\n\n# Decision boundary\nx1_min, x1_max = X_logistic['x1'].min(), X_logistic['x1'].max()\nx2_min, x2_max = X_logistic['x2'].min(), X_logistic['x2'].max()\nxx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n                       np.linspace(x2_min, x2_max, 100))\nZ = model_logistic.predict(np.c_[xx1.ravel(), xx2.ravel()])\nZ = Z.reshape(xx1.shape)\nplt.contour(xx1, xx2, Z, levels=[0.5], colors='red', linewidths=2)\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.title('Logistic Regression Decision Boundary')\nplt.colorbar(scatter)\n\n# Probability surface\nplt.subplot(1, 3, 2)\nZ_prob = model_logistic.predict_proba(np.c_[xx1.ravel(), xx2.ravel()])[:, 1]\nZ_prob = Z_prob.reshape(xx1.shape)\ncontour = plt.contourf(xx1, xx2, Z_prob, levels=20, cmap='RdYlBu')\nplt.colorbar(contour)\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.title('Probability Surface')\n\n# ROC curve\nfrom sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(y_logistic, y_prob_logistic)\nroc_auc = auc(fpr, tpr)\n\nplt.subplot(1, 3, 3)\nplt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\nplt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Applications\n\n### Real Estate Price Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def real_estate_example():\n    \"\"\"Simulate real estate price prediction\"\"\"\n    np.random.seed(42)\n    n = 500\n    \n    # Generate features\n    square_feet = np.random.normal(2000, 500, n)\n    bedrooms = np.random.poisson(3, n)\n    bathrooms = np.random.poisson(2, n)\n    age = np.random.exponential(10, n)\n    distance_to_city = np.random.exponential(5, n)\n    \n    # Generate price with realistic relationships\n    base_price = 200000\n    price = (base_price + \n             100 * square_feet + \n             15000 * bedrooms + \n             25000 * bathrooms - \n             2000 * age - \n             5000 * distance_to_city + \n             np.random.normal(0, 20000, n))\n    \n    df_real_estate = pd.DataFrame({\n        'square_feet': square_feet,\n        'bedrooms': bedrooms,\n        'bathrooms': bathrooms,\n        'age': age,\n        'distance_to_city': distance_to_city,\n        'price': price\n    })\n    \n    return df_real_estate\n\ndf_real_estate = real_estate_example()\n\n# Fit model\nX_real_estate = df_real_estate.drop('price', axis=1)\ny_real_estate = df_real_estate['price']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X_real_estate, y_real_estate, \n                                                    test_size=0.3, random_state=42)\n\n# Fit multiple models\nmodels_real_estate = {\n    'Linear': LinearRegression(),\n    'Ridge': Ridge(alpha=1.0),\n    'Lasso': Lasso(alpha=0.1)\n}\n\nresults_real_estate = {}\n\nfor name, model in models_real_estate.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    results_real_estate[name] = {\n        'r2': r2_score(y_test, y_pred),\n        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n        'mae': np.mean(np.abs(y_test - y_pred))\n    }\n\nprint(\"Real Estate Price Prediction Results\")\nfor name, result in results_real_estate.items():\n    print(f\"{name}: R² = {result['r2']:.3f}, RMSE = ${result['rmse']:.0f}, MAE = ${result['mae']:.0f}\")\n\n# Feature importance\nmodel_linear = models_real_estate['Linear']\nfeature_importance = pd.DataFrame({\n    'feature': X_real_estate.columns,\n    'coefficient': model_linear.coef_\n})\nfeature_importance = feature_importance.sort_values('coefficient', key=abs, ascending=False)\n\nprint(f\"\\nFeature Importance:\")\nfor _, row in feature_importance.iterrows():\n    print(f\"{row['feature']}: {row['coefficient']:.2f}\")\n\n# Visualize results\nplt.figure(figsize=(15, 5))\n\n# Actual vs Predicted\nplt.subplot(1, 3, 1)\ny_pred_linear = models_real_estate['Linear'].predict(X_test)\nplt.scatter(y_test, y_pred_linear, alpha=0.7, color='skyblue')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.title('Actual vs Predicted Prices')\n\n# Feature importance\nplt.subplot(1, 3, 2)\nplt.barh(feature_importance['feature'], feature_importance['coefficient'], alpha=0.7)\nplt.xlabel('Coefficient')\nplt.title('Feature Importance')\n\n# Residuals\nplt.subplot(1, 3, 3)\nresiduals_real_estate = y_test - y_pred_linear\nplt.scatter(y_pred_linear, residuals_real_estate, alpha=0.7, color='lightgreen')\nplt.axhline(0, color='red', linestyle='--', alpha=0.7)\nplt.xlabel('Predicted Price')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Problems\n\n1. **Model Comparison**: Create a function that compares multiple regression models and provides comprehensive diagnostics.\n\n2. **Feature Engineering**: Implement automated feature engineering techniques (polynomial features, interactions, etc.).\n\n3. **Cross-Validation**: Build a robust cross-validation framework for regression models.\n\n4. **Model Interpretation**: Create functions to interpret regression coefficients and their significance.\n\n## Further Reading\n\n- \"Applied Linear Regression Models\" by Kutner, Nachtsheim, and Neter\n- \"Introduction to Linear Regression Analysis\" by Montgomery, Peck, and Vining\n- \"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman\n- \"Regression Analysis by Example\" by Chatterjee and Hadi\n\n## Key Takeaways\n\n- **Linear regression** models linear relationships between variables\n- **Multiple regression** extends to multiple predictors\n- **Model diagnostics** are crucial for validating assumptions\n- **Variable selection** helps build parsimonious models\n- **Regularization** prevents overfitting and improves generalization\n- **Polynomial regression** captures non-linear relationships\n- **Logistic regression** handles binary classification problems\n- **Cross-validation** provides reliable model performance estimates\n\nIn the next chapter, we'll explore time series analysis, including trend analysis, seasonality, and forecasting techniques."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}