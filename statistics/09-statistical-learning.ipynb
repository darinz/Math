{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistical Learning\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Pandas](https://img.shields.io/badge/Pandas-1.3+-blue.svg)](https://pandas.pydata.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.4+-orange.svg)](https://matplotlib.org/)\n[![Seaborn](https://img.shields.io/badge/Seaborn-0.11+-blue.svg)](https://seaborn.pydata.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-green.svg)](https://scipy.org/)\n[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)\n[![XGBoost](https://img.shields.io/badge/XGBoost-1.5+-green.svg)](https://xgboost.readthedocs.io/)\n\n# Chapter 9: Statistical Learning\n\n## Overview\n\nStatistical learning encompasses the methods and techniques used to build predictive models from data. This chapter covers cross-validation, model selection, regularization, ensemble methods, and model evaluation - all essential skills for machine learning and data science.\n\n## Learning Objectives\n\nBy the end of this chapter, you will be able to:\n- Implement various cross-validation techniques\n- Understand the bias-variance tradeoff\n- Apply regularization methods to prevent overfitting\n- Use ensemble methods for improved predictions\n- Evaluate model performance using appropriate metrics\n- Select optimal models using information criteria\n\n## Prerequisites\n\n- Understanding of regression and classification concepts\n- Familiarity with scikit-learn\n- Basic knowledge of probability and statistics\n\n## Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import (\n    train_test_split, cross_val_score, KFold, \n    LeaveOneOut, StratifiedKFold, GridSearchCV\n)\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import (\n    mean_squared_error, accuracy_score, classification_report,\n    confusion_matrix, roc_curve, auc, precision_recall_curve\n)\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for better plots\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 1. Cross-Validation\n\nCross-validation is a technique for assessing how well a model will generalize to new, unseen data. It helps prevent overfitting and provides a more reliable estimate of model performance.\n\n### Mathematical Foundation\n\n**Cross-Validation Framework:**\nThe goal is to estimate the generalization error:\n$$E_{gen} = E_{(X,Y)}[L(Y, f(X))]$$\nwhere $L$ is the loss function and $f$ is our learned model.\n\n**K-Fold Cross-Validation:**\n1. Partition data into K folds: $D = D_1 \\cup D_2 \\cup ... \\cup D_K$\n2. For each fold k, train on $D_{-k} = D \\setminus D_k$ and validate on $D_k$\n3. Estimate generalization error:\n   $$\\hat{E}_{CV} = \\frac{1}{K}\\sum_{k=1}^{K} \\frac{1}{|D_k|}\\sum_{(x_i,y_i) \\in D_k} L(y_i, f_{-k}(x_i))$$\n   where $f_{-k}$ is the model trained on $D_{-k}$\n\n**Properties:**\n- **Unbiased**: $E[\\hat{E}_{CV}] = E_{gen}$ under certain conditions\n- **Variance**: $Var(\\hat{E}_{CV}) \\approx \\frac{1}{K}Var(L(Y,f(X))) + \\frac{K-1}{K}Cov(L(Y,f_{-k}(X)), L(Y,f_{-k'}(X)))$\n\n### 1.1 Holdout Validation\n\nThe simplest form of validation splits data into training and test sets.\n\n**Mathematical Implementation:**\n- Training set: $D_{train} = \\{(x_i, y_i)\\}_{i=1}^{n_{train}}$\n- Test set: $D_{test} = \\{(x_i, y_i)\\}_{i=n_{train}+1}^{n}$\n- Model training: $f_{train} = \\arg\\min_f \\sum_{i=1}^{n_{train}} L(y_i, f(x_i))$\n- Test error: $\\hat{E}_{test} = \\frac{1}{n_{test}}\\sum_{i=n_{train}+1}^{n} L(y_i, f_{train}(x_i))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample data\nnp.random.seed(42)\nX = np.random.randn(1000, 10)\ny = 2*X[:, 0] + 1.5*X[:, 1] - 0.5*X[:, 2] + np.random.normal(0, 0.1, 1000)\n\n# Holdout validation\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Evaluate\ntrain_score = model.score(X_train, y_train)\ntest_score = model.score(X_test, y_test)\n\nprint(f\"Training R²: {train_score:.4f}\")\nprint(f\"Test R²: {test_score:.4f}\")\n\n# Calculate MSE for comparison\ntrain_mse = mean_squared_error(y_train, model.predict(X_train))\ntest_mse = mean_squared_error(y_test, model.predict(X_test))\nprint(f\"Training MSE: {train_mse:.4f}\")\nprint(f\"Test MSE: {test_mse:.4f}\")\n\n# Overfitting check\noverfitting_ratio = train_mse / test_mse\nprint(f\"Overfitting ratio (train/test MSE): {overfitting_ratio:.3f}\")\nif overfitting_ratio < 0.8:\n    print(\"Warning: Possible overfitting detected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 K-Fold Cross-Validation\n\nK-fold cross-validation divides data into K folds and trains K models, each using K-1 folds for training and 1 fold for validation.\n\n**Mathematical Implementation:**\nFor K-fold CV with K=5:\n1. Partition: $D = D_1 \\cup D_2 \\cup D_3 \\cup D_4 \\cup D_5$\n2. Train models: $f_{-k}$ on $D \\setminus D_k$ for k=1,2,3,4,5\n3. CV error: $\\hat{E}_{CV} = \\frac{1}{5}\\sum_{k=1}^{5} \\frac{1}{|D_k|}\\sum_{(x_i,y_i) \\in D_k} L(y_i, f_{-k}(x_i))$\n\n**Standard Error of CV Estimate:**\n$$SE(\\hat{E}_{CV}) = \\sqrt{\\frac{1}{K(K-1)}\\sum_{k=1}^{K}(E_k - \\bar{E})^2}$$\nwhere $E_k$ is the error on fold k and $\\bar{E}$ is the mean CV error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-fold cross-validation\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = cross_val_score(LinearRegression(), X, y, cv=kfold, scoring='r2')\n\nprint(f\"Cross-validation scores: {cv_scores}\")\nprint(f\"Mean CV score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n\n# Manual K-fold implementation for understanding\ndef manual_kfold_cv(X, y, k=5, model_class=LinearRegression):\n    \"\"\"\n    Manual implementation of K-fold cross-validation\n    \n    Mathematical steps:\n    1. Partition data into k folds\n    2. For each fold k:\n       - Train model on all folds except k\n       - Predict on fold k\n       - Calculate error\n    3. Average errors across all folds\n    \n    Parameters:\n    X: array-like, features\n    y: array-like, target\n    k: int, number of folds\n    model_class: class, model to use\n    \n    Returns:\n    tuple: (mean_score, std_score, fold_scores)\n    \"\"\"\n    n_samples = len(X)\n    fold_size = n_samples // k\n    fold_scores = []\n    \n    for i in range(k):\n        # Define test indices for this fold\n        test_start = i * fold_size\n        test_end = test_start + fold_size if i < k-1 else n_samples\n        test_indices = list(range(test_start, test_end))\n        train_indices = list(set(range(n_samples)) - set(test_indices))\n        \n        # Split data\n        X_train_fold = X[train_indices]\n        y_train_fold = y[train_indices]\n        X_test_fold = X[test_indices]\n        y_test_fold = y[test_indices]\n        \n        # Train and evaluate\n        model = model_class()\n        model.fit(X_train_fold, y_train_fold)\n        score = model.score(X_test_fold, y_test_fold)\n        fold_scores.append(score)\n    \n    return np.mean(fold_scores), np.std(fold_scores), fold_scores\n\nmanual_mean, manual_std, manual_scores = manual_kfold_cv(X, y, k=5)\nprint(f\"\\nManual K-fold CV:\")\nprint(f\"Mean score: {manual_mean:.4f}\")\nprint(f\"Std score: {manual_std:.4f}\")\nprint(f\"Individual fold scores: {[f'{s:.4f}' for s in manual_scores]}\")\n\n# Visualize CV scores\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.plot(range(1, 6), cv_scores, 'bo-', label='sklearn')\nplt.plot(range(1, 6), manual_scores, 'ro-', label='manual')\nplt.xlabel('Fold')\nplt.ylabel('R² Score')\nplt.title('K-Fold Cross-Validation Scores')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 3, 2)\nplt.boxplot([cv_scores, manual_scores], labels=['sklearn', 'manual'])\nplt.ylabel('R² Score')\nplt.title('Distribution of CV Scores')\nplt.grid(True)\n\nplt.subplot(1, 3, 3)\n# Show fold assignments\nfold_assignments = np.zeros(len(X))\nfor i, (train_idx, test_idx) in enumerate(kfold.split(X)):\n    fold_assignments[test_idx] = i + 1\n\nplt.scatter(range(len(X)), fold_assignments, alpha=0.6)\nplt.xlabel('Sample Index')\nplt.ylabel('Fold Assignment')\nplt.title('K-Fold Data Partitioning')\nplt.yticks(range(1, 6))\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Leave-One-Out Cross-Validation\n\nLOOCV uses n-1 samples for training and 1 sample for validation, repeated n times.\n\n**Mathematical Implementation:**\n$$\\hat{E}_{LOOCV} = \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, f_{-i}(x_i))$$\nwhere $f_{-i}$ is trained on all data except observation i.\n\n**Properties:**\n- **Unbiased**: $E[\\hat{E}_{LOOCV}] = E_{gen}$\n- **High Variance**: Due to high correlation between predictions\n- **Computational Cost**: O(n) model fits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leave-One-Out CV (computationally expensive for large datasets)\nloocv = LeaveOneOut()\nloocv_scores = cross_val_score(LinearRegression(), X[:100], y[:100], cv=loocv, scoring='r2')\n\nprint(f\"LOOCV mean score: {loocv_scores.mean():.4f}\")\nprint(f\"LOOCV std score: {loocv_scores.std():.4f}\")\n\n# Compare with K-fold\nkfold_scores = cross_val_score(LinearRegression(), X[:100], y[:100], cv=5, scoring='r2')\nprint(f\"5-fold CV mean score: {kfold_scores.mean():.4f}\")\nprint(f\"5-fold CV std score: {kfold_scores.std():.4f}\")\n\n# Computational comparison\nimport time\n\n# Time K-fold\nstart_time = time.time()\nkfold_scores = cross_val_score(LinearRegression(), X[:100], y[:100], cv=5, scoring='r2')\nkfold_time = time.time() - start_time\n\n# Time LOOCV\nstart_time = time.time()\nloocv_scores = cross_val_score(LinearRegression(), X[:100], y[:100], cv=LeaveOneOut(), scoring='r2')\nloocv_time = time.time() - start_time\n\nprint(f\"\\nComputational Comparison:\")\nprint(f\"K-fold time: {kfold_time:.3f} seconds\")\nprint(f\"LOOCV time: {loocv_time:.3f} seconds\")\nprint(f\"Speed ratio: {loocv_time/kfold_time:.1f}x slower\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Stratified Cross-Validation\n\nFor classification problems, stratified CV maintains the proportion of samples for each class.\n\n**Mathematical Implementation:**\nFor binary classification with classes 0 and 1:\n- Original proportions: $p_0 = \\frac{n_0}{n}$, $p_1 = \\frac{n_1}{n}$\n- In each fold k: maintain $p_0^{(k)} \\approx p_0$, $p_1^{(k)} \\approx p_1$\n\n**Benefits:**\n1. **Balanced Folds**: Prevents folds with only one class\n2. **Better Estimates**: More reliable performance estimates\n3. **Reduced Variance**: Especially important for imbalanced datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate classification data\nnp.random.seed(42)\nX_clf = np.random.randn(1000, 5)\ny_clf = (X_clf[:, 0] + X_clf[:, 1] > 0).astype(int)\n\n# Check class distribution\nprint(f\"Original class distribution:\")\nprint(f\"Class 0: {np.sum(y_clf == 0)} ({np.mean(y_clf == 0)*100:.1f}%)\")\nprint(f\"Class 1: {np.sum(y_clf == 1)} ({np.mean(y_clf == 1)*100:.1f}%)\")\n\n# Stratified K-fold for classification\nstratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nclf_scores = cross_val_score(\n    LogisticRegression(), X_clf, y_clf, \n    cv=stratified_kfold, scoring='accuracy'\n)\n\nprint(f\"\\nStratified CV accuracy: {clf_scores.mean():.4f} (+/- {clf_scores.std() * 2:.4f})\")\n\n# Compare with regular K-fold\nregular_kfold = KFold(n_splits=5, shuffle=True, random_state=42)\nregular_scores = cross_val_score(\n    LogisticRegression(), X_clf, y_clf, \n    cv=regular_kfold, scoring='accuracy'\n)\n\nprint(f\"Regular CV accuracy: {regular_scores.mean():.4f} (+/- {regular_scores.std() * 2:.4f})\")\n\n# Check class distribution in folds\nprint(f\"\\nClass distribution in first fold (stratified):\")\nfor i, (train_idx, test_idx) in enumerate(stratified_kfold.split(X_clf, y_clf)):\n    if i == 0:  # Show only first fold\n        test_y = y_clf[test_idx]\n        print(f\"Test fold - Class 0: {np.sum(test_y == 0)} ({np.mean(test_y == 0)*100:.1f}%)\")\n        print(f\"Test fold - Class 1: {np.sum(test_y == 1)} ({np.mean(test_y == 1)*100:.1f}%)\")\n        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 2. Bias-Variance Tradeoff\n\nThe bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between model complexity and generalization error.\n\n### Mathematical Foundation\n\n**Decomposition of Expected Prediction Error:**\nFor a model $f(x)$ and true function $f^*(x)$:\n$$E[(Y - f(X))^2] = \\underbrace{(E[f(X)] - f^*(X))^2}_{\\text{Bias}^2} + \\underbrace{E[(f(X) - E[f(X)])^2]}_{\\text{Variance}} + \\underbrace{\\sigma^2}_{\\text{Irreducible Error}}$$\n\n**Interpretation:**\n- **Bias**: How far the average prediction is from the true value (underfitting)\n- **Variance**: How much predictions vary around their average (overfitting)\n- **Irreducible Error**: Noise in the data that cannot be reduced\n\n**Tradeoff:**\n- **Simple models**: High bias, low variance\n- **Complex models**: Low bias, high variance\n- **Optimal complexity**: Minimizes total error\n\n### 2.1 Understanding Bias and Variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_polynomial_data(n_samples=100, noise=0.1, degree=1):\n    \"\"\"\n    Generate polynomial data with noise\n    \n    Mathematical implementation:\n    y = f*(x) + ε where f*(x) = 2x + 1 and ε ~ N(0, noise²)\n    \n    Parameters:\n    n_samples: int, number of samples\n    noise: float, standard deviation of noise\n    degree: int, true polynomial degree (always 1 in this case)\n    \n    Returns:\n    tuple: (X, y_noisy, y_true)\n    \"\"\"\n    np.random.seed(42)\n    X = np.linspace(-3, 3, n_samples)\n    y_true = 2 * X + 1  # True linear relationship\n    y_noisy = y_true + np.random.normal(0, noise, n_samples)\n    return X.reshape(-1, 1), y_noisy, y_true\n\ndef fit_polynomial(X, y, degree):\n    \"\"\"\n    Fit polynomial of given degree\n    \n    Mathematical implementation:\n    f(x) = β₀ + β₁x + β₂x² + ... + βₖxᵏ\n    \n    Parameters:\n    X: array-like, features\n    y: array-like, target\n    degree: int, polynomial degree\n    \n    Returns:\n    Pipeline: fitted polynomial model\n    \"\"\"\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.pipeline import Pipeline\n    from sklearn.linear_model import LinearRegression\n    \n    poly = PolynomialFeatures(degree=degree, include_bias=False)\n    model = Pipeline([\n        ('poly', poly),\n        ('linear', LinearRegression())\n    ])\n    model.fit(X, y)\n    return model\n\ndef estimate_bias_variance(X, y, model_class, n_bootstrap=100):\n    \"\"\"\n    Estimate bias and variance using bootstrap sampling\n    \n    Mathematical implementation:\n    1. Generate B bootstrap samples\n    2. Train model on each bootstrap sample: f^b(x)\n    3. Calculate predictions: f^b(xᵢ) for each sample\n    4. Bias² = (E[f^b(x)] - f*(x))²\n    5. Variance = E[(f^b(x) - E[f^b(x)])²]\n    \n    Parameters:\n    X: array-like, features\n    y: array-like, target\n    model_class: class, model to use\n    n_bootstrap: int, number of bootstrap samples\n    \n    Returns:\n    tuple: (bias_squared, variance, total_error)\n    \"\"\"\n    n_samples = len(X)\n    predictions = np.zeros((n_bootstrap, n_samples))\n    \n    # Generate bootstrap samples and predictions\n    for i in range(n_bootstrap):\n        # Bootstrap sample\n        indices = np.random.choice(n_samples, n_samples, replace=True)\n        X_boot = X[indices]\n        y_boot = y[indices]\n        \n        # Train model\n        model = model_class()\n        model.fit(X_boot, y_boot)\n        \n        # Predict on original X\n        predictions[i, :] = model.predict(X)\n    \n    # Calculate bias and variance\n    mean_predictions = np.mean(predictions, axis=0)\n    \n    # Bias² = (E[f(x)] - f*(x))²\n    # We approximate f*(x) with the true function values\n    X_flat = X.flatten()\n    true_function = 2 * X_flat + 1  # True f*(x)\n    bias_squared = np.mean((mean_predictions - true_function)**2)\n    \n    # Variance = E[(f(x) - E[f(x)])²]\n    variance = np.mean(np.var(predictions, axis=0))\n    \n    # Total error = Bias² + Variance + Irreducible Error\n    irreducible_error = 0.1**2  # noise variance\n    total_error = bias_squared + variance + irreducible_error\n    \n    return bias_squared, variance, total_error\n\n# Generate data\nX, y, y_true = generate_polynomial_data(n_samples=50, noise=0.5)\n\n# Fit models with different complexities\ndegrees = [1, 3, 10, 15]\nmodels = []\npredictions = []\nbias_var_results = []\n\nfor degree in degrees:\n    model = fit_polynomial(X, y, degree)\n    models.append(model)\n    pred = model.predict(X)\n    predictions.append(pred)\n    \n    # Estimate bias and variance\n    bias_sq, var, total_err = estimate_bias_variance(X, y, \n                                                   lambda: fit_polynomial(X, y, degree))\n    bias_var_results.append({\n        'degree': degree,\n        'bias_squared': bias_sq,\n        'variance': var,\n        'total_error': total_err\n    })\n\n# Visualize bias-variance tradeoff\nplt.figure(figsize=(15, 5))\n\n# Model fits\nplt.subplot(1, 3, 1)\nX_plot = np.linspace(-3, 3, 100).reshape(-1, 1)\ncolors = ['blue', 'green', 'orange', 'red']\n\nfor i, (degree, pred, color) in enumerate(zip(degrees, predictions, colors)):\n    plt.plot(X_plot.flatten(), models[i].predict(X_plot), \n             color=color, linewidth=2, label=f'Degree {degree}')\n\nplt.scatter(X.flatten(), y, alpha=0.6, color='black', label='Data')\nplt.plot(X.flatten(), y_true, 'k--', linewidth=2, label='True function')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Polynomial Fits')\nplt.legend()\nplt.grid(True)\n\n# Bias-Variance decomposition\nplt.subplot(1, 3, 2)\ndegrees_plot = [r['degree'] for r in bias_var_results]\nbias_sq_plot = [r['bias_squared'] for r in bias_var_results]\nvariance_plot = [r['variance'] for r in bias_var_results]\ntotal_error_plot = [r['total_error'] for r in bias_var_results]\n\nplt.plot(degrees_plot, bias_sq_plot, 'bo-', linewidth=2, label='Bias²')\nplt.plot(degrees_plot, variance_plot, 'ro-', linewidth=2, label='Variance')\nplt.plot(degrees_plot, total_error_plot, 'go-', linewidth=2, label='Total Error')\nplt.xlabel('Polynomial Degree')\nplt.ylabel('Error')\nplt.title('Bias-Variance Tradeoff')\nplt.legend()\nplt.grid(True)\n\n# Training vs Test error\nplt.subplot(1, 3, 3)\ntrain_errors = []\ntest_errors = []\n\nfor degree in degrees:\n    model = fit_polynomial(X, y, degree)\n    \n    # Training error\n    train_pred = model.predict(X)\n    train_error = mean_squared_error(y, train_pred)\n    train_errors.append(train_error)\n    \n    # Test error (simulate with new data)\n    X_test = np.random.uniform(-3, 3, 100).reshape(-1, 1)\n    y_test_true = 2 * X_test.flatten() + 1\n    y_test_noisy = y_test_true + np.random.normal(0, 0.5, 100)\n    test_pred = model.predict(X_test)\n    test_error = mean_squared_error(y_test_noisy, test_pred)\n    test_errors.append(test_error)\n\nplt.plot(degrees, train_errors, 'bo-', linewidth=2, label='Training Error')\nplt.plot(degrees, test_errors, 'ro-', linewidth=2, label='Test Error')\nplt.xlabel('Polynomial Degree')\nplt.ylabel('Mean Squared Error')\nplt.title('Training vs Test Error')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Print numerical results\nprint(\"Bias-Variance Analysis:\")\nfor result in bias_var_results:\n    print(f\"Degree {result['degree']}:\")\n    print(f\"  Bias²: {result['bias_squared']:.4f}\")\n    print(f\"  Variance: {result['variance']:.4f}\")\n    print(f\"  Total Error: {result['total_error']:.4f}\")\n    print(f\"  Bias/Variance Ratio: {result['bias_squared']/result['variance']:.2f}\")\n    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 3. Model Selection\n\nModel selection involves choosing the best model from a set of candidates based on performance metrics and complexity.\n\n### 3.1 Information Criteria\n\nInformation criteria balance model fit with complexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\nimport statsmodels.api as sm\n\ndef calculate_aic_bic(X, y, model):\n    \"\"\"Calculate AIC and BIC for a model\"\"\"\n    model.fit(X, y)\n    y_pred = model.predict(X)\n    n = len(y)\n    k = X.shape[1] + 1  # +1 for intercept\n    \n    # Calculate RSS\n    rss = np.sum((y - y_pred) ** 2)\n    \n    # AIC = n * log(RSS/n) + 2k\n    aic = n * np.log(rss/n) + 2*k\n    \n    # BIC = n * log(RSS/n) + k*log(n)\n    bic = n * np.log(rss/n) + k*np.log(n)\n    \n    return aic, bic\n\n# Compare different polynomial models\naic_scores = []\nbic_scores = []\nmse_scores = []\n\nfor degree in range(1, 11):\n    model = fit_polynomial(X, y, degree)\n    aic, bic = calculate_aic_bic(X, y, model)\n    mse = mean_squared_error(y, model.predict(X))\n    \n    aic_scores.append(aic)\n    bic_scores.append(bic)\n    mse_scores.append(mse)\n\n# Plot information criteria\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.plot(range(1, 11), mse_scores, 'bo-')\nplt.xlabel('Polynomial Degree')\nplt.ylabel('Mean Squared Error')\nplt.title('Training MSE')\nplt.grid(True)\n\nplt.subplot(1, 3, 2)\nplt.plot(range(1, 11), aic_scores, 'ro-')\nplt.xlabel('Polynomial Degree')\nplt.ylabel('AIC')\nplt.title('Akaike Information Criterion')\nplt.grid(True)\n\nplt.subplot(1, 3, 3)\nplt.plot(range(1, 11), bic_scores, 'go-')\nplt.xlabel('Polynomial Degree')\nplt.ylabel('BIC')\nplt.title('Bayesian Information Criterion')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Best model by MSE: Degree {np.argmin(mse_scores) + 1}\")\nprint(f\"Best model by AIC: Degree {np.argmin(aic_scores) + 1}\")\nprint(f\"Best model by BIC: Degree {np.argmin(bic_scores) + 1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Grid Search with Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\n\n# Define parameter grid\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': ['scale', 'auto', 0.1, 0.01],\n    'kernel': ['rbf', 'linear']\n}\n\n# Grid search with cross-validation\ngrid_search = GridSearchCV(\n    SVR(), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1\n)\ngrid_search.fit(X, y)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best CV score: {-grid_search.best_score_:.4f}\")\n\n# Visualize grid search results\nresults = grid_search.cv_results_\nC_values = [0.1, 1, 10, 100]\n\nplt.figure(figsize=(12, 5))\n\n# Plot for RBF kernel\nrbf_scores = []\nfor C in C_values:\n    mask = (results['param_C'] == C) & (results['param_kernel'] == 'rbf') & (results['param_gamma'] == 'scale')\n    if np.any(mask):\n        rbf_scores.append(-results['mean_test_score'][mask][0])\n    else:\n        rbf_scores.append(np.nan)\n\nplt.subplot(1, 2, 1)\nplt.plot(C_values, rbf_scores, 'bo-')\nplt.xscale('log')\nplt.xlabel('C')\nplt.ylabel('MSE')\nplt.title('SVR RBF Kernel Performance')\nplt.grid(True)\n\n# Plot for linear kernel\nlinear_scores = []\nfor C in C_values:\n    mask = (results['param_C'] == C) & (results['param_kernel'] == 'linear')\n    if np.any(mask):\n        linear_scores.append(-results['mean_test_score'][mask][0])\n    else:\n        linear_scores.append(np.nan)\n\nplt.subplot(1, 2, 2)\nplt.plot(C_values, linear_scores, 'ro-')\nplt.xscale('log')\nplt.xlabel('C')\nplt.ylabel('MSE')\nplt.title('SVR Linear Kernel Performance')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 4. Regularization\n\nRegularization techniques help prevent overfitting by adding constraints to the model parameters.\n\n### 4.1 Ridge Regression (L2 Regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate data with multicollinearity\nnp.random.seed(42)\nX_ridge = np.random.randn(100, 10)\n# Create multicollinearity\nX_ridge[:, 2] = X_ridge[:, 0] + 0.1 * np.random.randn(100)\nX_ridge[:, 3] = X_ridge[:, 1] + 0.1 * np.random.randn(100)\n\ny_ridge = 2*X_ridge[:, 0] + 1.5*X_ridge[:, 1] + np.random.normal(0, 0.1, 100)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_ridge, y_ridge, test_size=0.3, random_state=42\n)\n\n# Standardize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Compare Linear Regression vs Ridge\nalpha_values = [0, 0.1, 1, 10, 100]\ntrain_scores = []\ntest_scores = []\ncoefficients = []\n\nfor alpha in alpha_values:\n    if alpha == 0:\n        model = LinearRegression()\n    else:\n        model = Ridge(alpha=alpha)\n    \n    model.fit(X_train_scaled, y_train)\n    train_scores.append(model.score(X_train_scaled, y_train))\n    test_scores.append(model.score(X_test_scaled, y_test))\n    coefficients.append(model.coef_)\n\n# Visualize results\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.plot(alpha_values, train_scores, 'bo-', label='Training')\nplt.plot(alpha_values, test_scores, 'ro-', label='Test')\nplt.xscale('log')\nplt.xlabel('Alpha (Regularization Strength)')\nplt.ylabel('R² Score')\nplt.title('Ridge Regression Performance')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 3, 2)\ncoefficients = np.array(coefficients)\nfor i in range(coefficients.shape[1]):\n    plt.plot(alpha_values, coefficients[:, i], 'o-', label=f'Feature {i+1}')\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Coefficient Value')\nplt.title('Coefficient Shrinkage')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\n\nplt.subplot(1, 3, 3)\n# Show coefficient magnitudes\ncoef_magnitudes = np.abs(coefficients)\nplt.imshow(coef_magnitudes.T, aspect='auto', cmap='viridis')\nplt.colorbar(label='|Coefficient|')\nplt.xlabel('Alpha Index')\nplt.ylabel('Feature')\nplt.title('Coefficient Magnitudes Heatmap')\nplt.xticks(range(len(alpha_values)), [f'{a}' for a in alpha_values])\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Lasso Regression (L1 Regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lasso regression\nlasso_train_scores = []\nlasso_test_scores = []\nlasso_coefficients = []\n\nfor alpha in alpha_values:\n    if alpha == 0:\n        model = LinearRegression()\n    else:\n        model = Lasso(alpha=alpha)\n    \n    model.fit(X_train_scaled, y_train)\n    lasso_train_scores.append(model.score(X_train_scaled, y_train))\n    lasso_test_scores.append(model.score(X_test_scaled, y_test))\n    lasso_coefficients.append(model.coef_)\n\n# Visualize Lasso results\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.plot(alpha_values, lasso_train_scores, 'bo-', label='Training')\nplt.plot(alpha_values, lasso_test_scores, 'ro-', label='Test')\nplt.xscale('log')\nplt.xlabel('Alpha (Regularization Strength)')\nplt.ylabel('R² Score')\nplt.title('Lasso Regression Performance')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 3, 2)\nlasso_coefficients = np.array(lasso_coefficients)\nfor i in range(lasso_coefficients.shape[1]):\n    plt.plot(alpha_values, lasso_coefficients[:, i], 'o-', label=f'Feature {i+1}')\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Coefficient Value')\nplt.title('Lasso Coefficient Path')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\n\nplt.subplot(1, 3, 3)\n# Count non-zero coefficients\nnon_zero_counts = np.sum(lasso_coefficients != 0, axis=1)\nplt.plot(alpha_values, non_zero_counts, 'go-')\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Number of Non-zero Coefficients')\nplt.title('Feature Selection by Lasso')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Compare Ridge vs Lasso\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(alpha_values, test_scores, 'bo-', label='Ridge')\nplt.plot(alpha_values, lasso_test_scores, 'ro-', label='Lasso')\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Test R² Score')\nplt.title('Ridge vs Lasso Performance')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nridge_coef_var = np.var(coefficients, axis=1)\nlasso_coef_var = np.var(lasso_coefficients, axis=1)\nplt.plot(alpha_values, ridge_coef_var, 'bo-', label='Ridge')\nplt.plot(alpha_values, lasso_coef_var, 'ro-', label='Lasso')\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Coefficient Variance')\nplt.title('Coefficient Stability')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 5. Ensemble Methods\n\nEnsemble methods combine multiple models to improve prediction accuracy and robustness.\n\n### 5.1 Bagging (Bootstrap Aggregating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Generate data\nnp.random.seed(42)\nX_ensemble = np.random.randn(200, 5)\ny_ensemble = 2*X_ensemble[:, 0] + 1.5*X_ensemble[:, 1] + np.random.normal(0, 0.5, 200)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_ensemble, y_ensemble, test_size=0.3, random_state=42\n)\n\n# Compare single tree vs bagging\nsingle_tree = DecisionTreeRegressor(random_state=42)\nbagging = BaggingRegressor(\n    DecisionTreeRegressor(random_state=42),\n    n_estimators=100,\n    random_state=42\n)\n\n# Train models\nsingle_tree.fit(X_train, y_train)\nbagging.fit(X_train, y_train)\n\n# Evaluate\nsingle_tree_score = single_tree.score(X_test, y_test)\nbagging_score = bagging.score(X_test, y_test)\n\nprint(f\"Single Tree R²: {single_tree_score:.4f}\")\nprint(f\"Bagging R²: {bagging_score:.4f}\")\n\n# Visualize predictions\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, single_tree.predict(X_test), alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Single Tree Predictions')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.scatter(y_test, bagging.predict(X_test), alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Bagging Predictions')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n\n# Random Forest\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\nrf_score = rf.score(X_test, y_test)\n\nprint(f\"Random Forest R²: {rf_score:.4f}\")\n\n# Feature importance\nfeature_importance = rf.feature_importances_\nfeature_names = [f'Feature {i+1}' for i in range(X_train.shape[1])]\n\nplt.figure(figsize=(10, 6))\nplt.bar(feature_names, feature_importance)\nplt.xlabel('Features')\nplt.ylabel('Importance')\nplt.title('Random Forest Feature Importance')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Compare ensemble methods\nensemble_scores = {\n    'Single Tree': single_tree_score,\n    'Bagging': bagging_score,\n    'Random Forest': rf_score\n}\n\nplt.figure(figsize=(8, 6))\nmethods = list(ensemble_scores.keys())\nscores = list(ensemble_scores.values())\ncolors = ['red', 'blue', 'green']\n\nplt.bar(methods, scores, color=colors)\nplt.ylabel('R² Score')\nplt.title('Comparison of Ensemble Methods')\nplt.ylim(0, 1)\nfor i, v in enumerate(scores):\n    plt.text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n\n# Gradient Boosting\ngb = GradientBoostingRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=3,\n    random_state=42\n)\ngb.fit(X_train, y_train)\ngb_score = gb.score(X_test, y_test)\n\nprint(f\"Gradient Boosting R²: {gb_score:.4f}\")\n\n# Learning curves\ntrain_scores = []\ntest_scores = []\n\nfor i in range(1, 101, 10):\n    gb_partial = GradientBoostingRegressor(\n        n_estimators=i,\n        learning_rate=0.1,\n        max_depth=3,\n        random_state=42\n    )\n    gb_partial.fit(X_train, y_train)\n    train_scores.append(gb_partial.score(X_train, y_train))\n    test_scores.append(gb_partial.score(X_test, y_test))\n\n# Plot learning curves\nplt.figure(figsize=(10, 6))\nn_estimators = range(1, 101, 10)\nplt.plot(n_estimators, train_scores, 'bo-', label='Training')\nplt.plot(n_estimators, test_scores, 'ro-', label='Test')\nplt.xlabel('Number of Estimators')\nplt.ylabel('R² Score')\nplt.title('Gradient Boosting Learning Curves')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Compare all ensemble methods\nall_scores = {\n    'Single Tree': single_tree_score,\n    'Bagging': bagging_score,\n    'Random Forest': rf_score,\n    'Gradient Boosting': gb_score\n}\n\nplt.figure(figsize=(10, 6))\nmethods = list(all_scores.keys())\nscores = list(all_scores.values())\ncolors = ['red', 'blue', 'green', 'orange']\n\nplt.bar(methods, scores, color=colors)\nplt.ylabel('R² Score')\nplt.title('Comparison of All Ensemble Methods')\nplt.ylim(0, 1)\nfor i, v in enumerate(scores):\n    plt.text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 6. Model Evaluation\n\nProper model evaluation is crucial for understanding model performance and making informed decisions.\n\n### 6.1 Classification Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate classification data\nnp.random.seed(42)\nX_clf = np.random.randn(1000, 10)\ny_clf = (X_clf[:, 0] + X_clf[:, 1] > 0).astype(int)\n\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n    X_clf, y_clf, test_size=0.3, random_state=42\n)\n\n# Train multiple classifiers\nclassifiers = {\n    'Logistic Regression': LogisticRegression(random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n}\n\nresults = {}\n\nfor name, clf in classifiers.items():\n    clf.fit(X_train_clf, y_train_clf)\n    y_pred = clf.predict(X_test_clf)\n    y_pred_proba = clf.predict_proba(X_test_clf)[:, 1]\n    \n    results[name] = {\n        'accuracy': accuracy_score(y_test_clf, y_pred),\n        'predictions': y_pred,\n        'probabilities': y_pred_proba\n    }\n\n# Compare accuracy\nplt.figure(figsize=(10, 6))\nnames = list(results.keys())\naccuracies = [results[name]['accuracy'] for name in names]\ncolors = ['red', 'blue', 'green']\n\nplt.bar(names, accuracies, color=colors)\nplt.ylabel('Accuracy')\nplt.title('Classifier Accuracy Comparison')\nplt.ylim(0, 1)\nfor i, v in enumerate(accuracies):\n    plt.text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\nplt.tight_layout()\nplt.show()\n\n# ROC Curves\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nfor name, result in results.items():\n    fpr, tpr, _ = roc_curve(y_test_clf, result['probabilities'])\n    auc_score = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves')\nplt.legend()\nplt.grid(True)\n\n# Precision-Recall Curves\nplt.subplot(1, 2, 2)\nfor name, result in results.items():\n    precision, recall, _ = precision_recall_curve(y_test_clf, result['probabilities'])\n    plt.plot(recall, precision, label=name)\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curves')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Confusion Matrix for best classifier\nbest_classifier = max(results.keys(), key=lambda x: results[x]['accuracy'])\ny_pred_best = results[best_classifier]['predictions']\n\ncm = confusion_matrix(y_test_clf, y_pred_best)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title(f'Confusion Matrix - {best_classifier}')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.tight_layout()\nplt.show()\n\n# Detailed classification report\nprint(f\"\\nClassification Report - {best_classifier}\")\nprint(classification_report(y_test_clf, y_pred_best))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Regression Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate regression data\nnp.random.seed(42)\nX_reg = np.random.randn(1000, 10)\ny_reg = 2*X_reg[:, 0] + 1.5*X_reg[:, 1] - 0.5*X_reg[:, 2] + np.random.normal(0, 0.5, 1000)\n\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n    X_reg, y_reg, test_size=0.3, random_state=42\n)\n\n# Train multiple regressors\nregressors = {\n    'Linear Regression': LinearRegression(),\n    'Ridge Regression': Ridge(alpha=1.0),\n    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n}\n\nreg_results = {}\n\nfor name, reg in regressors.items():\n    reg.fit(X_train_reg, y_train_reg)\n    y_pred = reg.predict(X_test_reg)\n    \n    reg_results[name] = {\n        'r2': reg.score(X_test_reg, y_test_reg),\n        'mse': mean_squared_error(y_test_reg, y_pred),\n        'rmse': np.sqrt(mean_squared_error(y_test_reg, y_pred)),\n        'mae': np.mean(np.abs(y_test_reg - y_pred)),\n        'predictions': y_pred\n    }\n\n# Compare metrics\nmetrics = ['r2', 'mse', 'rmse', 'mae']\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\nfor i, metric in enumerate(metrics):\n    row, col = i // 2, i % 2\n    names = list(reg_results.keys())\n    values = [reg_results[name][metric] for name in names]\n    \n    axes[row, col].bar(names, values)\n    axes[row, col].set_title(f'{metric.upper()} Comparison')\n    axes[row, col].set_ylabel(metric.upper())\n    axes[row, col].tick_params(axis='x', rotation=45)\n    \n    # Add value labels\n    for j, v in enumerate(values):\n        axes[row, col].text(j, v + max(values)*0.01, f'{v:.4f}', \n                           ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n# Residual plots\nplt.figure(figsize=(15, 10))\nfor i, (name, result) in enumerate(reg_results.items()):\n    plt.subplot(2, 2, i+1)\n    residuals = y_test_reg - result['predictions']\n    plt.scatter(result['predictions'], residuals, alpha=0.6)\n    plt.axhline(y=0, color='r', linestyle='--')\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Residuals')\n    plt.title(f'Residual Plot - {name}')\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 7. Practical Applications\n\n### 7.1 Model Selection for House Price Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sample data (simulated house prices)\nnp.random.seed(42)\nn_samples = 1000\n\n# Generate features\nsquare_feet = np.random.uniform(800, 4000, n_samples)\nbedrooms = np.random.randint(1, 6, n_samples)\nbathrooms = np.random.randint(1, 4, n_samples)\nage = np.random.randint(0, 50, n_samples)\ndistance_to_city = np.random.uniform(0, 30, n_samples)\n\n# Generate target (house prices)\nbase_price = 200000\nprice = (base_price + \n         100 * square_feet + \n         15000 * bedrooms + \n         20000 * bathrooms - \n         2000 * age - \n         5000 * distance_to_city + \n         np.random.normal(0, 20000, n_samples))\n\n# Create DataFrame\nhouse_data = pd.DataFrame({\n    'square_feet': square_feet,\n    'bedrooms': bedrooms,\n    'bathrooms': bathrooms,\n    'age': age,\n    'distance_to_city': distance_to_city,\n    'price': price\n})\n\nX_house = house_data.drop('price', axis=1)\ny_house = house_data['price']\n\n# Split data\nX_train_house, X_test_house, y_train_house, y_test_house = train_test_split(\n    X_house, y_house, test_size=0.3, random_state=42\n)\n\n# Define models to compare\nmodels = {\n    'Linear Regression': LinearRegression(),\n    'Ridge Regression': Ridge(alpha=1.0),\n    'Lasso Regression': Lasso(alpha=0.1),\n    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n}\n\n# Cross-validation comparison\ncv_results = {}\nfor name, model in models.items():\n    cv_scores = cross_val_score(model, X_train_house, y_train_house, \n                               cv=5, scoring='neg_mean_squared_error')\n    cv_results[name] = {\n        'mean_mse': -cv_scores.mean(),\n        'std_mse': cv_scores.std(),\n        'mean_rmse': np.sqrt(-cv_scores.mean())\n    }\n\n# Display results\nprint(\"Cross-Validation Results:\")\nprint(\"-\" * 50)\nfor name, result in cv_results.items():\n    print(f\"{name:20} RMSE: {result['mean_rmse']:.2f} ± {result['std_mse']:.2f}\")\n\n# Visualize results\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nnames = list(cv_results.keys())\nrmses = [cv_results[name]['mean_rmse'] for name in names]\nplt.bar(names, rmses)\nplt.ylabel('RMSE')\nplt.title('Cross-Validation RMSE Comparison')\nplt.xticks(rotation=45)\nfor i, v in enumerate(rmses):\n    plt.text(i, v + max(rmses)*0.01, f'{v:.0f}', ha='center', va='bottom')\n\n# Test set performance\nplt.subplot(1, 2, 2)\ntest_results = {}\nfor name, model in models.items():\n    model.fit(X_train_house, y_train_house)\n    y_pred = model.predict(X_test_house)\n    test_results[name] = np.sqrt(mean_squared_error(y_test_house, y_pred))\n\ntest_rmses = list(test_results.values())\nplt.bar(names, test_rmses)\nplt.ylabel('RMSE')\nplt.title('Test Set RMSE Comparison')\nplt.xticks(rotation=45)\nfor i, v in enumerate(test_rmses):\n    plt.text(i, v + max(test_rmses)*0.01, f'{v:.0f}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n# Feature importance for best model\nbest_model_name = min(test_results.keys(), key=lambda x: test_results[x])\nbest_model = models[best_model_name]\nbest_model.fit(X_train_house, y_train_house)\n\nif hasattr(best_model, 'feature_importances_'):\n    importance = best_model.feature_importances_\nelse:\n    importance = np.abs(best_model.coef_)\n\nplt.figure(figsize=(10, 6))\nfeature_names = X_house.columns\nplt.bar(feature_names, importance)\nplt.xlabel('Features')\nplt.ylabel('Importance')\nplt.title(f'Feature Importance - {best_model_name}')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform, randint\n\n# Define parameter distributions for Random Forest\nparam_dist = {\n    'n_estimators': randint(50, 300),\n    'max_depth': randint(3, 20),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 10)\n}\n\n# Randomized search\nrandom_search = RandomizedSearchCV(\n    RandomForestRegressor(random_state=42),\n    param_distributions=param_dist,\n    n_iter=100,\n    cv=5,\n    scoring='neg_mean_squared_error',\n    random_state=42,\n    n_jobs=-1\n)\n\nrandom_search.fit(X_train_house, y_train_house)\n\nprint(f\"Best parameters: {random_search.best_params_}\")\nprint(f\"Best CV score: {np.sqrt(-random_search.best_score_):.2f}\")\n\n# Compare with default parameters\ndefault_rf = RandomForestRegressor(random_state=42)\ndefault_rf.fit(X_train_house, y_train_house)\ndefault_score = np.sqrt(mean_squared_error(y_test_house, default_rf.predict(X_test_house)))\n\ntuned_rf = random_search.best_estimator_\ntuned_score = np.sqrt(mean_squared_error(y_test_house, tuned_rf.predict(X_test_house)))\n\nprint(f\"\\nTest RMSE - Default: {default_score:.2f}\")\nprint(f\"Test RMSE - Tuned: {tuned_score:.2f}\")\nprint(f\"Improvement: {((default_score - tuned_score) / default_score * 100):.1f}%\")\n\n# Visualize parameter importance\nparam_importance = random_search.cv_results_\nn_estimators_scores = []\nmax_depth_scores = []\n\nfor i in range(len(param_importance['param_n_estimators'])):\n    n_estimators_scores.append((param_importance['param_n_estimators'][i], \n                               -param_importance['mean_test_score'][i]))\n    max_depth_scores.append((param_importance['param_max_depth'][i], \n                           -param_importance['mean_test_score'][i]))\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nn_est_values, n_est_scores = zip(*n_estimators_scores)\nplt.scatter(n_est_values, n_est_scores, alpha=0.6)\nplt.xlabel('n_estimators')\nplt.ylabel('MSE')\nplt.title('n_estimators vs MSE')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\ndepth_values, depth_scores = zip(*max_depth_scores)\nplt.scatter(depth_values, depth_scores, alpha=0.6)\nplt.xlabel('max_depth')\nplt.ylabel('MSE')\nplt.title('max_depth vs MSE')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 8. Practice Problems\n\n### Problem 1: Cross-Validation Comparison\nCompare the performance of different cross-validation strategies (K-fold, stratified K-fold, leave-one-out) on a classification dataset.\n\n### Problem 2: Regularization Analysis\nGenerate data with multicollinearity and compare the performance of Linear Regression, Ridge Regression, and Lasso Regression.\n\n### Problem 3: Ensemble Method Comparison\nImplement and compare bagging, random forest, and gradient boosting on a regression problem.\n\n### Problem 4: Model Selection\nUse information criteria (AIC, BIC) and cross-validation to select the optimal polynomial degree for a regression problem.\n\n### Problem 5: Hyperparameter Tuning\nUse grid search and random search to tune hyperparameters for a machine learning model and compare the results.\n\n---\n\n## 9. Further Reading\n\n### Books\n- \"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman\n- \"An Introduction to Statistical Learning\" by James, Witten, Hastie, and Tibshirani\n- \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n\n### Papers\n- \"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection\" by Kohavi\n- \"Random Forests\" by Breiman\n- \"Greedy Function Approximation: A Gradient Boosting Machine\" by Friedman\n\n### Online Resources\n- Scikit-learn documentation on model selection\n- Cross-validation tutorials\n- Ensemble methods guides\n\n---\n\n## 10. Key Takeaways\n\n1. **Cross-validation** provides reliable estimates of model performance and helps prevent overfitting.\n\n2. **The bias-variance tradeoff** is fundamental to understanding model complexity and generalization.\n\n3. **Regularization** techniques (Ridge, Lasso) help prevent overfitting by constraining model parameters.\n\n4. **Ensemble methods** (bagging, boosting, random forest) often provide better predictions than individual models.\n\n5. **Model evaluation** requires multiple metrics and careful interpretation of results.\n\n6. **Hyperparameter tuning** can significantly improve model performance but requires computational resources.\n\n7. **Information criteria** (AIC, BIC) provide principled ways to balance model fit and complexity.\n\n8. **Feature selection** and importance analysis help understand model behavior and improve interpretability.\n\n---\n\n**Next Chapter**: [Advanced Topics](10-advanced-topics.md) - Non-parametric methods, survival analysis, and specialized statistical techniques."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}