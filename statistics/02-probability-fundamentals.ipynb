{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Probability Fundamentals\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Pandas](https://img.shields.io/badge/Pandas-1.3+-blue.svg)](https://pandas.pydata.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.4+-orange.svg)](https://matplotlib.org/)\n[![Seaborn](https://img.shields.io/badge/Seaborn-0.11+-blue.svg)](https://seaborn.pydata.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-green.svg)](https://scipy.org/)\n\nProbability theory is the mathematical foundation for statistics, machine learning, and data science. Understanding probability concepts is essential for making informed decisions under uncertainty.\n\n## Table of Contents\n- [Basic Probability Concepts](#basic-probability-concepts)\n- [Random Variables](#random-variables)\n- [Probability Distributions](#probability-distributions)\n- [Joint and Conditional Probability](#joint-and-conditional-probability)\n- [Bayes' Theorem](#bayes-theorem)\n- [Central Limit Theorem](#central-limit-theorem)\n- [Practical Applications](#practical-applications)\n\n## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, binom, poisson, expon, uniform, beta, gamma\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for better plots\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nnp.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Probability Concepts\n\nProbability provides a mathematical framework for quantifying uncertainty. It allows us to make predictions and decisions in the face of incomplete information.\n\n### Sample Space and Events\n\n**Sample Space (Ω)**: The set of all possible outcomes of an experiment.\n\n**Event**: A subset of the sample space.\n\n**Mathematical Definition:**\n- Sample Space: Ω = {ω₁, ω₂, ..., ωₙ}\n- Event A: A ⊆ Ω\n- Probability Function: P: 2^Ω → [0,1] satisfying:\n  1. P(Ω) = 1\n  2. P(A) ≥ 0 for all A ⊆ Ω\n  3. P(A∪B) = P(A) + P(B) if A∩B = ∅ (additivity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_probability(event_outcomes, total_outcomes):\n    \"\"\"\n    Calculate probability of an event\n    \n    Mathematical implementation:\n    P(A) = |A| / |Ω|\n    where |A| is the number of outcomes in event A\n    and |Ω| is the total number of possible outcomes\n    \n    Parameters:\n    event_outcomes: int, number of favorable outcomes\n    total_outcomes: int, total number of possible outcomes\n    \n    Returns:\n    float: probability between 0 and 1\n    \"\"\"\n    if total_outcomes == 0:\n        raise ValueError(\"Total outcomes cannot be zero\")\n    if event_outcomes < 0 or event_outcomes > total_outcomes:\n        raise ValueError(\"Event outcomes must be between 0 and total outcomes\")\n    \n    return event_outcomes / total_outcomes\n\n# Example: Rolling a fair die\nsample_space = {1, 2, 3, 4, 5, 6}  # Ω\nevent_even = {2, 4, 6}  # A = \"rolling an even number\"\nevent_odd = {1, 3, 5}   # B = \"rolling an odd number\"\n\np_even = calculate_probability(len(event_even), len(sample_space))\np_odd = calculate_probability(len(event_odd), len(sample_space))\n\nprint(f\"Sample space: {sample_space}\")\nprint(f\"Event 'even numbers': {event_even}\")\nprint(f\"P(even) = {p_even:.3f}\")\nprint(f\"P(odd) = {p_odd:.3f}\")\nprint(f\"P(even) + P(odd) = {p_even + p_odd:.3f}\")\n\n# Verify probability axioms\nprint(f\"P(Ω) = {calculate_probability(len(sample_space), len(sample_space)):.3f}\")\nprint(f\"P(∅) = {calculate_probability(0, len(sample_space)):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conditional Probability\n\n**Conditional Probability** measures the probability of an event given that another event has occurred.\n\n**Mathematical Definition:**\n$$P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\quad \\text{where } P(B) > 0$$\n\n**Interpretation:**\n- P(A|B) is the probability of A occurring given that B has occurred\n- This \"updates\" our belief about A based on the information that B occurred\n- The formula can be rearranged to give: P(A∩B) = P(A|B) × P(B)\n\n**Properties:**\n1. **Range**: 0 ≤ P(A|B) ≤ 1\n2. **Normalization**: P(Ω|B) = 1\n3. **Additivity**: P(A∪C|B) = P(A|B) + P(C|B) if A∩C = ∅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conditional_probability(p_a_and_b, p_b):\n    \"\"\"\n    Calculate conditional probability P(A|B)\n    \n    Mathematical implementation:\n    P(A|B) = P(A∩B) / P(B)\n    \n    Parameters:\n    p_a_and_b: float, P(A∩B)\n    p_b: float, P(B)\n    \n    Returns:\n    float: P(A|B)\n    \"\"\"\n    if p_b == 0:\n        raise ValueError(\"Cannot condition on event with zero probability\")\n    if p_a_and_b > p_b:\n        raise ValueError(\"P(A∩B) cannot be greater than P(B)\")\n    \n    return p_a_and_b / p_b\n\n# Example: Medical test scenario\n# P(disease) = 0.01 (1% of population has disease)\n# P(positive test|disease) = 0.95 (95% sensitivity)\n# P(negative test|no disease) = 0.90 (90% specificity)\n\np_disease = 0.01\np_positive_given_disease = 0.95\np_negative_given_no_disease = 0.90\n\n# Calculate P(positive test and disease)\np_positive_and_disease = p_positive_given_disease * p_disease\n\n# Calculate P(positive test)\np_positive = (p_positive_given_disease * p_disease + \n              (1 - p_negative_given_no_disease) * (1 - p_disease))\n\n# Calculate P(disease|positive test) using Bayes' theorem\np_disease_given_positive = conditional_probability(p_positive_and_disease, p_positive)\n\nprint(f\"P(disease) = {p_disease:.3f}\")\nprint(f\"P(positive|disease) = {p_positive_given_disease:.3f}\")\nprint(f\"P(negative|no disease) = {p_negative_given_no_disease:.3f}\")\nprint(f\"P(positive and disease) = {p_positive_and_disease:.4f}\")\nprint(f\"P(positive) = {p_positive:.4f}\")\nprint(f\"P(disease|positive) = {p_disease_given_positive:.4f}\")\n\n# Demonstrate the relationship\nprint(f\"P(disease|positive) × P(positive) = {p_disease_given_positive * p_positive:.4f}\")\nprint(f\"P(positive|disease) × P(disease) = {p_positive_given_disease * p_disease:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Independence\n\nTwo events A and B are **independent** if the occurrence of one does not affect the probability of the other.\n\n**Mathematical Definition:**\nEvents A and B are independent if and only if:\n$$P(A \\cap B) = P(A) \\times P(B)$$\n\n**Equivalent Definitions:**\n1. P(A|B) = P(A) (if P(B) > 0)\n2. P(B|A) = P(B) (if P(A) > 0)\n\n**Properties:**\n1. **Symmetry**: If A is independent of B, then B is independent of A\n2. **Transitivity**: Independence is not transitive\n3. **Complement**: If A and B are independent, then A and B^c are independent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_independence(p_a, p_b, p_a_and_b, tolerance=1e-10):\n    \"\"\"\n    Check if two events are independent\n    \n    Mathematical implementation:\n    Events A and B are independent if P(A∩B) = P(A) × P(B)\n    \n    Parameters:\n    p_a: float, P(A)\n    p_b: float, P(B)\n    p_a_and_b: float, P(A∩B)\n    tolerance: float, numerical tolerance for comparison\n    \n    Returns:\n    bool: True if events are independent\n    \"\"\"\n    expected_p_a_and_b = p_a * p_b\n    return abs(p_a_and_b - expected_p_a_and_b) < tolerance\n\n# Example: Coin flips\np_heads_first = 0.5\np_heads_second = 0.5\np_both_heads = 0.25  # For fair coins\n\nis_independent = check_independence(p_heads_first, p_heads_second, p_both_heads)\nprint(f\"P(first flip heads) = {p_heads_first}\")\nprint(f\"P(second flip heads) = {p_heads_second}\")\nprint(f\"P(both heads) = {p_both_heads}\")\nprint(f\"P(first) × P(second) = {p_heads_first * p_heads_second}\")\nprint(f\"Events are independent: {is_independent}\")\n\n# Example: Dependent events (drawing cards without replacement)\n# P(first card is ace) = 4/52\n# P(second card is ace|first card is ace) = 3/51\n# P(second card is ace|first card is not ace) = 4/51\n\np_first_ace = 4/52\np_second_ace_given_first_ace = 3/51\np_second_ace_given_first_not_ace = 4/51\n\n# Calculate P(second ace)\np_second_ace = (p_second_ace_given_first_ace * p_first_ace + \n                p_second_ace_given_first_not_ace * (1 - p_first_ace))\n\n# Calculate P(both aces)\np_both_aces = p_first_ace * p_second_ace_given_first_ace\n\nis_independent_cards = check_independence(p_first_ace, p_second_ace, p_both_aces)\nprint(f\"\\nCard example:\")\nprint(f\"P(first ace) = {p_first_ace:.4f}\")\nprint(f\"P(second ace) = {p_second_ace:.4f}\")\nprint(f\"P(both aces) = {p_both_aces:.4f}\")\nprint(f\"P(first) × P(second) = {p_first_ace * p_second_ace:.4f}\")\nprint(f\"Events are independent: {is_independent_cards}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Law of Total Probability\n\nThe **Law of Total Probability** allows us to calculate the probability of an event by conditioning on a partition of the sample space.\n\n**Mathematical Definition:**\nIf B₁, B₂, ..., Bₙ form a partition of Ω (i.e., they are mutually exclusive and exhaustive), then:\n$$P(A) = \\sum_{i=1}^{n} P(A|B_i) \\times P(B_i)$$\n\n**Special Case (Two Events):**\n$$P(A) = P(A|B) \\times P(B) + P(A|B^c) \\times P(B^c)$$\n\n**Applications:**\n- Medical diagnosis\n- Quality control\n- Risk assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def law_of_total_probability(conditional_probs, partition_probs):\n    \"\"\"\n    Calculate probability using Law of Total Probability\n    \n    Mathematical implementation:\n    P(A) = Σ P(A|B_i) × P(B_i)\n    \n    Parameters:\n    conditional_probs: list, P(A|B_i) for each partition\n    partition_probs: list, P(B_i) for each partition\n    \n    Returns:\n    float: P(A)\n    \"\"\"\n    if len(conditional_probs) != len(partition_probs):\n        raise ValueError(\"Conditional and partition probabilities must have same length\")\n    \n    if abs(sum(partition_probs) - 1.0) > 1e-10:\n        raise ValueError(\"Partition probabilities must sum to 1\")\n    \n    return sum(c * p for c, p in zip(conditional_probs, partition_probs))\n\n# Example: Quality control\n# Three machines produce widgets with different defect rates\n# Machine 1: 60% of production, 2% defect rate\n# Machine 2: 30% of production, 3% defect rate  \n# Machine 3: 10% of production, 5% defect rate\n\nproduction_shares = [0.60, 0.30, 0.10]  # P(B_i)\ndefect_rates = [0.02, 0.03, 0.05]       # P(defect|B_i)\n\noverall_defect_rate = law_of_total_probability(defect_rates, production_shares)\n\nprint(\"Quality Control Example:\")\nfor i, (share, rate) in enumerate(zip(production_shares, defect_rates)):\n    print(f\"Machine {i+1}: {share*100:.0f}% production, {rate*100:.1f}% defect rate\")\n\nprint(f\"Overall defect rate: {overall_defect_rate:.4f} ({overall_defect_rate*100:.2f}%)\")\n\n# Verification\nmanual_calculation = sum(rate * share for rate, share in zip(defect_rates, production_shares))\nprint(f\"Manual calculation: {manual_calculation:.4f}\")\n\n# Example: Medical diagnosis with multiple symptoms\n# P(disease) = 0.01\n# P(symptom1|disease) = 0.8, P(symptom1|no disease) = 0.1\n# P(symptom2|disease) = 0.6, P(symptom2|no disease) = 0.05\n\np_disease = 0.01\np_no_disease = 1 - p_disease\n\n# Calculate P(symptom1)\np_symptom1_given_disease = 0.8\np_symptom1_given_no_disease = 0.1\n\np_symptom1 = law_of_total_probability(\n    [p_symptom1_given_disease, p_symptom1_given_no_disease],\n    [p_disease, p_no_disease]\n)\n\nprint(f\"\\nMedical Diagnosis Example:\")\nprint(f\"P(disease) = {p_disease:.3f}\")\nprint(f\"P(symptom1|disease) = {p_symptom1_given_disease:.1f}\")\nprint(f\"P(symptom1|no disease) = {p_symptom1_given_no_disease:.1f}\")\nprint(f\"P(symptom1) = {p_symptom1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bayes' Theorem\n\n**Bayes' Theorem** provides a way to update probabilities based on new evidence.\n\n**Mathematical Definition:**\n$$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$$\n\nWhere P(B) can be calculated using the Law of Total Probability:\n$$P(B) = P(B|A) \\times P(A) + P(B|A^c) \\times P(A^c)$$\n\n**Interpretation:**\n- P(A): Prior probability (before evidence)\n- P(A|B): Posterior probability (after evidence)\n- P(B|A): Likelihood (how likely is the evidence given the hypothesis)\n- P(B): Evidence (total probability of observing the evidence)\n\n**Applications:**\n- Medical diagnosis\n- Spam filtering\n- Machine learning (Naive Bayes)\n- Bayesian inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):\n    \"\"\"\n    Calculate posterior probability using Bayes' theorem\n    \n    Mathematical implementation:\n    P(A|B) = P(B|A) × P(A) / P(B)\n    where P(B) = P(B|A) × P(A) + P(B|A^c) × P(A^c)\n    \n    Parameters:\n    p_a: float, prior probability P(A)\n    p_b_given_a: float, likelihood P(B|A)\n    p_b_given_not_a: float, likelihood P(B|A^c)\n    \n    Returns:\n    float: posterior probability P(A|B)\n    \"\"\"\n    p_not_a = 1 - p_a\n    \n    # Calculate P(B) using Law of Total Probability\n    p_b = p_b_given_a * p_a + p_b_given_not_a * p_not_a\n    \n    if p_b == 0:\n        raise ValueError(\"Evidence probability cannot be zero\")\n    \n    # Apply Bayes' theorem\n    p_a_given_b = (p_b_given_a * p_a) / p_b\n    \n    return p_a_given_b\n\n# Example: Medical test revisited\np_disease = 0.01  # Prior: 1% of population has disease\np_positive_given_disease = 0.95  # Sensitivity\np_positive_given_no_disease = 0.10  # 1 - Specificity\n\np_disease_given_positive = bayes_theorem(\n    p_disease, p_positive_given_disease, p_positive_given_no_disease\n)\n\nprint(\"Bayes' Theorem - Medical Test:\")\nprint(f\"Prior P(disease) = {p_disease:.3f}\")\nprint(f\"Likelihood P(positive|disease) = {p_positive_given_disease:.2f}\")\nprint(f\"Likelihood P(positive|no disease) = {p_positive_given_no_disease:.2f}\")\nprint(f\"Posterior P(disease|positive) = {p_disease_given_positive:.4f}\")\n\n# Demonstrate how prior affects posterior\npriors = [0.001, 0.01, 0.1, 0.5]\nprint(f\"\\nEffect of Prior on Posterior:\")\nfor prior in priors:\n    posterior = bayes_theorem(prior, p_positive_given_disease, p_positive_given_no_disease)\n    print(f\"Prior: {prior:.3f} → Posterior: {posterior:.4f}\")\n\n# Visualize Bayes' theorem\nplt.figure(figsize=(10, 6))\npriors = np.linspace(0.001, 0.5, 100)\nposteriors = []\nsensitivity = 0.95\nspecificity = 0.90\n\nfor prior in priors:\n    likelihood = sensitivity\n    evidence = (sensitivity * prior + (1 - specificity) * (1 - prior))\n    posterior = (likelihood * prior) / evidence\n    posteriors.append(posterior)\n\nplt.plot(priors, posteriors, 'b-', linewidth=2, label='Posterior')\nplt.plot(priors, priors, 'r--', linewidth=2, label='Prior')\nplt.xlabel('Prior Probability')\nplt.ylabel('Posterior Probability')\nplt.title(\"Bayes' Theorem: How Prior Affects Posterior\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Variables\n\nA **random variable** is a function that assigns a numerical value to each outcome in a sample space. Random variables are the foundation for probability distributions and statistical modeling.\n\n### Discrete Random Variables\n\nA **discrete random variable** takes on a countable number of distinct values.\n\n**Mathematical Definition:**\nA discrete random variable X is a function X: Ω → ℝ such that:\n- The range of X is countable\n- For each x ∈ ℝ, the set {ω ∈ Ω : X(ω) = x} is an event\n\n**Probability Mass Function (PMF):**\n$$p_X(x) = P(X = x)$$\n\n**Properties of PMF:**\n1. $p_X(x) \\geq 0$ for all x\n2. $\\sum_{x} p_X(x) = 1$\n3. $P(X \\in A) = \\sum_{x \\in A} p_X(x)$\n\n**Cumulative Distribution Function (CDF):**\n$$F_X(x) = P(X \\leq x) = \\sum_{k \\leq x} p_X(k)$$\n\n**Expected Value (Mean):**\n$$\\mu = E[X] = \\sum_{x} x \\cdot p_X(x)$$\n\n**Variance:**\n$$\\sigma^2 = \\text{Var}(X) = E[(X - \\mu)^2] = \\sum_{x} (x - \\mu)^2 \\cdot p_X(x)$$\n\n**Moment Generating Function (MGF):**\n$$M_X(t) = E[e^{tX}] = \\sum_{x} e^{tx} \\cdot p_X(x)$$\n\n**Properties of Expected Value:**\n1. **Linearity**: E[aX + b] = aE[X] + b\n2. **Additivity**: E[X + Y] = E[X] + E[Y] (if X, Y are independent)\n3. **Monotonicity**: If X ≤ Y, then E[X] ≤ E[Y]\n\n**Properties of Variance:**\n1. **Non-negativity**: Var(X) ≥ 0\n2. **Scale**: Var(aX + b) = a²Var(X)\n3. **Additivity**: Var(X + Y) = Var(X) + Var(Y) (if X, Y are independent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discrete_pmf(values, probabilities):\n    \"\"\"\n    Create a discrete probability mass function\n    \n    Mathematical implementation:\n    PMF: p_X(x) = P(X = x)\n    \n    Parameters:\n    values: array-like, possible values of the random variable\n    probabilities: array-like, corresponding probabilities\n    \n    Returns:\n    dict: PMF as {value: probability}\n    \"\"\"\n    if len(values) != len(probabilities):\n        raise ValueError(\"Values and probabilities must have same length\")\n    \n    if abs(sum(probabilities) - 1.0) > 1e-10:\n        raise ValueError(\"Probabilities must sum to 1\")\n    \n    if any(p < 0 for p in probabilities):\n        raise ValueError(\"Probabilities must be non-negative\")\n    \n    return dict(zip(values, probabilities))\n\ndef discrete_cdf(pmf, x):\n    \"\"\"\n    Calculate cumulative distribution function for discrete random variable\n    \n    Mathematical implementation:\n    CDF: F_X(x) = P(X ≤ x) = Σ_{k≤x} p_X(k)\n    \n    Parameters:\n    pmf: dict, probability mass function\n    x: float, point to evaluate CDF\n    \n    Returns:\n    float: F_X(x)\n    \"\"\"\n    return sum(prob for value, prob in pmf.items() if value <= x)\n\ndef discrete_expected_value(pmf):\n    \"\"\"\n    Calculate expected value of discrete random variable\n    \n    Mathematical implementation:\n    E[X] = Σ x × p_X(x)\n    \n    Parameters:\n    pmf: dict, probability mass function\n    \n    Returns:\n    float: expected value\n    \"\"\"\n    return sum(value * prob for value, prob in pmf.items())\n\ndef discrete_variance(pmf):\n    \"\"\"\n    Calculate variance of discrete random variable\n    \n    Mathematical implementation:\n    Var(X) = E[(X - μ)²] = Σ (x - μ)² × p_X(x)\n    \n    Parameters:\n    pmf: dict, probability mass function\n    \n    Returns:\n    float: variance\n    \"\"\"\n    mu = discrete_expected_value(pmf)\n    return sum((value - mu)**2 * prob for value, prob in pmf.items())\n\ndef discrete_mgf(pmf, t):\n    \"\"\"\n    Calculate moment generating function for discrete random variable\n    \n    Mathematical implementation:\n    M_X(t) = E[e^(tX)] = Σ e^(tx) × p_X(x)\n    \n    Parameters:\n    pmf: dict, probability mass function\n    t: float, parameter for MGF\n    \n    Returns:\n    float: M_X(t)\n    \"\"\"\n    return sum(np.exp(t * value) * prob for value, prob in pmf.items())\n\ndef discrete_moments_from_mgf(pmf, k=4):\n    \"\"\"\n    Calculate moments using moment generating function\n    \n    Mathematical implementation:\n    E[X^k] = M_X^(k)(0) = d^k/dt^k M_X(t) |_{t=0}\n    \n    Parameters:\n    pmf: dict, probability mass function\n    k: int, order of moment\n    \n    Returns:\n    list: moments from order 1 to k\n    \"\"\"\n    moments = []\n    for i in range(1, k + 1):\n        # Numerical differentiation\n        h = 1e-6\n        if i == 1:\n            moment = (discrete_mgf(pmf, h) - discrete_mgf(pmf, -h)) / (2 * h)\n        elif i == 2:\n            moment = (discrete_mgf(pmf, h) - 2 * discrete_mgf(pmf, 0) + discrete_mgf(pmf, -h)) / (h**2)\n        else:\n            # Higher order moments using finite differences\n            moment = discrete_mgf(pmf, 0)  # Placeholder for higher orders\n        moments.append(moment)\n    return moments\n\n# Example: Fair die\ndie_values = [1, 2, 3, 4, 5, 6]\ndie_probs = [1/6] * 6\ndie_pmf = discrete_pmf(die_values, die_probs)\n\nprint(\"Fair Die Example:\")\nprint(f\"PMF: {die_pmf}\")\nprint(f\"Expected value: {discrete_expected_value(die_pmf):.3f}\")\nprint(f\"Variance: {discrete_variance(die_pmf):.3f}\")\n\n# Calculate CDF at various points\nfor x in [0, 1, 3.5, 6, 7]:\n    cdf_val = discrete_cdf(die_pmf, x)\n    print(f\"F({x}) = {cdf_val:.3f}\")\n\n# Calculate MGF at t = 0.5\nmgf_val = discrete_mgf(die_pmf, 0.5)\nprint(f\"MGF at t=0.5: {mgf_val:.4f}\")\n\n# Calculate moments from MGF\nmoments = discrete_moments_from_mgf(die_pmf, 2)\nprint(f\"First moment (mean): {moments[0]:.3f}\")\nprint(f\"Second moment: {moments[1]:.3f}\")\n\n# Example: Loaded die (biased towards 6)\nloaded_values = [1, 2, 3, 4, 5, 6]\nloaded_probs = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]  # 50% chance of rolling 6\nloaded_pmf = discrete_pmf(loaded_values, loaded_probs)\n\nprint(f\"\\nLoaded Die Example:\")\nprint(f\"PMF: {loaded_pmf}\")\nprint(f\"Expected value: {discrete_expected_value(loaded_pmf):.3f}\")\nprint(f\"Variance: {discrete_variance(loaded_pmf):.3f}\")\n\n# Compare fair vs loaded die\nprint(f\"\\nComparison:\")\nprint(f\"Fair die E[X]: {discrete_expected_value(die_pmf):.3f}\")\nprint(f\"Loaded die E[X]: {discrete_expected_value(loaded_pmf):.3f}\")\nprint(f\"Fair die Var(X): {discrete_variance(die_pmf):.3f}\")\nprint(f\"Loaded die Var(X): {discrete_variance(loaded_pmf):.3f}\")\n\n# Example: Bernoulli random variable (coin flip)\ndef bernoulli_pmf(p):\n    \"\"\"\n    Create Bernoulli PMF\n    \n    Mathematical implementation:\n    X ~ Bernoulli(p)\n    p_X(0) = 1-p, p_X(1) = p\n    E[X] = p, Var(X) = p(1-p)\n    \"\"\"\n    return {0: 1-p, 1: p}\n\np = 0.7  # Probability of success\nbernoulli = bernoulli_pmf(p)\n\nprint(f\"\\nBernoulli Example (p={p}):\")\nprint(f\"PMF: {bernoulli}\")\nprint(f\"E[X] = {discrete_expected_value(bernoulli):.3f}\")\nprint(f\"Var(X) = {discrete_variance(bernoulli):.3f}\")\n\n# Theoretical values for Bernoulli\nprint(f\"Theoretical E[X] = p = {p:.3f}\")\nprint(f\"Theoretical Var(X) = p(1-p) = {p*(1-p):.3f}\")\n\n# Calculate MGF for Bernoulli\nt_values = np.linspace(-2, 2, 100)\nmgf_values = [discrete_mgf(bernoulli, t) for t in t_values]\n\n# Visualize MGF\nplt.figure(figsize=(12, 8))\n\n# Plot 1: PMF\nplt.subplot(2, 2, 1)\nvalues = list(bernoulli.keys())\nprobs = list(bernoulli.values())\nplt.bar(values, probs, alpha=0.7, color='skyblue', edgecolor='navy')\nplt.xlabel('X')\nplt.ylabel('P(X = x)')\nplt.title('Bernoulli PMF')\nplt.grid(True, alpha=0.3)\n\n# Plot 2: CDF\nplt.subplot(2, 2, 2)\nx_range = np.linspace(-0.5, 1.5, 1000)\ncdf_values = [discrete_cdf(bernoulli, x) for x in x_range]\nplt.step(x_range, cdf_values, where='post', linewidth=2, color='green')\nplt.xlabel('x')\nplt.ylabel('F(x)')\nplt.title('Bernoulli CDF')\nplt.grid(True, alpha=0.3)\n\n# Plot 3: MGF\nplt.subplot(2, 2, 3)\nplt.plot(t_values, mgf_values, 'r-', linewidth=2)\nplt.xlabel('t')\nplt.ylabel('M_X(t)')\nplt.title('Bernoulli MGF')\nplt.grid(True, alpha=0.3)\n\n# Plot 4: Comparison of distributions\nplt.subplot(2, 2, 4)\nx_pos = np.arange(2)\nfair_probs = [1/6] * 6\nloaded_probs = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]\n\nplt.bar(x_pos - 0.2, [discrete_expected_value(die_pmf), discrete_variance(die_pmf)], \n        0.4, label='Fair Die', alpha=0.7)\nplt.bar(x_pos + 0.2, [discrete_expected_value(loaded_pmf), discrete_variance(loaded_pmf)], \n        0.4, label='Loaded Die', alpha=0.7)\nplt.xticks(x_pos, ['Mean', 'Variance'])\nplt.ylabel('Value')\nplt.title('Comparison of Moments')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Continuous Random Variables\n\nA **continuous random variable** takes on values in a continuous range.\n\n**Mathematical Definition:**\nA continuous random variable X is a function X: Ω → ℝ such that:\n- The range of X is uncountable\n- There exists a function f_X(x) ≥ 0 such that:\n  $$P(X \\in A) = \\int_A f_X(x) dx$$\n\n**Probability Density Function (PDF):**\n$$f_X(x) = \\frac{d}{dx} F_X(x)$$\n\n**Properties of PDF:**\n1. $f_X(x) \\geq 0$ for all x\n2. $\\int_{-\\infty}^{\\infty} f_X(x) dx = 1$\n3. $P(X \\in A) = \\int_A f_X(x) dx$\n\n**Cumulative Distribution Function (CDF):**\n$$F_X(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f_X(t) dt$$\n\n**Expected Value:**\n$$\\mu = E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f_X(x) dx$$\n\n**Variance:**\n$$\\sigma^2 = \\text{Var}(X) = E[(X - \\mu)^2] = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 \\cdot f_X(x) dx$$\n\n**Moment Generating Function:**\n$$M_X(t) = E[e^{tX}] = \\int_{-\\infty}^{\\infty} e^{tx} \\cdot f_X(x) dx$$\n\n**Properties:**\n1. **Linearity**: E[aX + b] = aE[X] + b\n2. **Additivity**: E[X + Y] = E[X] + E[Y] (if X, Y are independent)\n3. **Variance**: Var(aX + b) = a²Var(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def continuous_pdf_check(pdf_func, a, b, tolerance=1e-6):\n    \"\"\"\n    Check if a function is a valid PDF on interval [a, b]\n    \n    Mathematical implementation:\n    Check: ∫_a^b f(x) dx = 1 and f(x) ≥ 0 for all x ∈ [a, b]\n    \n    Parameters:\n    pdf_func: function, PDF to check\n    a, b: float, interval bounds\n    tolerance: float, numerical tolerance\n    \n    Returns:\n    bool: True if valid PDF\n    \"\"\"\n    from scipy.integrate import quad\n    \n    # Check if integral equals 1\n    integral, _ = quad(pdf_func, a, b)\n    if abs(integral - 1.0) > tolerance:\n        return False\n    \n    # Check if function is non-negative (sample points)\n    x_samples = np.linspace(a, b, 1000)\n    if any(pdf_func(x) < -tolerance for x in x_samples):\n        return False\n    \n    return True\n\ndef continuous_cdf(pdf_func, x, a, b):\n    \"\"\"\n    Calculate CDF for continuous random variable\n    \n    Mathematical implementation:\n    F_X(x) = ∫_a^x f_X(t) dt\n    \n    Parameters:\n    pdf_func: function, PDF\n    x: float, point to evaluate\n    a, b: float, support interval\n    \n    Returns:\n    float: F_X(x)\n    \"\"\"\n    from scipy.integrate import quad\n    \n    if x < a:\n        return 0.0\n    elif x > b:\n        return 1.0\n    else:\n        integral, _ = quad(pdf_func, a, x)\n        return integral\n\ndef continuous_expected_value(pdf_func, a, b):\n    \"\"\"\n    Calculate expected value of continuous random variable\n    \n    Mathematical implementation:\n    E[X] = ∫_a^b x × f_X(x) dx\n    \n    Parameters:\n    pdf_func: function, PDF\n    a, b: float, support interval\n    \n    Returns:\n    float: expected value\n    \"\"\"\n    from scipy.integrate import quad\n    \n    def integrand(x):\n        return x * pdf_func(x)\n    \n    integral, _ = quad(integrand, a, b)\n    return integral\n\ndef continuous_variance(pdf_func, a, b):\n    \"\"\"\n    Calculate variance of continuous random variable\n    \n    Mathematical implementation:\n    Var(X) = E[(X - μ)²] = ∫_a^b (x - μ)² × f_X(x) dx\n    \n    Parameters:\n    pdf_func: function, PDF\n    a, b: float, support interval\n    \n    Returns:\n    float: variance\n    \"\"\"\n    from scipy.integrate import quad\n    \n    mu = continuous_expected_value(pdf_func, a, b)\n    \n    def integrand(x):\n        return (x - mu)**2 * pdf_func(x)\n    \n    integral, _ = quad(integrand, a, b)\n    return integral\n\ndef continuous_mgf(pdf_func, t, a, b):\n    \"\"\"\n    Calculate moment generating function for continuous random variable\n    \n    Mathematical implementation:\n    M_X(t) = E[e^(tX)] = ∫_a^b e^(tx) × f_X(x) dx\n    \n    Parameters:\n    pdf_func: function, PDF\n    t: float, parameter for MGF\n    a, b: float, support interval\n    \n    Returns:\n    float: M_X(t)\n    \"\"\"\n    from scipy.integrate import quad\n    \n    def integrand(x):\n        return np.exp(t * x) * pdf_func(x)\n    \n    integral, _ = quad(integrand, a, b)\n    return integral\n\n# Example: Uniform distribution on [0, 1]\ndef uniform_pdf(x):\n    \"\"\"PDF of uniform distribution on [0, 1]\"\"\"\n    if 0 <= x <= 1:\n        return 1.0\n    else:\n        return 0.0\n\nprint(\"Uniform Distribution Example [0, 1]:\")\nprint(f\"Valid PDF: {continuous_pdf_check(uniform_pdf, 0, 1)}\")\n\n# Calculate CDF at various points\nfor x in [-0.5, 0, 0.25, 0.5, 0.75, 1, 1.5]:\n    cdf_val = continuous_cdf(uniform_pdf, x, 0, 1)\n    print(f\"F({x}) = {cdf_val:.3f}\")\n\n# Calculate moments\nuniform_mean = continuous_expected_value(uniform_pdf, 0, 1)\nuniform_var = continuous_variance(uniform_pdf, 0, 1)\n\nprint(f\"E[X] = {uniform_mean:.3f}\")\nprint(f\"Var(X) = {uniform_var:.3f}\")\n\n# Theoretical values for uniform [0, 1]\nprint(f\"Theoretical E[X] = (a+b)/2 = 0.5\")\nprint(f\"Theoretical Var(X) = (b-a)²/12 = {1/12:.3f}\")\n\n# Calculate MGF\nt_values = np.linspace(-2, 2, 100)\nmgf_values = [continuous_mgf(uniform_pdf, t, 0, 1) for t in t_values]\n\n# Example: Exponential distribution\ndef exponential_pdf(x, lambda_param=1.0):\n    \"\"\"PDF of exponential distribution with rate λ\"\"\"\n    if x >= 0:\n        return lambda_param * np.exp(-lambda_param * x)\n    else:\n        return 0.0\n\nlambda_val = 2.0\nexp_pdf = lambda x: exponential_pdf(x, lambda_val)\n\nprint(f\"\\nExponential Distribution Example (λ={lambda_val}):\")\nprint(f\"Valid PDF: {continuous_pdf_check(exp_pdf, 0, np.inf)}\")\n\n# Calculate CDF at various points\nfor x in [0, 0.5, 1, 2, 3]:\n    cdf_val = continuous_cdf(exp_pdf, x, 0, np.inf)\n    print(f\"F({x}) = {cdf_val:.3f}\")\n\n# Calculate moments\nexp_mean = continuous_expected_value(exp_pdf, 0, np.inf)\nexp_var = continuous_variance(exp_pdf, 0, np.inf)\n\nprint(f\"E[X] = {exp_mean:.3f}\")\nprint(f\"Var(X) = {exp_var:.3f}\")\n\n# Theoretical values for exponential\nprint(f\"Theoretical E[X] = 1/λ = {1/lambda_val:.3f}\")\nprint(f\"Theoretical Var(X) = 1/λ² = {1/lambda_val**2:.3f}\")\n\n# Example: Custom PDF (triangular distribution)\ndef triangular_pdf(x):\n    \"\"\"PDF of triangular distribution on [0, 2] with peak at 1\"\"\"\n    if 0 <= x <= 1:\n        return x\n    elif 1 <= x <= 2:\n        return 2 - x\n    else:\n        return 0.0\n\nprint(f\"\\nTriangular Distribution Example [0, 2]:\")\nprint(f\"Valid PDF: {continuous_pdf_check(triangular_pdf, 0, 2)}\")\n\n# Calculate moments\ntri_mean = continuous_expected_value(triangular_pdf, 0, 2)\ntri_var = continuous_variance(triangular_pdf, 0, 2)\n\nprint(f\"E[X] = {tri_mean:.3f}\")\nprint(f\"Var(X) = {tri_var:.3f}\")\n\n# Calculate CDF at various points\nfor x in [0, 0.5, 1, 1.5, 2]:\n    cdf_val = continuous_cdf(triangular_pdf, x, 0, 2)\n    print(f\"F({x}) = {cdf_val:.3f}\")\n\n# Visualize continuous distributions\nplt.figure(figsize=(15, 10))\n\n# Plot 1: Uniform PDF and CDF\nplt.subplot(2, 3, 1)\nx_uniform = np.linspace(-0.5, 1.5, 1000)\ny_pdf = [uniform_pdf(x) for x in x_uniform]\ny_cdf = [continuous_cdf(uniform_pdf, x, 0, 1) for x in x_uniform]\n\nplt.plot(x_uniform, y_pdf, 'b-', linewidth=2, label='PDF')\nplt.plot(x_uniform, y_cdf, 'r--', linewidth=2, label='CDF')\nplt.xlabel('x')\nplt.ylabel('f(x), F(x)')\nplt.title('Uniform Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Exponential PDF and CDF\nplt.subplot(2, 3, 2)\nx_exp = np.linspace(0, 4, 1000)\ny_pdf_exp = [exp_pdf(x) for x in x_exp]\ny_cdf_exp = [continuous_cdf(exp_pdf, x, 0, np.inf) for x in x_exp]\n\nplt.plot(x_exp, y_pdf_exp, 'b-', linewidth=2, label='PDF')\nplt.plot(x_exp, y_cdf_exp, 'r--', linewidth=2, label='CDF')\nplt.xlabel('x')\nplt.ylabel('f(x), F(x)')\nplt.title('Exponential Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 3: Triangular PDF and CDF\nplt.subplot(2, 3, 3)\nx_tri = np.linspace(-0.5, 2.5, 1000)\ny_pdf_tri = [triangular_pdf(x) for x in x_tri]\ny_cdf_tri = [continuous_cdf(triangular_pdf, x, 0, 2) for x in x_tri]\n\nplt.plot(x_tri, y_pdf_tri, 'b-', linewidth=2, label='PDF')\nplt.plot(x_tri, y_cdf_tri, 'r--', linewidth=2, label='CDF')\nplt.xlabel('x')\nplt.ylabel('f(x), F(x)')\nplt.title('Triangular Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 4: MGF comparison\nplt.subplot(2, 3, 4)\nt_range = np.linspace(-1, 1, 100)\nmgf_uniform = [continuous_mgf(uniform_pdf, t, 0, 1) for t in t_range]\nmgf_exp = [continuous_mgf(exp_pdf, t, 0, np.inf) for t in t_range]\n\nplt.plot(t_range, mgf_uniform, 'b-', linewidth=2, label='Uniform')\nplt.plot(t_range, mgf_exp, 'r-', linewidth=2, label='Exponential')\nplt.xlabel('t')\nplt.ylabel('M_X(t)')\nplt.title('Moment Generating Functions')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 5: Moments comparison\nplt.subplot(2, 3, 5)\ndistributions = ['Uniform', 'Exponential', 'Triangular']\nmeans = [uniform_mean, exp_mean, tri_mean]\nvariances = [uniform_var, exp_var, tri_var]\n\nx_pos = np.arange(len(distributions))\nplt.bar(x_pos - 0.2, means, 0.4, label='Mean', alpha=0.7)\nplt.bar(x_pos + 0.2, variances, 0.4, label='Variance', alpha=0.7)\nplt.xticks(x_pos, distributions)\nplt.ylabel('Value')\nplt.title('Moments Comparison')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 6: Probability calculations\nplt.subplot(2, 3, 6)\n# P(0.3 ≤ X ≤ 0.7) for uniform\np_uniform = continuous_cdf(uniform_pdf, 0.7, 0, 1) - continuous_cdf(uniform_pdf, 0.3, 0, 1)\n# P(X > 1) for exponential\np_exp = 1 - continuous_cdf(exp_pdf, 1, 0, np.inf)\n# P(0.5 ≤ X ≤ 1.5) for triangular\np_tri = continuous_cdf(triangular_pdf, 1.5, 0, 2) - continuous_cdf(triangular_pdf, 0.5, 0, 2)\n\nprobabilities = [p_uniform, p_exp, p_tri]\nplt.bar(distributions, probabilities, alpha=0.7, color=['blue', 'red', 'green'])\nplt.ylabel('Probability')\nplt.title('Example Probabilities')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate mathematical properties\nprint(f\"\\nMathematical Properties Verification:\")\nprint(f\"Uniform distribution:\")\nprint(f\"  E[X] = 0.5, Var(X) = 1/12 ≈ 0.0833\")\nprint(f\"  Calculated: E[X] = {uniform_mean:.4f}, Var(X) = {uniform_var:.4f}\")\n\nprint(f\"\\nExponential distribution:\")\nprint(f\"  E[X] = 1/λ = {1/lambda_val:.3f}, Var(X) = 1/λ² = {1/lambda_val**2:.3f}\")\nprint(f\"  Calculated: E[X] = {exp_mean:.4f}, Var(X) = {exp_var:.4f}\")\n\nprint(f\"\\nTriangular distribution:\")\nprint(f\"  Theoretical: E[X] = 1, Var(X) = 1/6 ≈ 0.1667\")\nprint(f\"  Calculated: E[X] = {tri_mean:.4f}, Var(X) = {tri_var:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Joint Random Variables\n\nWhen we have multiple random variables, we can study their joint behavior.\n\n**Joint PMF (Discrete):**\n$$p_{X,Y}(x,y) = P(X = x, Y = y)$$\n\n**Joint PDF (Continuous):**\n$$f_{X,Y}(x,y) = \\frac{\\partial^2}{\\partial x \\partial y} F_{X,Y}(x,y)$$\n\n**Marginal Distributions:**\n- Discrete: $p_X(x) = \\sum_y p_{X,Y}(x,y)$\n- Continuous: $f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) dy$\n\n**Independence:**\nX and Y are independent if:\n- Discrete: $p_{X,Y}(x,y) = p_X(x) \\cdot p_Y(y)$\n- Continuous: $f_{X,Y}(x,y) = f_X(x) \\cdot f_Y(y)$\n\n**Covariance:**\n$$\\text{Cov}(X,Y) = E[(X - \\mu_X)(Y - \\mu_Y)] = E[XY] - E[X]E[Y]$$\n\n**Correlation:**\n$$\\rho_{X,Y} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}$$\n\n**Properties of Covariance:**\n1. **Symmetry**: Cov(X,Y) = Cov(Y,X)\n2. **Linearity**: Cov(aX + b, cY + d) = ac Cov(X,Y)\n3. **Additivity**: Cov(X + Y, Z) = Cov(X,Z) + Cov(Y,Z)\n4. **Independence**: If X, Y independent, then Cov(X,Y) = 0\n\n**Properties of Correlation:**\n1. **Range**: -1 ≤ ρ ≤ 1\n2. **Linear Relationship**: |ρ| = 1 if and only if Y = aX + b\n3. **Independence**: If X, Y independent, then ρ = 0 (but not conversely)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def joint_pmf_discrete(x_values, y_values, joint_probs):\n    \"\"\"\n    Create joint PMF for discrete random variables\n    \n    Mathematical implementation:\n    p_{X,Y}(x,y) = P(X = x, Y = y)\n    \n    Parameters:\n    x_values: array-like, values of X\n    y_values: array-like, values of Y\n    joint_probs: 2D array, joint probabilities\n    \n    Returns:\n    dict: joint PMF as {(x, y): probability}\n    \"\"\"\n    if joint_probs.shape != (len(x_values), len(y_values)):\n        raise ValueError(\"Joint probabilities must match dimensions\")\n    \n    if abs(np.sum(joint_probs) - 1.0) > 1e-10:\n        raise ValueError(\"Joint probabilities must sum to 1\")\n    \n    joint_pmf = {}\n    for i, x in enumerate(x_values):\n        for j, y in enumerate(y_values):\n            joint_pmf[(x, y)] = joint_probs[i, j]\n    \n    return joint_pmf\n\ndef marginal_pmf_x(joint_pmf):\n    \"\"\"\n    Calculate marginal PMF of X\n    \n    Mathematical implementation:\n    p_X(x) = Σ_y p_{X,Y}(x,y)\n    \n    Parameters:\n    joint_pmf: dict, joint PMF\n    \n    Returns:\n    dict: marginal PMF of X\n    \"\"\"\n    marginal = {}\n    for (x, y), prob in joint_pmf.items():\n        if x in marginal:\n            marginal[x] += prob\n        else:\n            marginal[x] = prob\n    return marginal\n\ndef marginal_pmf_y(joint_pmf):\n    \"\"\"\n    Calculate marginal PMF of Y\n    \n    Mathematical implementation:\n    p_Y(y) = Σ_x p_{X,Y}(x,y)\n    \n    Parameters:\n    joint_pmf: dict, joint PMF\n    \n    Returns:\n    dict: marginal PMF of Y\n    \"\"\"\n    marginal = {}\n    for (x, y), prob in joint_pmf.items():\n        if y in marginal:\n            marginal[y] += prob\n        else:\n            marginal[y] = prob\n    return marginal\n\ndef check_independence_discrete(joint_pmf):\n    \"\"\"\n    Check if discrete random variables are independent\n    \n    Mathematical implementation:\n    Check if p_{X,Y}(x,y) = p_X(x) × p_Y(y) for all (x,y)\n    \n    Parameters:\n    joint_pmf: dict, joint PMF\n    \n    Returns:\n    bool: True if independent\n    \"\"\"\n    marginal_x = marginal_pmf_x(joint_pmf)\n    marginal_y = marginal_pmf_y(joint_pmf)\n    \n    for (x, y), joint_prob in joint_pmf.items():\n        expected_prob = marginal_x[x] * marginal_y[y]\n        if abs(joint_prob - expected_prob) > 1e-10:\n            return False\n    return True\n\ndef covariance_discrete(joint_pmf):\n    \"\"\"\n    Calculate covariance of discrete random variables\n    \n    Mathematical implementation:\n    Cov(X,Y) = E[XY] - E[X]E[Y]\n    \n    Parameters:\n    joint_pmf: dict, joint PMF\n    \n    Returns:\n    float: covariance\n    \"\"\"\n    # Calculate E[XY]\n    e_xy = sum(x * y * prob for (x, y), prob in joint_pmf.items())\n    \n    # Calculate E[X] and E[Y]\n    marginal_x = marginal_pmf_x(joint_pmf)\n    marginal_y = marginal_pmf_y(joint_pmf)\n    \n    e_x = sum(x * prob for x, prob in marginal_x.items())\n    e_y = sum(y * prob for y, prob in marginal_y.items())\n    \n    return e_xy - e_x * e_y\n\ndef correlation_discrete(joint_pmf):\n    \"\"\"\n    Calculate correlation coefficient of discrete random variables\n    \n    Mathematical implementation:\n    ρ = Cov(X,Y) / (σ_X × σ_Y)\n    \n    Parameters:\n    joint_pmf: dict, joint PMF\n    \n    Returns:\n    float: correlation coefficient\n    \"\"\"\n    # Calculate covariance\n    cov_xy = covariance_discrete(joint_pmf)\n    \n    # Calculate standard deviations\n    marginal_x = marginal_pmf_x(joint_pmf)\n    marginal_y = marginal_pmf_y(joint_pmf)\n    \n    e_x = sum(x * prob for x, prob in marginal_x.items())\n    e_y = sum(y * prob for y, prob in marginal_y.items())\n    \n    var_x = sum((x - e_x)**2 * prob for x, prob in marginal_x.items())\n    var_y = sum((y - e_y)**2 * prob for y, prob in marginal_y.items())\n    \n    std_x = np.sqrt(var_x)\n    std_y = np.sqrt(var_y)\n    \n    if std_x == 0 or std_y == 0:\n        return 0\n    \n    return cov_xy / (std_x * std_y)\n\n# Example: Two dice\nx_values = [1, 2, 3, 4, 5, 6]  # First die\ny_values = [1, 2, 3, 4, 5, 6]  # Second die\n\n# Independent case: fair dice\njoint_probs_indep = np.ones((6, 6)) / 36  # Each outcome has probability 1/36\njoint_pmf_indep = joint_pmf_discrete(x_values, y_values, joint_probs_indep)\n\nprint(\"Independent Dice Example:\")\nprint(f\"Independent: {check_independence_discrete(joint_pmf_indep)}\")\nprint(f\"Covariance: {covariance_discrete(joint_pmf_indep):.6f}\")\nprint(f\"Correlation: {correlation_discrete(joint_pmf_indep):.6f}\")\n\n# Dependent case: sum of dice\ndef create_sum_dice_pmf():\n    \"\"\"Create joint PMF where Y = X + 1 (mod 6)\"\"\"\n    joint_probs = np.zeros((6, 6))\n    for i in range(6):\n        j = (i + 1) % 6\n        joint_probs[i, j] = 1/6\n    return joint_pmf_discrete(x_values, y_values, joint_probs)\n\njoint_pmf_dep = create_sum_dice_pmf()\n\nprint(f\"\\nDependent Dice Example:\")\nprint(f\"Independent: {check_independence_discrete(joint_pmf_dep)}\")\nprint(f\"Covariance: {covariance_discrete(joint_pmf_dep):.6f}\")\nprint(f\"Correlation: {correlation_discrete(joint_pmf_dep):.6f}\")\n\n# Calculate marginal distributions\nmarginal_x_indep = marginal_pmf_x(joint_pmf_indep)\nmarginal_y_indep = marginal_pmf_y(joint_pmf_indep)\n\nprint(f\"\\nMarginal distributions (independent case):\")\nprint(f\"P(X=1) = {marginal_x_indep[1]:.3f}\")\nprint(f\"P(Y=1) = {marginal_y_indep[1]:.3f}\")\nprint(f\"P(X=1, Y=1) = {joint_pmf_indep[(1, 1)]:.3f}\")\nprint(f\"P(X=1) × P(Y=1) = {marginal_x_indep[1] * marginal_y_indep[1]:.3f}\")\n\n# Demonstrate covariance properties\nprint(f\"\\nCovariance Properties:\")\nprint(f\"1. Symmetry: Cov(X,Y) = Cov(Y,X)\")\nprint(f\"   Independent case: {covariance_discrete(joint_pmf_indep):.6f}\")\n\n# Create a new joint PMF with linear transformation\ndef create_transformed_pmf(joint_pmf, a, b, c, d):\n    \"\"\"Create joint PMF for aX + b and cY + d\"\"\"\n    new_joint_pmf = {}\n    for (x, y), prob in joint_pmf.items():\n        new_x = a * x + b\n        new_y = c * y + d\n        new_joint_pmf[(new_x, new_y)] = prob\n    return new_joint_pmf\n\n# Test linearity property: Cov(aX + b, cY + d) = ac Cov(X,Y)\na, b, c, d = 2, 1, 3, 2\ntransformed_pmf = create_transformed_pmf(joint_pmf_indep, a, b, c, d)\ncov_transformed = covariance_discrete(transformed_pmf)\ncov_original = covariance_discrete(joint_pmf_indep)\nexpected_cov = a * c * cov_original\n\nprint(f\"2. Linearity: Cov({a}X + {b}, {c}Y + {d}) = {a}×{c}×Cov(X,Y)\")\nprint(f\"   Expected: {expected_cov:.6f}\")\nprint(f\"   Calculated: {cov_transformed:.6f}\")\nprint(f\"   Property holds: {abs(cov_transformed - expected_cov) < 1e-10}\")\n\n# Visualize joint distributions\nplt.figure(figsize=(15, 10))\n\n# Plot 1: Independent joint PMF\nplt.subplot(2, 3, 1)\njoint_matrix_indep = np.array([[joint_pmf_indep[(x, y)] for y in y_values] for x in x_values])\nsns.heatmap(joint_matrix_indep, annot=True, fmt='.3f', cmap='Blues', \n            xticklabels=y_values, yticklabels=x_values)\nplt.title('Independent Joint PMF')\nplt.xlabel('Y')\nplt.ylabel('X')\n\n# Plot 2: Dependent joint PMF\nplt.subplot(2, 3, 2)\njoint_matrix_dep = np.array([[joint_pmf_dep[(x, y)] for y in y_values] for x in x_values])\nsns.heatmap(joint_matrix_dep, annot=True, fmt='.3f', cmap='Reds', \n            xticklabels=y_values, yticklabels=x_values)\nplt.title('Dependent Joint PMF')\nplt.xlabel('Y')\nplt.ylabel('X')\n\n# Plot 3: Marginal distributions comparison\nplt.subplot(2, 3, 3)\nx_pos = np.arange(len(x_values))\nmarginal_x_vals = [marginal_x_indep[x] for x in x_values]\nmarginal_y_vals = [marginal_y_indep[y] for y in y_values]\n\nplt.bar(x_pos - 0.2, marginal_x_vals, 0.4, label='P(X=x)', alpha=0.7)\nplt.bar(x_pos + 0.2, marginal_y_vals, 0.4, label='P(Y=y)', alpha=0.7)\nplt.xticks(x_pos, x_values)\nplt.ylabel('Probability')\nplt.title('Marginal Distributions')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 4: Independence verification\nplt.subplot(2, 3, 4)\n# Check independence for each (x,y) pair\nindependence_errors = []\nfor x in x_values:\n    for y in y_values:\n        joint_prob = joint_pmf_indep[(x, y)]\n        marginal_product = marginal_x_indep[x] * marginal_y_indep[y]\n        error = abs(joint_prob - marginal_product)\n        independence_errors.append(error)\n\nplt.hist(independence_errors, bins=10, alpha=0.7, color='green', edgecolor='black')\nplt.xlabel('|P(X,Y) - P(X)P(Y)|')\nplt.ylabel('Frequency')\nplt.title('Independence Verification')\nplt.grid(True, alpha=0.3)\n\n# Plot 5: Covariance vs correlation\nplt.subplot(2, 3, 5)\ncases = ['Independent', 'Dependent']\ncovariances = [covariance_discrete(joint_pmf_indep), covariance_discrete(joint_pmf_dep)]\ncorrelations = [correlation_discrete(joint_pmf_indep), correlation_discrete(joint_pmf_dep)]\n\nx_pos = np.arange(len(cases))\nplt.bar(x_pos - 0.2, covariances, 0.4, label='Covariance', alpha=0.7)\nplt.bar(x_pos + 0.2, correlations, 0.4, label='Correlation', alpha=0.7)\nplt.xticks(x_pos, cases)\nplt.ylabel('Value')\nplt.title('Covariance vs Correlation')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 6: Joint vs marginal relationship\nplt.subplot(2, 3, 6)\n# Show that sum of joint PMF equals marginal PMF\njoint_sums_x = [sum(joint_pmf_indep[(x, y)] for y in y_values) for x in x_values]\nmarginal_x_check = [marginal_x_indep[x] for x in x_values]\n\nplt.plot(x_values, joint_sums_x, 'bo-', label='Sum of joint PMF', linewidth=2)\nplt.plot(x_values, marginal_x_check, 'ro-', label='Marginal PMF', linewidth=2)\nplt.xlabel('X')\nplt.ylabel('Probability')\nplt.title('Joint → Marginal Relationship')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate mathematical relationships\nprint(f\"\\nMathematical Relationships:\")\nprint(f\"1. Marginal from Joint: Σ_y P(X=x, Y=y) = P(X=x)\")\nprint(f\"   Verification: {all(abs(joint_sums_x[i] - marginal_x_check[i]) < 1e-10 for i in range(len(x_values)))}\")\n\nprint(f\"\\n2. Independence: P(X=x, Y=y) = P(X=x) × P(Y=y)\")\nprint(f\"   Independent case: {check_independence_discrete(joint_pmf_indep)}\")\nprint(f\"   Dependent case: {check_independence_discrete(joint_pmf_dep)}\")\n\nprint(f\"\\n3. Correlation bounds: -1 ≤ ρ ≤ 1\")\nprint(f\"   Independent: ρ = {correlation_discrete(joint_pmf_indep):.6f}\")\nprint(f\"   Dependent: ρ = {correlation_discrete(joint_pmf_dep):.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Probability Distributions\n\nProbability distributions describe how probabilities are distributed over the possible values of a random variable. Understanding these distributions is crucial for statistical modeling, hypothesis testing, and machine learning.\n\n### Discrete Distributions\n\n#### Binomial Distribution\n\nThe **binomial distribution** models the number of successes in a fixed number of independent Bernoulli trials.\n\n**Mathematical Definition:**\n$$X \\sim \\text{Binomial}(n, p)$$\n\n**Probability Mass Function:**\n$$P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k = 0, 1, 2, \\ldots, n$$\n\nWhere $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ is the binomial coefficient.\n\n**Properties:**\n1. **Support**: {0, 1, 2, ..., n}\n2. **Parameters**: n (number of trials), p (probability of success)\n3. **Mean**: E[X] = np\n4. **Variance**: Var(X) = np(1-p)\n5. **Moment Generating Function**: M(t) = (pe^t + (1-p))^n\n\n**Derivation of Mean:**\n$$E[X] = \\sum_{k=0}^{n} k \\binom{n}{k} p^k (1-p)^{n-k}$$\n$$= \\sum_{k=1}^{n} k \\frac{n!}{k!(n-k)!} p^k (1-p)^{n-k}$$\n$$= np \\sum_{k=1}^{n} \\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} (1-p)^{n-k}$$\n$$= np \\sum_{j=0}^{n-1} \\binom{n-1}{j} p^j (1-p)^{n-1-j} = np$$\n\n**Applications:**\n- Quality control (defective items)\n- Medical trials (success/failure)\n- Survey responses (yes/no)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def binomial_pmf(k, n, p):\n    \"\"\"\n    Calculate binomial PMF\n    \n    Mathematical implementation:\n    P(X = k) = C(n,k) × p^k × (1-p)^(n-k)\n    \n    Parameters:\n    k: int, number of successes\n    n: int, number of trials\n    p: float, probability of success\n    \n    Returns:\n    float: P(X = k)\n    \"\"\"\n    if k < 0 or k > n:\n        return 0.0\n    \n    # Calculate binomial coefficient\n    from scipy.special import comb\n    binomial_coeff = comb(n, k)\n    \n    return binomial_coeff * (p**k) * ((1-p)**(n-k))\n\ndef binomial_cdf(k, n, p):\n    \"\"\"\n    Calculate binomial CDF\n    \n    Mathematical implementation:\n    F(k) = P(X ≤ k) = Σ_{i=0}^k C(n,i) × p^i × (1-p)^(n-i)\n    \n    Parameters:\n    k: int, number of successes\n    n: int, number of trials\n    p: float, probability of success\n    \n    Returns:\n    float: P(X ≤ k)\n    \"\"\"\n    return sum(binomial_pmf(i, n, p) for i in range(k + 1))\n\ndef binomial_moments(n, p):\n    \"\"\"\n    Calculate binomial moments\n    \n    Mathematical implementation:\n    E[X] = np\n    Var(X) = np(1-p)\n    \n    Parameters:\n    n: int, number of trials\n    p: float, probability of success\n    \n    Returns:\n    tuple: (mean, variance)\n    \"\"\"\n    mean = n * p\n    variance = n * p * (1 - p)\n    return mean, variance\n\n# Example: Coin flipping\nn_trials = 10\np_success = 0.5\n\nprint(\"Binomial Distribution - Coin Flipping:\")\nprint(f\"Parameters: n={n_trials}, p={p_success}\")\n\n# Calculate PMF for all possible values\npmf_values = {}\nfor k in range(n_trials + 1):\n    pmf_values[k] = binomial_pmf(k, n_trials, p_success)\n\nprint(f\"PMF: {pmf_values}\")\n\n# Calculate moments\nmean, variance = binomial_moments(n_trials, p_success)\nprint(f\"E[X] = {mean:.3f}\")\nprint(f\"Var(X) = {variance:.3f}\")\n\n# Calculate specific probabilities\nprint(f\"P(X = 5) = {binomial_pmf(5, n_trials, p_success):.4f}\")\nprint(f\"P(X ≤ 5) = {binomial_cdf(5, n_trials, p_success):.4f}\")\nprint(f\"P(X > 5) = {1 - binomial_cdf(5, n_trials, p_success):.4f}\")\n\n# Compare with theoretical values\ntheoretical_mean = n_trials * p_success\ntheoretical_var = n_trials * p_success * (1 - p_success)\nprint(f\"Theoretical E[X] = {theoretical_mean:.3f}\")\nprint(f\"Theoretical Var(X) = {theoretical_var:.3f}\")\n\n# Example: Quality control\ndefective_rate = 0.05  # 5% defective items\nsample_size = 20\n\nprint(f\"\\nQuality Control Example:\")\nprint(f\"Sample size: {sample_size}, Defective rate: {defective_rate:.3f}\")\n\n# Probability of finding exactly 2 defective items\nprob_exactly_2 = binomial_pmf(2, sample_size, defective_rate)\nprint(f\"P(exactly 2 defective) = {prob_exactly_2:.4f}\")\n\n# Probability of finding at most 2 defective items\nprob_at_most_2 = binomial_cdf(2, sample_size, defective_rate)\nprint(f\"P(at most 2 defective) = {prob_at_most_2:.4f}\")\n\n# Probability of finding more than 2 defective items\nprob_more_than_2 = 1 - binomial_cdf(2, sample_size, defective_rate)\nprint(f\"P(more than 2 defective) = {prob_more_than_2:.4f}\")\n\n# Expected number of defective items\nexpected_defective = sample_size * defective_rate\nprint(f\"Expected defective items: {expected_defective:.2f}\")\n\n# Visualize the distribution\nk_values = list(range(sample_size + 1))\nprobabilities = [binomial_pmf(k, sample_size, defective_rate) for k in k_values]\n\nplt.figure(figsize=(10, 6))\nplt.bar(k_values, probabilities, alpha=0.7, color='skyblue', edgecolor='navy')\nplt.axvline(expected_defective, color='red', linestyle='--', \n            label=f'Mean = {expected_defective:.2f}')\nplt.xlabel('Number of Defective Items')\nplt.ylabel('Probability')\nplt.title(f'Binomial Distribution (n={sample_size}, p={defective_rate})')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Poisson Distribution\n\nThe **Poisson distribution** models the number of events occurring in a fixed interval of time or space.\n\n**Mathematical Definition:**\n$$X \\sim \\text{Poisson}(\\lambda)$$\n\n**Probability Mass Function:**\n$$P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad k = 0, 1, 2, \\ldots$$\n\n**Properties:**\n1. **Support**: {0, 1, 2, ...}\n2. **Parameter**: λ (rate parameter, λ > 0)\n3. **Mean**: E[X] = λ\n4. **Variance**: Var(X) = λ\n5. **Moment Generating Function**: M(t) = e^{λ(e^t - 1)}\n\n**Derivation of Mean:**\n$$E[X] = \\sum_{k=0}^{\\infty} k \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n$$= \\lambda e^{-\\lambda} \\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!}$$\n$$= \\lambda e^{-\\lambda} \\sum_{j=0}^{\\infty} \\frac{\\lambda^j}{j!} = \\lambda e^{-\\lambda} e^{\\lambda} = \\lambda$$\n\n**Applications:**\n- Arrival times (customers, calls)\n- Rare events (accidents, mutations)\n- Radioactive decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def poisson_pmf(k, lambda_param):\n    \"\"\"\n    Calculate Poisson PMF\n    \n    Mathematical implementation:\n    P(X = k) = (λ^k × e^(-λ)) / k!\n    \n    Parameters:\n    k: int, number of events\n    lambda_param: float, rate parameter\n    \n    Returns:\n    float: P(X = k)\n    \"\"\"\n    if k < 0:\n        return 0.0\n    \n    from scipy.special import factorial\n    return (lambda_param**k * np.exp(-lambda_param)) / factorial(k)\n\ndef poisson_cdf(k, lambda_param):\n    \"\"\"\n    Calculate Poisson CDF\n    \n    Mathematical implementation:\n    F(k) = P(X ≤ k) = Σ_{i=0}^k (λ^i × e^(-λ)) / i!\n    \n    Parameters:\n    k: int, number of events\n    lambda_param: float, rate parameter\n    \n    Returns:\n    float: P(X ≤ k)\n    \"\"\"\n    return sum(poisson_pmf(i, lambda_param) for i in range(k + 1))\n\ndef poisson_moments(lambda_param):\n    \"\"\"\n    Calculate Poisson moments\n    \n    Mathematical implementation:\n    E[X] = λ\n    Var(X) = λ\n    \n    Parameters:\n    lambda_param: float, rate parameter\n    \n    Returns:\n    tuple: (mean, variance)\n    \"\"\"\n    return lambda_param, lambda_param\n\n# Example: Customer arrivals\narrival_rate = 3.0  # 3 customers per hour\n\nprint(\"Poisson Distribution - Customer Arrivals:\")\nprint(f\"Rate parameter: λ = {arrival_rate}\")\n\n# Calculate PMF for first 10 values\npmf_values = {}\nfor k in range(11):\n    pmf_values[k] = poisson_pmf(k, arrival_rate)\n\nprint(f\"PMF (first 11 values): {pmf_values}\")\n\n# Calculate moments\nmean, variance = poisson_moments(arrival_rate)\nprint(f\"E[X] = {mean:.3f}\")\nprint(f\"Var(X) = {variance:.3f}\")\n\n# Calculate specific probabilities\nprint(f\"P(X = 0) = {poisson_pmf(0, arrival_rate):.4f}\")\nprint(f\"P(X = 1) = {poisson_pmf(1, arrival_rate):.4f}\")\nprint(f\"P(X ≤ 2) = {poisson_cdf(2, arrival_rate):.4f}\")\nprint(f\"P(X > 5) = {1 - poisson_cdf(5, arrival_rate):.4f}\")\n\n# Example: Rare disease cases\ndisease_rate = 0.1  # 0.1 cases per 1000 people\n\nprint(f\"\\nRare Disease Example:\")\nprint(f\"Disease rate: λ = {disease_rate} per 1000 people\")\n\n# Probability of exactly 0 cases\nprob_no_cases = poisson_pmf(0, disease_rate)\nprint(f\"P(no cases) = {prob_no_cases:.4f}\")\n\n# Probability of at least 1 case\nprob_at_least_one = 1 - poisson_pmf(0, disease_rate)\nprint(f\"P(at least 1 case) = {prob_at_least_one:.4f}\")\n\n# Expected number of cases\nexpected_cases = disease_rate\nprint(f\"Expected cases: {expected_cases:.3f}\")\n\n# Visualize the distribution\nk_values = list(range(10))\nprobabilities = [poisson_pmf(k, arrival_rate) for k in k_values]\n\nplt.figure(figsize=(10, 6))\nplt.bar(k_values, probabilities, alpha=0.7, color='lightgreen', edgecolor='darkgreen')\nplt.axvline(arrival_rate, color='red', linestyle='--', \n            label=f'Mean = {arrival_rate:.1f}')\nplt.xlabel('Number of Events')\nplt.ylabel('Probability')\nplt.title(f'Poisson Distribution (λ = {arrival_rate})')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Continuous Distributions\n\n#### Normal (Gaussian) Distribution\n\nThe **normal distribution** is the most important continuous distribution in statistics.\n\n**Mathematical Definition:**\n$$X \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n\n**Probability Density Function:**\n$$f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n\n**Properties:**\n1. **Support**: (-∞, ∞)\n2. **Parameters**: μ (mean), σ² (variance)\n3. **Mean**: E[X] = μ\n4. **Variance**: Var(X) = σ²\n5. **Moment Generating Function**: M(t) = e^{μt + σ²t²/2}\n6. **Symmetry**: f(μ + x) = f(μ - x)\n7. **68-95-99.7 Rule**: P(μ-σ ≤ X ≤ μ+σ) ≈ 0.68, P(μ-2σ ≤ X ≤ μ+2σ) ≈ 0.95, P(μ-3σ ≤ X ≤ μ+3σ) ≈ 0.997\n\n**Standard Normal Distribution:**\n$$Z \\sim \\mathcal{N}(0, 1)$$\n$$f(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}}$$\n\n**Applications:**\n- Measurement errors\n- Natural phenomena\n- Central Limit Theorem\n- Statistical inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normal_pdf(x, mu, sigma):\n    \"\"\"\n    Calculate normal PDF\n    \n    Mathematical implementation:\n    f(x) = (1/(σ√(2π))) × e^(-((x-μ)²)/(2σ²))\n    \n    Parameters:\n    x: float or array, point(s) to evaluate\n    mu: float, mean\n    sigma: float, standard deviation\n    \n    Returns:\n    float or array: f(x)\n    \"\"\"\n    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma)**2)\n\ndef normal_cdf(x, mu, sigma):\n    \"\"\"\n    Calculate normal CDF\n    \n    Mathematical implementation:\n    F(x) = P(X ≤ x) = ∫_{-∞}^x f(t) dt\n    \n    Parameters:\n    x: float or array, point(s) to evaluate\n    mu: float, mean\n    sigma: float, standard deviation\n    \n    Returns:\n    float or array: F(x)\n    \"\"\"\n    from scipy.stats import norm\n    return norm.cdf(x, mu, sigma)\n\ndef normal_moments(mu, sigma):\n    \"\"\"\n    Calculate normal moments\n    \n    Mathematical implementation:\n    E[X] = μ\n    Var(X) = σ²\n    \n    Parameters:\n    mu: float, mean\n    sigma: float, standard deviation\n    \n    Returns:\n    tuple: (mean, variance)\n    \"\"\"\n    return mu, sigma**2\n\ndef standard_normal_pdf(z):\n    \"\"\"\n    Calculate standard normal PDF\n    \n    Mathematical implementation:\n    f(z) = (1/√(2π)) × e^(-z²/2)\n    \n    Parameters:\n    z: float or array, point(s) to evaluate\n    \n    Returns:\n    float or array: f(z)\n    \"\"\"\n    return normal_pdf(z, 0, 1)\n\ndef standard_normal_cdf(z):\n    \"\"\"\n    Calculate standard normal CDF\n    \n    Mathematical implementation:\n    Φ(z) = P(Z ≤ z) = ∫_{-∞}^z f(t) dt\n    \n    Parameters:\n    z: float or array, point(s) to evaluate\n    \n    Returns:\n    float or array: Φ(z)\n    \"\"\"\n    return normal_cdf(z, 0, 1)\n\n# Example: Height distribution\nmu_height = 170  # cm\nsigma_height = 10  # cm\n\nprint(\"Normal Distribution - Height:\")\nprint(f\"Parameters: μ = {mu_height}, σ = {sigma_height}\")\n\n# Calculate moments\nmean, variance = normal_moments(mu_height, sigma_height)\nprint(f\"E[X] = {mean:.3f}\")\nprint(f\"Var(X) = {variance:.3f}\")\n\n# Calculate specific probabilities using 68-95-99.7 rule\nprob_1sigma = normal_cdf(mu_height + sigma_height, mu_height, sigma_height) - \\\n              normal_cdf(mu_height - sigma_height, mu_height, sigma_height)\nprob_2sigma = normal_cdf(mu_height + 2*sigma_height, mu_height, sigma_height) - \\\n              normal_cdf(mu_height - 2*sigma_height, mu_height, sigma_height)\nprob_3sigma = normal_cdf(mu_height + 3*sigma_height, mu_height, sigma_height) - \\\n              normal_cdf(mu_height - 3*sigma_height, mu_height, sigma_height)\n\nprint(f\"P(μ-σ ≤ X ≤ μ+σ) = {prob_1sigma:.3f}\")\nprint(f\"P(μ-2σ ≤ X ≤ μ+2σ) = {prob_2sigma:.3f}\")\nprint(f\"P(μ-3σ ≤ X ≤ μ+3σ) = {prob_3sigma:.3f}\")\n\n# Calculate specific probabilities\nprint(f\"P(X ≤ 180) = {normal_cdf(180, mu_height, sigma_height):.4f}\")\nprint(f\"P(X > 160) = {1 - normal_cdf(160, mu_height, sigma_height):.4f}\")\nprint(f\"P(165 ≤ X ≤ 175) = {normal_cdf(175, mu_height, sigma_height) - normal_cdf(165, mu_height, sigma_height):.4f}\")\n\n# Standard normal distribution\nprint(f\"\\nStandard Normal Distribution:\")\nprint(f\"P(Z ≤ 1.96) = {standard_normal_cdf(1.96):.4f}\")\nprint(f\"P(Z > -1.96) = {1 - standard_normal_cdf(-1.96):.4f}\")\nprint(f\"P(-1.96 ≤ Z ≤ 1.96) = {standard_normal_cdf(1.96) - standard_normal_cdf(-1.96):.4f}\")\n\n# Visualize the distribution\nx = np.linspace(mu_height - 4*sigma_height, mu_height + 4*sigma_height, 1000)\ny = normal_pdf(x, mu_height, sigma_height)\n\nplt.figure(figsize=(12, 8))\n\n# Main plot\nplt.subplot(2, 1, 1)\nplt.plot(x, y, 'b-', linewidth=2, label='Normal PDF')\nplt.axvline(mu_height, color='red', linestyle='--', label=f'Mean = {mu_height}')\nplt.fill_between(x, y, where=(x >= mu_height - sigma_height) & (x <= mu_height + sigma_height), \n                 alpha=0.3, color='green', label='68% (1σ)')\nplt.fill_between(x, y, where=(x >= mu_height - 2*sigma_height) & (x <= mu_height + 2*sigma_height), \n                 alpha=0.2, color='yellow', label='95% (2σ)')\nplt.xlabel('Height (cm)')\nplt.ylabel('Probability Density')\nplt.title(f'Normal Distribution (μ = {mu_height}, σ = {sigma_height})')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Standard normal\nplt.subplot(2, 1, 2)\nz = np.linspace(-4, 4, 1000)\ny_std = standard_normal_pdf(z)\nplt.plot(z, y_std, 'g-', linewidth=2, label='Standard Normal PDF')\nplt.axvline(0, color='red', linestyle='--', label='Mean = 0')\nplt.fill_between(z, y_std, where=(z >= -1) & (z <= 1), \n                 alpha=0.3, color='green', label='68%')\nplt.fill_between(z, y_std, where=(z >= -2) & (z <= 2), \n                 alpha=0.2, color='yellow', label='95%')\nplt.xlabel('Z')\nplt.ylabel('Probability Density')\nplt.title('Standard Normal Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exponential Distribution\n\nThe **exponential distribution** models the time between events in a Poisson process.\n\n**Mathematical Definition:**\n$$X \\sim \\text{Exponential}(\\lambda)$$\n\n**Probability Density Function:**\n$$f(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0$$\n\n**Properties:**\n1. **Support**: [0, ∞)\n2. **Parameter**: λ (rate parameter, λ > 0)\n3. **Mean**: E[X] = 1/λ\n4. **Variance**: Var(X) = 1/λ²\n5. **Memoryless Property**: P(X > s + t | X > s) = P(X > t)\n\n**Derivation of Mean:**\n$$E[X] = \\int_0^{\\infty} x \\lambda e^{-\\lambda x} dx$$\n$$= \\lambda \\int_0^{\\infty} x e^{-\\lambda x} dx$$\n$$= \\lambda \\left[ -\\frac{x}{\\lambda} e^{-\\lambda x} \\right]_0^{\\infty} + \\lambda \\int_0^{\\infty} \\frac{1}{\\lambda} e^{-\\lambda x} dx$$\n$$= 0 + \\int_0^{\\infty} e^{-\\lambda x} dx = \\frac{1}{\\lambda}$$\n\n**Applications:**\n- Time between arrivals\n- Component lifetimes\n- Radioactive decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def exponential_pdf(x, lambda_param):\n    \"\"\"\n    Calculate exponential PDF\n    \n    Mathematical implementation:\n    f(x) = λ × e^(-λx) for x ≥ 0\n    \n    Parameters:\n    x: float or array, point(s) to evaluate\n    lambda_param: float, rate parameter\n    \n    Returns:\n    float or array: f(x)\n    \"\"\"\n    if isinstance(x, (int, float)):\n        return lambda_param * np.exp(-lambda_param * x) if x >= 0 else 0.0\n    else:\n        return np.where(x >= 0, lambda_param * np.exp(-lambda_param * x), 0.0)\n\ndef exponential_cdf(x, lambda_param):\n    \"\"\"\n    Calculate exponential CDF\n    \n    Mathematical implementation:\n    F(x) = P(X ≤ x) = 1 - e^(-λx) for x ≥ 0\n    \n    Parameters:\n    x: float or array, point(s) to evaluate\n    lambda_param: float, rate parameter\n    \n    Returns:\n    float or array: F(x)\n    \"\"\"\n    if isinstance(x, (int, float)):\n        return 1 - np.exp(-lambda_param * x) if x >= 0 else 0.0\n    else:\n        return np.where(x >= 0, 1 - np.exp(-lambda_param * x), 0.0)\n\ndef exponential_moments(lambda_param):\n    \"\"\"\n    Calculate exponential moments\n    \n    Mathematical implementation:\n    E[X] = 1/λ\n    Var(X) = 1/λ²\n    \n    Parameters:\n    lambda_param: float, rate parameter\n    \n    Returns:\n    tuple: (mean, variance)\n    \"\"\"\n    mean = 1 / lambda_param\n    variance = 1 / (lambda_param**2)\n    return mean, variance\n\ndef exponential_memoryless_property(lambda_param, s, t):\n    \"\"\"\n    Demonstrate memoryless property\n    \n    Mathematical implementation:\n    P(X > s + t | X > s) = P(X > t)\n    \n    Parameters:\n    lambda_param: float, rate parameter\n    s: float, time already waited\n    t: float, additional time\n    \n    Returns:\n    tuple: (conditional_prob, unconditional_prob)\n    \"\"\"\n    # P(X > s + t | X > s) = P(X > s + t) / P(X > s)\n    prob_s_plus_t = 1 - exponential_cdf(s + t, lambda_param)\n    prob_s = 1 - exponential_cdf(s, lambda_param)\n    conditional_prob = prob_s_plus_t / prob_s\n    \n    # P(X > t)\n    unconditional_prob = 1 - exponential_cdf(t, lambda_param)\n    \n    return conditional_prob, unconditional_prob\n\n# Example: Time between customer arrivals\narrival_rate = 2.0  # 2 customers per hour\n\nprint(\"Exponential Distribution - Customer Arrivals:\")\nprint(f\"Rate parameter: λ = {arrival_rate}\")\n\n# Calculate moments\nmean, variance = exponential_moments(arrival_rate)\nprint(f\"E[X] = {mean:.3f} hours\")\nprint(f\"Var(X) = {variance:.3f} hours²\")\nprint(f\"Standard deviation = {np.sqrt(variance):.3f} hours\")\n\n# Calculate specific probabilities\nprint(f\"P(X ≤ 0.5) = {exponential_cdf(0.5, arrival_rate):.4f}\")\nprint(f\"P(X > 1) = {1 - exponential_cdf(1, arrival_rate):.4f}\")\nprint(f\"P(0.5 ≤ X ≤ 1.5) = {exponential_cdf(1.5, arrival_rate) - exponential_cdf(0.5, arrival_rate):.4f}\")\n\n# Demonstrate memoryless property\ns = 0.5  # Already waited 0.5 hours\nt = 0.3  # Additional 0.3 hours\n\ncond_prob, uncond_prob = exponential_memoryless_property(arrival_rate, s, t)\nprint(f\"\\nMemoryless Property:\")\nprint(f\"P(X > {s+t} | X > {s}) = {cond_prob:.4f}\")\nprint(f\"P(X > {t}) = {uncond_prob:.4f}\")\nprint(f\"Memoryless property holds: {abs(cond_prob - uncond_prob) < 1e-10}\")\n\n# Example: Component lifetime\nfailure_rate = 0.1  # 0.1 failures per year\n\nprint(f\"\\nComponent Lifetime Example:\")\nprint(f\"Failure rate: λ = {failure_rate} per year\")\n\n# Expected lifetime\nexpected_lifetime = 1 / failure_rate\nprint(f\"Expected lifetime: {expected_lifetime:.1f} years\")\n\n# Probability of surviving more than 5 years\nprob_survive_5 = 1 - exponential_cdf(5, failure_rate)\nprint(f\"P(survive > 5 years) = {prob_survive_5:.4f}\")\n\n# Probability of failing within first year\nprob_fail_1 = exponential_cdf(1, failure_rate)\nprint(f\"P(fail within 1 year) = {prob_fail_1:.4f}\")\n\n# Visualize the distribution\nx = np.linspace(0, 5, 1000)\ny = exponential_pdf(x, arrival_rate)\n\nplt.figure(figsize=(12, 8))\n\n# PDF\nplt.subplot(2, 1, 1)\nplt.plot(x, y, 'b-', linewidth=2, label='Exponential PDF')\nplt.axvline(mean, color='red', linestyle='--', label=f'Mean = {mean:.3f}')\nplt.fill_between(x, y, where=(x >= 0) & (x <= 1), \n                 alpha=0.3, color='green', label='P(X ≤ 1)')\nplt.xlabel('Time (hours)')\nplt.ylabel('Probability Density')\nplt.title(f'Exponential Distribution (λ = {arrival_rate})')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# CDF\nplt.subplot(2, 1, 2)\ny_cdf = exponential_cdf(x, arrival_rate)\nplt.plot(x, y_cdf, 'g-', linewidth=2, label='Exponential CDF')\nplt.axhline(0.632, color='red', linestyle='--', label='1 - 1/e ≈ 0.632')\nplt.axvline(mean, color='red', linestyle=':', alpha=0.7)\nplt.xlabel('Time (hours)')\nplt.ylabel('Cumulative Probability')\nplt.title('Exponential CDF')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Joint and Conditional Probability\n\n### Joint Probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def joint_probability_example():\n    \"\"\"Example: Joint probability of two dice\"\"\"\n    # Create joint probability table for two dice\n    dice1 = np.arange(1, 7)\n    dice2 = np.arange(1, 7)\n    \n    # Joint probability table\n    joint_prob = np.zeros((6, 6))\n    for i in range(6):\n        for j in range(6):\n            joint_prob[i, j] = 1/36  # Each outcome has equal probability\n    \n    # Marginal probabilities\n    marginal_dice1 = np.sum(joint_prob, axis=1)  # Sum over dice2\n    marginal_dice2 = np.sum(joint_prob, axis=0)  # Sum over dice1\n    \n    return joint_prob, marginal_dice1, marginal_dice2\n\njoint_prob, marg1, marg2 = joint_probability_example()\n\nprint(\"Joint Probability Table (Two Dice)\")\nprint(\"Dice1\\\\Dice2 | 1    2    3    4    5    6\")\nprint(\"-\" * 40)\nfor i in range(6):\n    row = f\"   {i+1}     |\"\n    for j in range(6):\n        row += f\" {joint_prob[i,j]:.3f}\"\n    print(row)\n\nprint(f\"\\nMarginal Probabilities:\")\nprint(f\"Dice1: {marg1}\")\nprint(f\"Dice2: {marg2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conditional Probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conditional_probability_example():\n    \"\"\"Example: Conditional probability with cards\"\"\"\n    # Standard deck of 52 cards\n    # Event A: Drawing a heart\n    # Event B: Drawing a face card (J, Q, K)\n    \n    # Total cards\n    total_cards = 52\n    \n    # Hearts\n    hearts = 13\n    # Face cards\n    face_cards = 12\n    # Face cards that are hearts\n    heart_face_cards = 3  # J♥, Q♥, K♥\n    \n    # Probabilities\n    p_hearts = hearts / total_cards\n    p_face_cards = face_cards / total_cards\n    p_hearts_and_face = heart_face_cards / total_cards\n    \n    # Conditional probabilities\n    p_hearts_given_face = p_hearts_and_face / p_face_cards\n    p_face_given_hearts = p_hearts_and_face / p_hearts\n    \n    return {\n        'P(Hearts)': p_hearts,\n        'P(Face Cards)': p_face_cards,\n        'P(Hearts ∩ Face Cards)': p_hearts_and_face,\n        'P(Hearts|Face Cards)': p_hearts_given_face,\n        'P(Face Cards|Hearts)': p_face_given_hearts\n    }\n\ncond_probs = conditional_probability_example()\nprint(\"Conditional Probability Example (Cards)\")\nfor event, prob in cond_probs.items():\n    print(f\"{event}: {prob:.3f}\")\n\n# Independence check\np_hearts = cond_probs['P(Hearts)']\np_face = cond_probs['P(Face Cards)']\np_joint = cond_probs['P(Hearts ∩ Face Cards)']\nprint(f\"\\nIndependence Check:\")\nprint(f\"P(Hearts) × P(Face Cards) = {p_hearts * p_face:.3f}\")\nprint(f\"P(Hearts ∩ Face Cards) = {p_joint:.3f}\")\nprint(f\"Independent: {abs(p_hearts * p_face - p_joint) < 1e-10}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayes' Theorem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bayes_theorem_example():\n    \"\"\"Example: Medical diagnosis with Bayes' theorem\"\"\"\n    # Disease prevalence: 1% of population has the disease\n    p_disease = 0.01\n    \n    # Test accuracy\n    p_positive_given_disease = 0.95    # Sensitivity\n    p_negative_given_no_disease = 0.90  # Specificity\n    \n    # Calculate other probabilities\n    p_no_disease = 1 - p_disease\n    p_positive_given_no_disease = 1 - p_negative_given_no_disease\n    \n    # Prior probability\n    prior = p_disease\n    \n    # Likelihood\n    likelihood = p_positive_given_disease\n    \n    # Evidence (total probability of positive test)\n    evidence = (p_positive_given_disease * p_disease + \n               p_positive_given_no_disease * p_no_disease)\n    \n    # Posterior probability using Bayes' theorem\n    posterior = (likelihood * prior) / evidence\n    \n    return {\n        'Prior P(Disease)': prior,\n        'Likelihood P(Positive|Disease)': likelihood,\n        'Evidence P(Positive)': evidence,\n        'Posterior P(Disease|Positive)': posterior\n    }\n\nbayes_result = bayes_theorem_example()\nprint(\"Bayes' Theorem Example (Medical Diagnosis)\")\nfor term, value in bayes_result.items():\n    print(f\"{term}: {value:.3f}\")\n\n# Demonstrate with different prior probabilities\ndef bayes_with_different_priors():\n    \"\"\"Show how posterior changes with different priors\"\"\"\n    priors = [0.001, 0.01, 0.1, 0.5]\n    sensitivity = 0.95\n    specificity = 0.90\n    \n    results = []\n    for prior in priors:\n        likelihood = sensitivity\n        evidence = (sensitivity * prior + (1 - specificity) * (1 - prior))\n        posterior = (likelihood * prior) / evidence\n        results.append((prior, posterior))\n    \n    return results\n\nbayes_results = bayes_with_different_priors()\nprint(f\"\\nPosterior Probabilities with Different Priors:\")\nfor prior, posterior in bayes_results:\n    print(f\"Prior: {prior:.3f} → Posterior: {posterior:.3f}\")\n\n# Visualize Bayes' theorem\nplt.figure(figsize=(10, 6))\npriors = np.linspace(0.001, 0.5, 100)\nposteriors = []\nsensitivity = 0.95\nspecificity = 0.90\n\nfor prior in priors:\n    likelihood = sensitivity\n    evidence = (sensitivity * prior + (1 - specificity) * (1 - prior))\n    posterior = (likelihood * prior) / evidence\n    posteriors.append(posterior)\n\nplt.plot(priors, posteriors, 'b-', linewidth=2, label='Posterior')\nplt.plot(priors, priors, 'r--', linewidth=2, label='Prior')\nplt.xlabel('Prior Probability')\nplt.ylabel('Posterior Probability')\nplt.title(\"Bayes' Theorem: How Prior Affects Posterior\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Central Limit Theorem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def central_limit_theorem_demonstration():\n    \"\"\"Demonstrate the Central Limit Theorem\"\"\"\n    # Start with a non-normal distribution (exponential)\n    population = np.random.exponential(2, 100000)\n    \n    # Take samples of different sizes\n    sample_sizes = [1, 5, 10, 30, 100]\n    sample_means = []\n    \n    for n in sample_sizes:\n        means = []\n        for _ in range(1000):\n            sample = np.random.choice(population, size=n, replace=False)\n            means.append(np.mean(sample))\n        sample_means.append(means)\n    \n    return population, sample_means, sample_sizes\n\npopulation, sample_means, sizes = central_limit_theorem_demonstration()\n\nprint(\"Central Limit Theorem Demonstration\")\nprint(f\"Population mean: {np.mean(population):.3f}\")\nprint(f\"Population std: {np.std(population):.3f}\")\n\nplt.figure(figsize=(15, 10))\n\n# Population distribution\nplt.subplot(2, 3, 1)\nplt.hist(population, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black')\nplt.title('Population Distribution (Exponential)')\nplt.xlabel('Value')\nplt.ylabel('Density')\n\n# Sample means distributions\nfor i, (means, size) in enumerate(zip(sample_means, sizes)):\n    plt.subplot(2, 3, i+2)\n    plt.hist(means, bins=30, density=True, alpha=0.7, color='lightgreen', edgecolor='black')\n    \n    # Overlay normal distribution\n    mu = np.mean(means)\n    sigma = np.std(means)\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n    plt.plot(x, y, 'r-', linewidth=2, label='Normal approx.')\n    \n    plt.title(f'Sample Means (n={size})')\n    plt.xlabel('Sample Mean')\n    plt.ylabel('Density')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Verify CLT predictions\nprint(f\"\\nCLT Verification:\")\nfor means, size in zip(sample_means, sizes):\n    expected_std = np.std(population) / np.sqrt(size)\n    actual_std = np.std(means)\n    print(f\"n={size}: Expected std = {expected_std:.3f}, Actual std = {actual_std:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Applications\n\n### Monte Carlo Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def monte_carlo_pi_estimation(n_points=10000):\n    \"\"\"Estimate π using Monte Carlo simulation\"\"\"\n    # Generate random points in a 2x2 square\n    x = np.random.uniform(-1, 1, n_points)\n    y = np.random.uniform(-1, 1, n_points)\n    \n    # Calculate distance from origin\n    distances = np.sqrt(x**2 + y**2)\n    \n    # Points inside unit circle\n    inside_circle = np.sum(distances <= 1)\n    \n    # Estimate π\n    pi_estimate = 4 * inside_circle / n_points\n    \n    return x, y, distances, pi_estimate\n\nx, y, distances, pi_est = monte_carlo_pi_estimation(100000)\nprint(f\"Monte Carlo π Estimation: {pi_est:.6f}\")\nprint(f\"Actual π: {np.pi:.6f}\")\nprint(f\"Error: {abs(pi_est - np.pi):.6f}\")\n\n# Visualize\nplt.figure(figsize=(8, 8))\ncircle = plt.Circle((0, 0), 1, fill=False, color='red', linewidth=2)\nplt.gca().add_patch(circle)\n\n# Plot points\ninside = distances <= 1\noutside = distances > 1\n\nplt.scatter(x[inside], y[inside], c='blue', alpha=0.6, s=1, label='Inside')\nplt.scatter(x[outside], y[outside], c='gray', alpha=0.6, s=1, label='Outside')\n\nplt.xlim(-1.1, 1.1)\nplt.ylim(-1.1, 1.1)\nplt.title(f'Monte Carlo π Estimation (π ≈ {pi_est:.4f})')\nplt.legend()\nplt.axis('equal')\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confidence Intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def confidence_interval_example():\n    \"\"\"Demonstrate confidence intervals\"\"\"\n    # Generate population\n    population = np.random.normal(100, 15, 10000)\n    true_mean = np.mean(population)\n    \n    # Take multiple samples\n    n_samples = 100\n    sample_size = 30\n    confidence_level = 0.95\n    \n    sample_means = []\n    confidence_intervals = []\n    \n    for _ in range(n_samples):\n        sample = np.random.choice(population, size=sample_size, replace=False)\n        sample_mean = np.mean(sample)\n        sample_std = np.std(sample, ddof=1)\n        \n        # Calculate confidence interval\n        t_value = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1)\n        margin_of_error = t_value * sample_std / np.sqrt(sample_size)\n        \n        sample_means.append(sample_mean)\n        confidence_intervals.append((sample_mean - margin_of_error, sample_mean + margin_of_error))\n    \n    # Count intervals containing true mean\n    intervals_containing_true = sum(1 for ci in confidence_intervals \n                                  if ci[0] <= true_mean <= ci[1])\n    \n    return sample_means, confidence_intervals, true_mean, intervals_containing_true\n\nmeans, intervals, true_mean, count = confidence_interval_example()\n\nprint(f\"Confidence Interval Example\")\nprint(f\"True population mean: {true_mean:.2f}\")\nprint(f\"Intervals containing true mean: {count}/{len(intervals)} ({count/len(intervals)*100:.1f}%)\")\n\n# Visualize confidence intervals\nplt.figure(figsize=(12, 8))\nx_positions = np.arange(len(intervals))\n\n# Plot confidence intervals\nfor i, (lower, upper) in enumerate(intervals):\n    if lower <= true_mean <= upper:\n        plt.plot([i, i], [lower, upper], 'b-', alpha=0.7)\n    else:\n        plt.plot([i, i], [lower, upper], 'r-', alpha=0.7)\n\nplt.axhline(y=true_mean, color='g', linestyle='--', linewidth=2, label='True Mean')\nplt.xlabel('Sample Number')\nplt.ylabel('Value')\nplt.title('Confidence Intervals (95%)')\nplt.legend()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Problems\n\n1. **Probability Calculations**: Create a function that calculates probabilities for various scenarios (e.g., card games, dice games).\n\n2. **Distribution Fitting**: Implement a function that fits different distributions to data and selects the best fit using goodness-of-fit tests.\n\n3. **Bayesian Inference**: Build a simple Bayesian classifier and compare it with frequentist approaches.\n\n4. **Monte Carlo Methods**: Use Monte Carlo simulation to solve complex probability problems.\n\n## Further Reading\n\n- \"Probability and Statistics for Engineering and the Sciences\" by Jay L. Devore\n- \"Introduction to Probability\" by Joseph K. Blitzstein and Jessica Hwang\n- \"Statistical Inference\" by George Casella and Roger L. Berger\n- \"Bayesian Data Analysis\" by Andrew Gelman et al.\n\n## Key Takeaways\n\n- **Probability** provides the foundation for statistical inference and machine learning\n- **Random variables** can be discrete or continuous, each with their own properties\n- **Probability distributions** model uncertainty in data and are essential for statistical modeling\n- **Joint and conditional probabilities** help understand relationships between events\n- **Bayes' theorem** is fundamental for updating beliefs with new evidence\n- **Central Limit Theorem** explains why normal distributions are so common\n- **Monte Carlo methods** provide powerful tools for solving complex probability problems\n\nIn the next chapter, we'll explore statistical inference, including hypothesis testing and confidence intervals."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}