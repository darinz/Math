{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Time Series Analysis\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Pandas](https://img.shields.io/badge/Pandas-1.3+-blue.svg)](https://pandas.pydata.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.4+-orange.svg)](https://matplotlib.org/)\n[![Seaborn](https://img.shields.io/badge/Seaborn-0.11+-blue.svg)](https://seaborn.pydata.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-green.svg)](https://scipy.org/)\n[![Statsmodels](https://img.shields.io/badge/Statsmodels-0.13+-blue.svg)](https://www.statsmodels.org/)\n[![Prophet](https://img.shields.io/badge/Prophet-1.1+-blue.svg)](https://facebook.github.io/prophet/)\n\nTime series analysis deals with data points collected over time. This chapter covers trend analysis, seasonality, forecasting models, and their applications in AI/ML.\n\n## Table of Contents\n- [Time Series Components](#time-series-components)\n- [Stationarity](#stationarity)\n- [Autocorrelation](#autocorrelation)\n- [Moving Averages](#moving-averages)\n- [ARIMA Models](#arima-models)\n- [Seasonal Decomposition](#seasonal-decomposition)\n- [Forecasting](#forecasting)\n- [Practical Applications](#practical-applications)\n\n## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\nnp.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time Series Components\n\nTime series data can be decomposed into several fundamental components that help us understand the underlying patterns and structure.\n\n### Mathematical Decomposition\n\nThe **classical decomposition** model represents a time series as:\n\n$$Y_t = T_t + S_t + C_t + R_t$$\n\nWhere:\n- $Y_t$ = Observed value at time t\n- $T_t$ = Trend component (long-term movement)\n- $S_t$ = Seasonal component (periodic patterns)\n- $C_t$ = Cyclical component (irregular cycles)\n- $R_t$ = Random/Residual component (unexplained variation)\n\n**Additive Model:**\n$$Y_t = T_t + S_t + C_t + R_t$$\n\n**Multiplicative Model:**\n$$Y_t = T_t \\times S_t \\times C_t \\times R_t$$\n\n**Log-Additive Model:**\n$$\\log(Y_t) = \\log(T_t) + \\log(S_t) + \\log(C_t) + \\log(R_t)$$\n\n### Trend Component (T_t)\n\nThe trend represents the long-term systematic change in the series.\n\n**Mathematical Properties:**\n1. **Monotonicity**: Trend should be smooth and systematic\n2. **Persistence**: Changes should be gradual, not abrupt\n3. **Global Nature**: Trend affects the entire series\n\n**Common Trend Models:**\n\n**Linear Trend:**\n$$T_t = \\beta_0 + \\beta_1 t + \\epsilon_t$$\n\n**Quadratic Trend:**\n$$T_t = \\beta_0 + \\beta_1 t + \\beta_2 t^2 + \\epsilon_t$$\n\n**Exponential Trend:**\n$$T_t = \\beta_0 e^{\\beta_1 t} + \\epsilon_t$$\n\n**Logistic Trend:**\n$$T_t = \\frac{L}{1 + e^{-k(t-t_0)}} + \\epsilon_t$$\n\nWhere:\n- $L$ = maximum level (carrying capacity)\n- $k$ = growth rate\n- $t_0$ = inflection point\n\n**Trend Estimation Methods:**\n\n**1. Moving Average:**\n$$\\hat{T}_t = \\frac{1}{2k+1} \\sum_{i=-k}^{k} Y_{t+i}$$\n\n**2. Exponential Smoothing:**\n$$\\hat{T}_t = \\alpha Y_t + (1-\\alpha) \\hat{T}_{t-1}$$\n\n**3. Linear Regression:**\n$$\\hat{T}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 t$$\n\n**4. Polynomial Regression:**\n$$\\hat{T}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 t + \\hat{\\beta}_2 t^2 + \\cdots + \\hat{\\beta}_p t^p$$\n\n### Seasonal Component (S_t)\n\nSeasonality represents regular, periodic patterns that repeat at fixed intervals.\n\n**Mathematical Properties:**\n1. **Periodicity**: $S_t = S_{t+s}$ where s is the seasonal period\n2. **Zero Sum**: $\\sum_{i=1}^{s} S_i = 0$ (additive model)\n3. **Product Unity**: $\\prod_{i=1}^{s} S_i = 1$ (multiplicative model)\n\n**Seasonal Models:**\n\n**Deterministic Seasonal:**\n$$S_t = \\sum_{j=1}^{s} \\alpha_j D_{j,t}$$\n\nWhere $D_{j,t}$ are seasonal dummy variables.\n\n**Harmonic Seasonal:**\n$$S_t = \\sum_{j=1}^{k} [A_j \\cos(2\\pi j t/s) + B_j \\sin(2\\pi j t/s)]$$\n\n**Seasonal Estimation:**\n\n**1. Seasonal Subseries Method:**\n$$\\hat{S}_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} (Y_{i,j} - \\bar{Y})$$\n\n**2. Seasonal Moving Average:**\n$$\\hat{S}_t = \\frac{1}{s} \\sum_{i=0}^{s-1} Y_{t-i} - \\hat{T}_t$$\n\n### Cyclical Component (C_t)\n\nCycles represent irregular, non-seasonal patterns that occur over longer periods.\n\n**Mathematical Properties:**\n1. **Non-periodic**: Cycles don't repeat at fixed intervals\n2. **Variable Amplitude**: Cycle strength can vary over time\n3. **Economic Nature**: Often related to business cycles\n\n**Cyclical Models:**\n\n**ARMA Process:**\n$$C_t = \\phi_1 C_{t-1} + \\phi_2 C_{t-2} + \\cdots + \\phi_p C_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\cdots + \\theta_q \\epsilon_{t-q}$$\n\n**Spectral Analysis:**\n$$C_t = \\int_{-\\pi}^{\\pi} e^{i\\omega t} dZ(\\omega)$$\n\nWhere $dZ(\\omega)$ is the spectral measure.\n\n### Random Component (R_t)\n\nThe residual component captures unexplained variation.\n\n**Mathematical Properties:**\n1. **Zero Mean**: $E[R_t] = 0$\n2. **Constant Variance**: $\\text{Var}(R_t) = \\sigma^2$\n3. **Uncorrelated**: $\\text{Cov}(R_t, R_{t-k}) = 0$ for $k \\neq 0$\n\n**Residual Analysis:**\n$$R_t = Y_t - \\hat{T}_t - \\hat{S}_t - \\hat{C}_t$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.signal import detrend\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nimport seaborn as sns\n\ndef generate_trend_component(n, trend_type='linear', **params):\n    \"\"\"\n    Generate trend component with different functional forms\n    \n    Mathematical implementation:\n    Linear: T_t = β₀ + β₁t\n    Quadratic: T_t = β₀ + β₁t + β₂t²\n    Exponential: T_t = β₀e^(β₁t)\n    Logistic: T_t = L/(1 + e^(-k(t-t₀)))\n    \n    Parameters:\n    n: int, number of observations\n    trend_type: str, type of trend\n    **params: trend parameters\n    \n    Returns:\n    array: trend component\n    \"\"\"\n    t = np.arange(n)\n    \n    if trend_type == 'linear':\n        beta0 = params.get('beta0', 0)\n        beta1 = params.get('beta1', 0.1)\n        return beta0 + beta1 * t\n    \n    elif trend_type == 'quadratic':\n        beta0 = params.get('beta0', 0)\n        beta1 = params.get('beta1', 0.1)\n        beta2 = params.get('beta2', 0.001)\n        return beta0 + beta1 * t + beta2 * t**2\n    \n    elif trend_type == 'exponential':\n        beta0 = params.get('beta0', 1)\n        beta1 = params.get('beta1', 0.01)\n        return beta0 * np.exp(beta1 * t)\n    \n    elif trend_type == 'logistic':\n        L = params.get('L', 100)\n        k = params.get('k', 0.1)\n        t0 = params.get('t0', n/2)\n        return L / (1 + np.exp(-k * (t - t0)))\n    \n    else:\n        raise ValueError(f\"Unknown trend type: {trend_type}\")\n\ndef generate_seasonal_component(n, period, seasonal_type='harmonic', **params):\n    \"\"\"\n    Generate seasonal component with different patterns\n    \n    Mathematical implementation:\n    Harmonic: S_t = Σ[A_j cos(2πjt/s) + B_j sin(2πjt/s)]\n    Deterministic: S_t = Σ α_j D_{j,t}\n    \n    Parameters:\n    n: int, number of observations\n    period: int, seasonal period\n    seasonal_type: str, type of seasonality\n    **params: seasonal parameters\n    \n    Returns:\n    array: seasonal component\n    \"\"\"\n    t = np.arange(n)\n    \n    if seasonal_type == 'harmonic':\n        k = params.get('k', 2)  # number of harmonics\n        seasonal = np.zeros(n)\n        \n        for j in range(1, k + 1):\n            A_j = params.get(f'A_{j}', np.random.normal(0, 1))\n            B_j = params.get(f'B_{j}', np.random.normal(0, 1))\n            seasonal += A_j * np.cos(2 * np.pi * j * t / period) + \\\n                       B_j * np.sin(2 * np.pi * j * t / period)\n        \n        return seasonal\n    \n    elif seasonal_type == 'deterministic':\n        # Create seasonal pattern with fixed amplitudes\n        amplitudes = params.get('amplitudes', [2, -1, 1, -2, 0.5, -0.5])\n        seasonal = np.zeros(n)\n        \n        for i in range(n):\n            season_idx = i % period\n            if season_idx < len(amplitudes):\n                seasonal[i] = amplitudes[season_idx]\n        \n        return seasonal\n    \n    else:\n        raise ValueError(f\"Unknown seasonal type: {seasonal_type}\")\n\ndef generate_cyclical_component(n, cycle_period=50, amplitude=1, **params):\n    \"\"\"\n    Generate cyclical component using ARMA-like process\n    \n    Mathematical implementation:\n    C_t = φ₁C_{t-1} + φ₂C_{t-2} + ... + ε_t\n    \n    Parameters:\n    n: int, number of observations\n    cycle_period: int, approximate cycle length\n    amplitude: float, cycle amplitude\n    **params: cycle parameters\n    \n    Returns:\n    array: cyclical component\n    \"\"\"\n    # Generate AR(2) process to create cycles\n    phi1 = params.get('phi1', 1.5)\n    phi2 = params.get('phi2', -0.7)\n    sigma = params.get('sigma', 0.1)\n    \n    cyclical = np.zeros(n)\n    epsilon = np.random.normal(0, sigma, n)\n    \n    # Initialize with random values\n    cyclical[0] = np.random.normal(0, 1)\n    cyclical[1] = np.random.normal(0, 1)\n    \n    # Generate AR(2) process\n    for t in range(2, n):\n        cyclical[t] = phi1 * cyclical[t-1] + phi2 * cyclical[t-2] + epsilon[t]\n    \n    # Scale to desired amplitude\n    cyclical = amplitude * cyclical / np.std(cyclical)\n    \n    return cyclical\n\ndef generate_random_component(n, distribution='normal', **params):\n    \"\"\"\n    Generate random component with different distributions\n    \n    Mathematical implementation:\n    Normal: R_t ~ N(0, σ²)\n    Student-t: R_t ~ t(ν)\n    GARCH-like: R_t ~ N(0, σ_t²) where σ_t² varies\n    \n    Parameters:\n    n: int, number of observations\n    distribution: str, distribution type\n    **params: distribution parameters\n    \n    Returns:\n    array: random component\n    \"\"\"\n    if distribution == 'normal':\n        sigma = params.get('sigma', 1.0)\n        return np.random.normal(0, sigma, n)\n    \n    elif distribution == 'student_t':\n        df = params.get('df', 5)\n        return np.random.standard_t(df, n)\n    \n    elif distribution == 'garch_like':\n        # Simple GARCH-like process with time-varying volatility\n        sigma = params.get('sigma', 1.0)\n        alpha = params.get('alpha', 0.1)\n        beta = params.get('beta', 0.8)\n        \n        volatility = np.zeros(n)\n        returns = np.zeros(n)\n        \n        volatility[0] = sigma\n        returns[0] = np.random.normal(0, volatility[0])\n        \n        for t in range(1, n):\n            volatility[t] = np.sqrt(alpha + beta * volatility[t-1]**2)\n            returns[t] = np.random.normal(0, volatility[t])\n        \n        return returns\n    \n    else:\n        raise ValueError(f\"Unknown distribution: {distribution}\")\n\ndef decompose_time_series(y, period=None, model='additive'):\n    \"\"\"\n    Decompose time series into components\n    \n    Mathematical implementation:\n    Additive: Y_t = T_t + S_t + C_t + R_t\n    Multiplicative: Y_t = T_t × S_t × C_t × R_t\n    \n    Parameters:\n    y: array, time series data\n    period: int, seasonal period\n    model: str, decomposition model\n    \n    Returns:\n    dict: decomposed components\n    \"\"\"\n    n = len(y)\n    \n    # Estimate trend using moving average\n    if period and period > 1:\n        # Use seasonal moving average for trend\n        trend = np.convolve(y, np.ones(period)/period, mode='same')\n    else:\n        # Use simple moving average\n        trend = np.convolve(y, np.ones(5)/5, mode='same')\n    \n    # Remove trend to get detrended series\n    detrended = y - trend\n    \n    # Estimate seasonal component\n    if period and period > 1:\n        seasonal = np.zeros(n)\n        for i in range(period):\n            indices = np.arange(i, n, period)\n            seasonal[indices] = np.mean(detrended[indices])\n        \n        # Center seasonal component\n        seasonal = seasonal - np.mean(seasonal)\n    else:\n        seasonal = np.zeros(n)\n    \n    # Remove seasonal to get seasonal-adjusted series\n    seasonal_adjusted = detrended - seasonal\n    \n    # Estimate cyclical component (simplified)\n    # Use moving average of seasonal-adjusted series\n    cyclical = np.convolve(seasonal_adjusted, np.ones(7)/7, mode='same')\n    \n    # Residual component\n    residual = seasonal_adjusted - cyclical\n    \n    return {\n        'trend': trend,\n        'seasonal': seasonal,\n        'cyclical': cyclical,\n        'residual': residual,\n        'detrended': detrended,\n        'seasonal_adjusted': seasonal_adjusted\n    }\n\ndef analyze_trend_properties(trend, t):\n    \"\"\"\n    Analyze mathematical properties of trend component\n    \n    Parameters:\n    trend: array, trend component\n    t: array, time index\n    \n    Returns:\n    dict: trend analysis results\n    \"\"\"\n    # Calculate trend characteristics\n    trend_slope = np.polyfit(t, trend, 1)[0]\n    trend_curvature = np.polyfit(t, trend, 2)[2]\n    \n    # Calculate trend persistence\n    trend_diff = np.diff(trend)\n    trend_persistence = np.corrcoef(trend[:-1], trend[1:])[0, 1]\n    \n    # Calculate trend strength\n    trend_variance = np.var(trend)\n    total_variance = np.var(trend + np.random.normal(0, 1, len(trend)))\n    trend_strength = trend_variance / total_variance\n    \n    return {\n        'slope': trend_slope,\n        'curvature': trend_curvature,\n        'persistence': trend_persistence,\n        'strength': trend_strength,\n        'variance': trend_variance\n    }\n\ndef analyze_seasonal_properties(seasonal, period):\n    \"\"\"\n    Analyze mathematical properties of seasonal component\n    \n    Parameters:\n    seasonal: array, seasonal component\n    period: int, seasonal period\n    \n    Returns:\n    dict: seasonal analysis results\n    \"\"\"\n    # Check periodicity\n    seasonal_periods = []\n    for i in range(period):\n        seasonal_periods.append(seasonal[i::period])\n    \n    periodicity_check = all(np.allclose(seasonal_periods[0], seasonal_periods[j], atol=1e-6) \n                           for j in range(1, period))\n    \n    # Calculate seasonal strength\n    seasonal_variance = np.var(seasonal)\n    total_variance = np.var(seasonal + np.random.normal(0, 1, len(seasonal)))\n    seasonal_strength = seasonal_variance / total_variance\n    \n    # Calculate seasonal pattern\n    seasonal_pattern = np.mean(seasonal_periods, axis=1)\n    \n    # Check zero-sum property (additive model)\n    zero_sum_check = abs(np.sum(seasonal_pattern)) < 1e-6\n    \n    return {\n        'periodicity': periodicity_check,\n        'strength': seasonal_strength,\n        'pattern': seasonal_pattern,\n        'zero_sum': zero_sum_check,\n        'variance': seasonal_variance\n    }\n\ndef analyze_cyclical_properties(cyclical):\n    \"\"\"\n    Analyze mathematical properties of cyclical component\n    \n    Parameters:\n    cyclical: array, cyclical component\n    \n    Returns:\n    dict: cyclical analysis results\n    \"\"\"\n    # Calculate autocorrelation\n    autocorr = np.corrcoef(cyclical[:-1], cyclical[1:])[0, 1]\n    \n    # Calculate cycle length using autocorrelation\n    acf = np.correlate(cyclical, cyclical, mode='full')\n    acf = acf[len(cyclical)-1:]\n    \n    # Find first peak after lag 1\n    peaks = []\n    for i in range(2, len(acf)-1):\n        if acf[i] > acf[i-1] and acf[i] > acf[i+1]:\n            peaks.append(i)\n    \n    cycle_length = peaks[0] if peaks else None\n    \n    # Calculate cycle amplitude\n    cycle_amplitude = np.max(cyclical) - np.min(cyclical)\n    \n    # Calculate cycle variance\n    cycle_variance = np.var(cyclical)\n    \n    return {\n        'autocorrelation': autocorr,\n        'cycle_length': cycle_length,\n        'amplitude': cycle_amplitude,\n        'variance': cycle_variance\n    }\n\ndef analyze_residual_properties(residual):\n    \"\"\"\n    Analyze mathematical properties of residual component\n    \n    Parameters:\n    residual: array, residual component\n    \n    Returns:\n    dict: residual analysis results\n    \"\"\"\n    # Check zero mean\n    mean_check = abs(np.mean(residual)) < 1e-6\n    \n    # Check constant variance (homoscedasticity)\n    # Split into segments and check variance\n    n_segments = 4\n    segment_size = len(residual) // n_segments\n    variances = []\n    \n    for i in range(n_segments):\n        start_idx = i * segment_size\n        end_idx = start_idx + segment_size\n        variances.append(np.var(residual[start_idx:end_idx]))\n    \n    variance_ratio = max(variances) / min(variances)\n    homoscedastic = variance_ratio < 2.0  # Simple threshold\n    \n    # Check uncorrelated (no autocorrelation)\n    autocorr_lag1 = np.corrcoef(residual[:-1], residual[1:])[0, 1]\n    uncorrelated = abs(autocorr_lag1) < 0.1  # Simple threshold\n    \n    # Normality test\n    _, normality_p = stats.normaltest(residual)\n    normal = normality_p > 0.05\n    \n    return {\n        'zero_mean': mean_check,\n        'homoscedastic': homoscedastic,\n        'uncorrelated': uncorrelated,\n        'normal': normal,\n        'variance': np.var(residual),\n        'autocorr_lag1': autocorr_lag1,\n        'normality_p': normality_p\n    }\n\n# Generate comprehensive time series example\nnp.random.seed(42)\nn = 200\nt = np.arange(n)\n\n# Generate components\ntrend = generate_trend_component(n, 'logistic', L=100, k=0.05, t0=100)\nseasonal = generate_seasonal_component(n, period=12, seasonal_type='harmonic', k=2)\ncyclical = generate_cyclical_component(n, cycle_period=40, amplitude=5)\nresidual = generate_random_component(n, 'normal', sigma=2)\n\n# Combine into time series\ny_additive = trend + seasonal + cyclical + residual\ny_multiplicative = trend * (1 + 0.1*seasonal) * (1 + 0.05*cyclical) * (1 + 0.02*residual)\n\nprint(\"Time Series Components Analysis\")\nprint(\"=\" * 50)\n\n# Analyze additive decomposition\ndecomposition = decompose_time_series(y_additive, period=12, model='additive')\n\nprint(f\"\\nAdditive Decomposition: Y_t = T_t + S_t + C_t + R_t\")\nprint(f\"Trend variance: {np.var(decomposition['trend']):.2f}\")\nprint(f\"Seasonal variance: {np.var(decomposition['seasonal']):.2f}\")\nprint(f\"Cyclical variance: {np.var(decomposition['cyclical']):.2f}\")\nprint(f\"Residual variance: {np.var(decomposition['residual']):.2f}\")\n\n# Analyze component properties\ntrend_analysis = analyze_trend_properties(decomposition['trend'], t)\nseasonal_analysis = analyze_seasonal_properties(decomposition['seasonal'], 12)\ncyclical_analysis = analyze_cyclical_properties(decomposition['cyclical'])\nresidual_analysis = analyze_residual_properties(decomposition['residual'])\n\nprint(f\"\\nTrend Analysis:\")\nprint(f\"  Slope: {trend_analysis['slope']:.4f}\")\nprint(f\"  Curvature: {trend_analysis['curvature']:.6f}\")\nprint(f\"  Persistence: {trend_analysis['persistence']:.4f}\")\nprint(f\"  Strength: {trend_analysis['strength']:.4f}\")\n\nprint(f\"\\nSeasonal Analysis:\")\nprint(f\"  Periodicity: {seasonal_analysis['periodicity']}\")\nprint(f\"  Strength: {seasonal_analysis['strength']:.4f}\")\nprint(f\"  Zero-sum property: {seasonal_analysis['zero_sum']}\")\n\nprint(f\"\\nCyclical Analysis:\")\nprint(f\"  Autocorrelation: {cyclical_analysis['autocorrelation']:.4f}\")\nprint(f\"  Cycle length: {cyclical_analysis['cycle_length']}\")\nprint(f\"  Amplitude: {cyclical_analysis['amplitude']:.2f}\")\n\nprint(f\"\\nResidual Analysis:\")\nprint(f\"  Zero mean: {residual_analysis['zero_mean']}\")\nprint(f\"  Homoscedastic: {residual_analysis['homoscedastic']}\")\nprint(f\"  Uncorrelated: {residual_analysis['uncorrelated']}\")\nprint(f\"  Normal: {residual_analysis['normal']}\")\n\n# Visualize decomposition\nplt.figure(figsize=(15, 12))\n\n# Original series\nplt.subplot(4, 2, 1)\nplt.plot(t, y_additive, 'b-', linewidth=1)\nplt.title('Original Time Series (Additive)')\nplt.ylabel('Y_t')\nplt.grid(True, alpha=0.3)\n\n# Components\nplt.subplot(4, 2, 2)\nplt.plot(t, trend, 'r-', linewidth=2, label='Trend')\nplt.plot(t, seasonal, 'g-', linewidth=1, label='Seasonal')\nplt.plot(t, cyclical, 'm-', linewidth=1, label='Cyclical')\nplt.plot(t, residual, 'k-', linewidth=0.5, label='Residual')\nplt.title('Individual Components')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Trend component\nplt.subplot(4, 2, 3)\nplt.plot(t, decomposition['trend'], 'r-', linewidth=2)\nplt.title('Trend Component')\nplt.ylabel('T_t')\nplt.grid(True, alpha=0.3)\n\n# Seasonal component\nplt.subplot(4, 2, 4)\nplt.plot(t, decomposition['seasonal'], 'g-', linewidth=1)\nplt.title('Seasonal Component')\nplt.ylabel('S_t')\nplt.grid(True, alpha=0.3)\n\n# Cyclical component\nplt.subplot(4, 2, 5)\nplt.plot(t, decomposition['cyclical'], 'm-', linewidth=1)\nplt.title('Cyclical Component')\nplt.ylabel('C_t')\nplt.grid(True, alpha=0.3)\n\n# Residual component\nplt.subplot(4, 2, 6)\nplt.plot(t, decomposition['residual'], 'k-', linewidth=0.5)\nplt.title('Residual Component')\nplt.ylabel('R_t')\nplt.grid(True, alpha=0.3)\n\n# Seasonal pattern\nplt.subplot(4, 2, 7)\nseasonal_pattern = seasonal_analysis['pattern']\nplt.bar(range(12), seasonal_pattern, alpha=0.7, color='green')\nplt.title('Seasonal Pattern (12-period)')\nplt.xlabel('Season')\nplt.ylabel('Amplitude')\nplt.grid(True, alpha=0.3)\n\n# Residual diagnostics\nplt.subplot(4, 2, 8)\nplt.hist(decomposition['residual'], bins=20, alpha=0.7, color='gray', edgecolor='black')\nplt.title('Residual Distribution')\nplt.xlabel('Residual Value')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate mathematical relationships\nprint(f\"\\nMathematical Relationships Verification:\")\nprint(f\"1. Additive Decomposition: Y_t = T_t + S_t + C_t + R_t\")\nreconstructed = (decomposition['trend'] + decomposition['seasonal'] + \n                decomposition['cyclical'] + decomposition['residual'])\nreconstruction_error = np.mean((y_additive - reconstructed)**2)\nprint(f\"   Reconstruction MSE: {reconstruction_error:.6f}\")\n\nprint(f\"\\n2. Seasonal Periodicity: S_t = S_{t+s}\")\nperiodicity_check = np.allclose(decomposition['seasonal'][:12], \n                               decomposition['seasonal'][12:24], atol=1e-6)\nprint(f\"   Periodicity holds: {periodicity_check}\")\n\nprint(f\"\\n3. Residual Properties:\")\nprint(f\"   Mean ≈ 0: {abs(np.mean(decomposition['residual'])):.6f}\")\nprint(f\"   Variance: {np.var(decomposition['residual']):.4f}\")\nprint(f\"   Autocorrelation lag-1: {residual_analysis['autocorr_lag1']:.4f}\")\n\n# Compare additive vs multiplicative\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(t, y_additive, 'b-', linewidth=1, label='Additive')\nplt.plot(t, y_multiplicative, 'r-', linewidth=1, label='Multiplicative')\nplt.title('Additive vs Multiplicative Models')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(2, 2, 2)\n# Show how multiplicative affects seasonality\ndecomp_mult = decompose_time_series(y_multiplicative, period=12, model='multiplicative')\nplt.plot(t, decomposition['seasonal'], 'b-', label='Additive Seasonal')\nplt.plot(t, decomp_mult['seasonal'], 'r-', label='Multiplicative Seasonal')\nplt.title('Seasonal Components Comparison')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(2, 2, 3)\n# Variance decomposition\nvariances = [np.var(decomposition['trend']), \n            np.var(decomposition['seasonal']),\n            np.var(decomposition['cyclical']),\n            np.var(decomposition['residual'])]\nlabels = ['Trend', 'Seasonal', 'Cyclical', 'Residual']\nplt.pie(variances, labels=labels, autopct='%1.1f%%')\nplt.title('Variance Decomposition')\n\nplt.subplot(2, 2, 4)\n# Autocorrelation function\ndef autocorr(x, max_lag=20):\n    acf = []\n    for lag in range(max_lag + 1):\n        if lag == 0:\n            acf.append(1.0)\n        else:\n            correlation = np.corrcoef(x[:-lag], x[lag:])[0, 1]\n            acf.append(correlation)\n    return acf\n\nacf_original = autocorr(y_additive)\nacf_residual = autocorr(decomposition['residual'])\n\nplt.plot(range(len(acf_original)), acf_original, 'b-', label='Original Series')\nplt.plot(range(len(acf_residual)), acf_residual, 'r-', label='Residuals')\nplt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\nplt.title('Autocorrelation Functions')\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stationarity\n\n### Testing for Stationarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_stationarity(timeseries):\n    \"\"\"Test stationarity using multiple methods\"\"\"\n    \n    # Augmented Dickey-Fuller test\n    adf_result = adfuller(timeseries)\n    adf_statistic = adf_result[0]\n    adf_pvalue = adf_result[1]\n    adf_critical_values = adf_result[4]\n    \n    # KPSS test\n    kpss_result = kpss(timeseries)\n    kpss_statistic = kpss_result[0]\n    kpss_pvalue = kpss_result[1]\n    kpss_critical_values = kpss_result[3]\n    \n    return {\n        'adf_statistic': adf_statistic,\n        'adf_pvalue': adf_pvalue,\n        'adf_critical_values': adf_critical_values,\n        'kpss_statistic': kpss_statistic,\n        'kpss_pvalue': kpss_pvalue,\n        'kpss_critical_values': kpss_critical_values\n    }\n\n# Test original series\noriginal_stationarity = test_stationarity(ts_data)\n\nprint(\"Stationarity Tests - Original Series\")\nprint(f\"ADF Test:\")\nprint(f\"  Statistic: {original_stationarity['adf_statistic']:.4f}\")\nprint(f\"  P-value: {original_stationarity['adf_pvalue']:.4f}\")\nprint(f\"  Critical values: {original_stationarity['adf_critical_values']}\")\n\nprint(f\"\\nKPSS Test:\")\nprint(f\"  Statistic: {original_stationarity['kpss_statistic']:.4f}\")\nprint(f\"  P-value: {original_stationarity['kpss_pvalue']:.4f}\")\nprint(f\"  Critical values: {original_stationarity['kpss_critical_values']}\")\n\n# Test differenced series\nts_diff = ts_data.diff().dropna()\ndiff_stationarity = test_stationarity(ts_diff)\n\nprint(f\"\\nStationarity Tests - Differenced Series\")\nprint(f\"ADF Test:\")\nprint(f\"  Statistic: {diff_stationarity['adf_statistic']:.4f}\")\nprint(f\"  P-value: {diff_stationarity['adf_pvalue']:.4f}\")\n\nprint(f\"\\nKPSS Test:\")\nprint(f\"  Statistic: {diff_stationarity['kpss_statistic']:.4f}\")\nprint(f\"  P-value: {diff_stationarity['kpss_pvalue']:.4f}\")\n\n# Visualize stationarity\nplt.figure(figsize=(15, 10))\n\n# Original series\nplt.subplot(3, 2, 1)\nplt.plot(ts_data.index, ts_data.values, 'b-', linewidth=1)\nplt.title('Original Time Series')\nplt.ylabel('Value')\n\nplt.subplot(3, 2, 2)\nplt.hist(ts_data.values, bins=30, alpha=0.7, color='blue', edgecolor='black')\nplt.title('Distribution of Original Series')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\n# First difference\nplt.subplot(3, 2, 3)\nplt.plot(ts_diff.index, ts_diff.values, 'g-', linewidth=1)\nplt.title('First Difference')\nplt.ylabel('Value')\n\nplt.subplot(3, 2, 4)\nplt.hist(ts_diff.values, bins=30, alpha=0.7, color='green', edgecolor='black')\nplt.title('Distribution of First Difference')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\n# Second difference\nts_diff2 = ts_diff.diff().dropna()\nplt.subplot(3, 2, 5)\nplt.plot(ts_diff2.index, ts_diff2.values, 'r-', linewidth=1)\nplt.title('Second Difference')\nplt.ylabel('Value')\n\nplt.subplot(3, 2, 6)\nplt.hist(ts_diff2.values, bins=30, alpha=0.7, color='red', edgecolor='black')\nplt.title('Distribution of Second Difference')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autocorrelation\n\n### ACF and PACF Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_autocorrelation(timeseries, max_lag=40):\n    \"\"\"Analyze autocorrelation and partial autocorrelation\"\"\"\n    \n    # Calculate ACF and PACF\n    acf_values = acf(timeseries, nlags=max_lag)\n    pacf_values = pacf(timeseries, nlags=max_lag)\n    \n    # Confidence intervals (95%)\n    confidence_interval = 1.96 / np.sqrt(len(timeseries))\n    \n    return acf_values, pacf_values, confidence_interval\n\nacf_vals, pacf_vals, ci = analyze_autocorrelation(ts_data)\n\nprint(\"Autocorrelation Analysis\")\nprint(f\"Confidence interval: ±{ci:.3f}\")\n\n# Visualize ACF and PACF\nplt.figure(figsize=(15, 5))\n\n# ACF plot\nplt.subplot(1, 2, 1)\nlags = np.arange(len(acf_vals))\nplt.bar(lags, acf_vals, alpha=0.7, color='skyblue', edgecolor='black')\nplt.axhline(ci, color='red', linestyle='--', alpha=0.7, label=f'95% CI: {ci:.3f}')\nplt.axhline(-ci, color='red', linestyle='--', alpha=0.7)\nplt.axhline(0, color='black', linestyle='-', alpha=0.7)\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.title('Autocorrelation Function (ACF)')\nplt.legend()\n\n# PACF plot\nplt.subplot(1, 2, 2)\nplt.bar(lags, pacf_vals, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.axhline(ci, color='red', linestyle='--', alpha=0.7, label=f'95% CI: {ci:.3f}')\nplt.axhline(-ci, color='red', linestyle='--', alpha=0.7)\nplt.axhline(0, color='black', linestyle='-', alpha=0.7)\nplt.xlabel('Lag')\nplt.ylabel('Partial Autocorrelation')\nplt.title('Partial Autocorrelation Function (PACF)')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Using statsmodels built-in functions\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\nplot_acf(ts_data, ax=axes[0], lags=40)\nplot_pacf(ts_data, ax=axes[1], lags=40)\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Moving Averages\n\n### Simple and Exponential Moving Averages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_moving_averages(timeseries, windows=[5, 10, 20]):\n    \"\"\"Calculate different types of moving averages\"\"\"\n    \n    moving_averages = {}\n    \n    # Simple Moving Average (SMA)\n    for window in windows:\n        moving_averages[f'SMA_{window}'] = timeseries.rolling(window=window).mean()\n    \n    # Exponential Moving Average (EMA)\n    for alpha in [0.1, 0.3, 0.5]:\n        moving_averages[f'EMA_{alpha}'] = timeseries.ewm(alpha=alpha).mean()\n    \n    # Weighted Moving Average (WMA)\n    for window in [5, 10]:\n        weights = np.arange(1, window + 1)\n        wma = timeseries.rolling(window=window).apply(\n            lambda x: np.dot(x, weights) / weights.sum(), raw=True\n        )\n        moving_averages[f'WMA_{window}'] = wma\n    \n    return moving_averages\n\nma_results = calculate_moving_averages(ts_data)\n\n# Visualize moving averages\nplt.figure(figsize=(15, 10))\n\n# Original data with SMAs\nplt.subplot(2, 2, 1)\nplt.plot(ts_data.index, ts_data.values, 'b-', alpha=0.7, label='Original', linewidth=1)\nfor window in [5, 10, 20]:\n    plt.plot(ts_data.index, ma_results[f'SMA_{window}'], \n             linewidth=2, label=f'SMA {window}')\nplt.title('Simple Moving Averages')\nplt.ylabel('Value')\nplt.legend()\n\n# Original data with EMAs\nplt.subplot(2, 2, 2)\nplt.plot(ts_data.index, ts_data.values, 'b-', alpha=0.7, label='Original', linewidth=1)\nfor alpha in [0.1, 0.3, 0.5]:\n    plt.plot(ts_data.index, ma_results[f'EMA_{alpha}'], \n             linewidth=2, label=f'EMA α={alpha}')\nplt.title('Exponential Moving Averages')\nplt.ylabel('Value')\nplt.legend()\n\n# Original data with WMAs\nplt.subplot(2, 2, 3)\nplt.plot(ts_data.index, ts_data.values, 'b-', alpha=0.7, label='Original', linewidth=1)\nfor window in [5, 10]:\n    plt.plot(ts_data.index, ma_results[f'WMA_{window}'], \n             linewidth=2, label=f'WMA {window}')\nplt.title('Weighted Moving Averages')\nplt.ylabel('Value')\nplt.legend()\n\n# Comparison of different methods\nplt.subplot(2, 2, 4)\nplt.plot(ts_data.index, ts_data.values, 'b-', alpha=0.7, label='Original', linewidth=1)\nplt.plot(ts_data.index, ma_results['SMA_10'], 'r-', linewidth=2, label='SMA 10')\nplt.plot(ts_data.index, ma_results['EMA_0.3'], 'g-', linewidth=2, label='EMA 0.3')\nplt.plot(ts_data.index, ma_results['WMA_10'], 'orange', linewidth=2, label='WMA 10')\nplt.title('Comparison of Moving Averages')\nplt.ylabel('Value')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Calculate performance metrics\ndef evaluate_moving_averages(original, predictions):\n    \"\"\"Evaluate moving average performance\"\"\"\n    # Remove NaN values for comparison\n    valid_mask = ~np.isnan(predictions)\n    if np.sum(valid_mask) == 0:\n        return {'mse': np.nan, 'mae': np.nan, 'mape': np.nan}\n    \n    original_valid = original[valid_mask]\n    predictions_valid = predictions[valid_mask]\n    \n    mse = mean_squared_error(original_valid, predictions_valid)\n    mae = mean_absolute_error(original_valid, predictions_valid)\n    mape = np.mean(np.abs((original_valid - predictions_valid) / original_valid)) * 100\n    \n    return {'mse': mse, 'mae': mae, 'mape': mape}\n\nprint(\"Moving Average Performance Evaluation\")\nfor name, ma_series in ma_results.items():\n    metrics = evaluate_moving_averages(ts_data.values, ma_series.values)\n    print(f\"{name}: MSE={metrics['mse']:.3f}, MAE={metrics['mae']:.3f}, MAPE={metrics['mape']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ARIMA Models\n\n### ARIMA Model Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_arima_models(timeseries, orders):\n    \"\"\"Fit multiple ARIMA models with different parameters\"\"\"\n    \n    models = {}\n    results = {}\n    \n    for order in orders:\n        try:\n            # Fit ARIMA model\n            model = ARIMA(timeseries, order=order)\n            fitted_model = model.fit()\n            \n            models[order] = fitted_model\n            results[order] = {\n                'aic': fitted_model.aic,\n                'bic': fitted_model.bic,\n                'log_likelihood': fitted_model.llf,\n                'residuals': fitted_model.resid\n            }\n            \n        except Exception as e:\n            print(f\"Error fitting ARIMA{order}: {e}\")\n            continue\n    \n    return models, results\n\n# Define different ARIMA orders to try\narima_orders = [\n    (1, 1, 0), (0, 1, 1), (1, 1, 1),\n    (2, 1, 0), (0, 1, 2), (2, 1, 2),\n    (1, 0, 1), (2, 0, 2), (1, 0, 0), (0, 0, 1)\n]\n\narima_models, arima_results = fit_arima_models(ts_data, arima_orders)\n\nprint(\"ARIMA Model Comparison\")\nprint(\"Order\\t\\tAIC\\t\\tBIC\\t\\tLog-Likelihood\")\nprint(\"-\" * 50)\nfor order, result in arima_results.items():\n    print(f\"{order}\\t\\t{result['aic']:.2f}\\t\\t{result['bic']:.2f}\\t\\t{result['log_likelihood']:.2f}\")\n\n# Find best model\nbest_order = min(arima_results.keys(), key=lambda x: arima_results[x]['aic'])\nbest_model = arima_models[best_order]\n\nprint(f\"\\nBest ARIMA model: {best_order}\")\nprint(f\"AIC: {arima_results[best_order]['aic']:.2f}\")\nprint(f\"BIC: {arima_results[best_order]['bic']:.2f}\")\n\n# Model diagnostics\nplt.figure(figsize=(15, 10))\n\n# Residuals\nresiduals = arima_results[best_order]['residuals']\nplt.subplot(2, 3, 1)\nplt.plot(residuals.index, residuals.values, 'b-', linewidth=1)\nplt.title('Residuals')\nplt.ylabel('Residual')\n\n# Residuals histogram\nplt.subplot(2, 3, 2)\nplt.hist(residuals.values, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\nplt.title('Residuals Distribution')\nplt.xlabel('Residual')\nplt.ylabel('Frequency')\n\n# Q-Q plot\nplt.subplot(2, 3, 3)\nstats.probplot(residuals.values, dist=\"norm\", plot=plt)\nplt.title('Q-Q Plot of Residuals')\n\n# ACF of residuals\nplt.subplot(2, 3, 4)\nplot_acf(residuals, ax=plt.gca(), lags=20)\nplt.title('ACF of Residuals')\n\n# PACF of residuals\nplt.subplot(2, 3, 5)\nplot_pacf(residuals, ax=plt.gca(), lags=20)\nplt.title('PACF of Residuals')\n\n# Residuals vs fitted\nfitted_values = ts_data - residuals\nplt.subplot(2, 3, 6)\nplt.scatter(fitted_values, residuals, alpha=0.7, color='lightgreen')\nplt.axhline(0, color='red', linestyle='--', alpha=0.7)\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Fitted')\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Seasonal Decomposition\n\n### STL Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_seasonal_decomposition(timeseries, period=12):\n    \"\"\"Perform seasonal decomposition using STL\"\"\"\n    \n    # STL decomposition\n    decomposition = seasonal_decompose(timeseries, period=period, extrapolate_trend='freq')\n    \n    return decomposition\n\ndecomposition = perform_seasonal_decomposition(ts_data)\n\n# Visualize decomposition\nplt.figure(figsize=(15, 10))\n\n# Original series\nplt.subplot(4, 1, 1)\nplt.plot(ts_data.index, ts_data.values, 'b-', linewidth=1)\nplt.title('Original Time Series')\nplt.ylabel('Value')\n\n# Trend\nplt.subplot(4, 1, 2)\nplt.plot(decomposition.trend.index, decomposition.trend.values, 'r-', linewidth=2)\nplt.title('Trend Component')\nplt.ylabel('Value')\n\n# Seasonal\nplt.subplot(4, 1, 3)\nplt.plot(decomposition.seasonal.index, decomposition.seasonal.values, 'g-', linewidth=2)\nplt.title('Seasonal Component')\nplt.ylabel('Value')\n\n# Residual\nplt.subplot(4, 1, 4)\nplt.plot(decomposition.resid.index, decomposition.resid.values, 'orange', linewidth=1)\nplt.title('Residual Component')\nplt.ylabel('Value')\n\nplt.tight_layout()\nplt.show()\n\n# Analyze seasonal patterns\nseasonal_data = decomposition.seasonal.values\nseasonal_period = 12\n\n# Extract seasonal pattern\nseasonal_pattern = []\nfor i in range(seasonal_period):\n    pattern_values = seasonal_data[i::seasonal_period]\n    seasonal_pattern.append(np.mean(pattern_values))\n\nprint(\"Seasonal Pattern Analysis\")\nprint(f\"Seasonal period: {seasonal_period}\")\nprint(\"Seasonal pattern (monthly averages):\")\nfor i, pattern in enumerate(seasonal_pattern):\n    print(f\"  Month {i+1}: {pattern:.3f}\")\n\n# Visualize seasonal pattern\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nmonths = range(1, seasonal_period + 1)\nplt.bar(months, seasonal_pattern, alpha=0.7, color='skyblue', edgecolor='black')\nplt.xlabel('Month')\nplt.ylabel('Seasonal Effect')\nplt.title('Seasonal Pattern')\nplt.xticks(months)\n\nplt.subplot(1, 2, 2)\nplt.plot(months, seasonal_pattern, 'ro-', linewidth=2, markersize=8)\nplt.xlabel('Month')\nplt.ylabel('Seasonal Effect')\nplt.title('Seasonal Pattern (Line Plot)')\nplt.xticks(months)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forecasting\n\n### Time Series Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_forecasting(model, steps=24):\n    \"\"\"Perform forecasting using fitted ARIMA model\"\"\"\n    \n    # Generate forecast\n    forecast = model.forecast(steps=steps)\n    forecast_conf = model.get_forecast(steps=steps)\n    \n    # Get confidence intervals\n    conf_int = forecast_conf.conf_int()\n    \n    return forecast, conf_int\n\n# Perform forecasting\nforecast_values, conf_intervals = perform_forecasting(best_model, steps=24)\n\n# Create forecast index\nlast_date = ts_data.index[-1]\nforecast_index = pd.date_range(start=last_date + pd.DateOffset(months=1), \n                              periods=24, freq='M')\n\nprint(\"Forecasting Results\")\nprint(f\"Forecast period: {forecast_index[0]} to {forecast_index[-1]}\")\nprint(f\"Number of forecast steps: {len(forecast_values)}\")\n\n# Visualize forecast\nplt.figure(figsize=(15, 8))\n\n# Historical data\nplt.plot(ts_data.index, ts_data.values, 'b-', linewidth=2, label='Historical Data')\n\n# Forecast\nplt.plot(forecast_index, forecast_values, 'r-', linewidth=2, label='Forecast')\n\n# Confidence intervals\nplt.fill_between(forecast_index, \n                 conf_intervals.iloc[:, 0], \n                 conf_intervals.iloc[:, 1], \n                 alpha=0.3, color='red', label='95% Confidence Interval')\n\nplt.xlabel('Date')\nplt.ylabel('Value')\nplt.title('Time Series Forecast')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Forecast evaluation metrics\ndef evaluate_forecast(actual, predicted):\n    \"\"\"Evaluate forecast performance\"\"\"\n    mse = mean_squared_error(actual, predicted)\n    mae = mean_absolute_error(actual, predicted)\n    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n    \n    return {'mse': mse, 'mae': mae, 'mape': mape}\n\n# For demonstration, let's create a test set\ntest_size = 12\ntrain_data = ts_data[:-test_size]\ntest_data = ts_data[-test_size:]\n\n# Fit model on training data\ntrain_model = ARIMA(train_data, order=best_order).fit()\n\n# Forecast on test period\ntest_forecast = train_model.forecast(steps=test_size)\n\n# Evaluate forecast\nforecast_metrics = evaluate_forecast(test_data.values, test_forecast.values)\n\nprint(\"Forecast Evaluation\")\nprint(f\"MSE: {forecast_metrics['mse']:.3f}\")\nprint(f\"MAE: {forecast_metrics['mae']:.3f}\")\nprint(f\"MAPE: {forecast_metrics['mape']:.2f}%\")\n\n# Visualize forecast evaluation\nplt.figure(figsize=(12, 6))\n\nplt.plot(test_data.index, test_data.values, 'b-', linewidth=2, label='Actual')\nplt.plot(test_data.index, test_forecast.values, 'r-', linewidth=2, label='Forecast')\nplt.xlabel('Date')\nplt.ylabel('Value')\nplt.title('Forecast vs Actual (Test Set)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Applications\n\n### Stock Price Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_stock_prices(n_days=252):\n    \"\"\"Simulate stock price data\"\"\"\n    np.random.seed(42)\n    \n    # Generate daily returns\n    daily_returns = np.random.normal(0.0005, 0.02, n_days)  # 0.05% daily return, 2% volatility\n    \n    # Generate price series\n    initial_price = 100\n    prices = [initial_price]\n    \n    for return_val in daily_returns:\n        new_price = prices[-1] * (1 + return_val)\n        prices.append(new_price)\n    \n    # Create datetime index\n    dates = pd.date_range('2023-01-01', periods=n_days, freq='D')\n    price_series = pd.Series(prices[1:], index=dates)\n    \n    return price_series\n\nstock_prices = simulate_stock_prices()\n\nprint(\"Stock Price Analysis\")\nprint(f\"Data shape: {stock_prices.shape}\")\nprint(f\"Date range: {stock_prices.index[0]} to {stock_prices.index[-1]}\")\nprint(f\"Initial price: ${stock_prices.iloc[0]:.2f}\")\nprint(f\"Final price: ${stock_prices.iloc[-1]:.2f}\")\nprint(f\"Total return: {((stock_prices.iloc[-1] / stock_prices.iloc[0]) - 1) * 100:.2f}%\")\n\n# Analyze stock prices\nplt.figure(figsize=(15, 10))\n\n# Price series\nplt.subplot(2, 3, 1)\nplt.plot(stock_prices.index, stock_prices.values, 'b-', linewidth=1)\nplt.title('Stock Price Series')\nplt.ylabel('Price ($)')\nplt.xticks(rotation=45)\n\n# Returns\nreturns = stock_prices.pct_change().dropna()\nplt.subplot(2, 3, 2)\nplt.plot(returns.index, returns.values, 'g-', linewidth=1)\nplt.title('Daily Returns')\nplt.ylabel('Return')\nplt.xticks(rotation=45)\n\n# Returns distribution\nplt.subplot(2, 3, 3)\nplt.hist(returns.values, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\nplt.title('Returns Distribution')\nplt.xlabel('Return')\nplt.ylabel('Frequency')\n\n# Moving averages\nsma_20 = stock_prices.rolling(window=20).mean()\nsma_50 = stock_prices.rolling(window=50).mean()\nplt.subplot(2, 3, 4)\nplt.plot(stock_prices.index, stock_prices.values, 'b-', alpha=0.7, label='Price', linewidth=1)\nplt.plot(sma_20.index, sma_20.values, 'r-', linewidth=2, label='SMA 20')\nplt.plot(sma_50.index, sma_50.values, 'g-', linewidth=2, label='SMA 50')\nplt.title('Price with Moving Averages')\nplt.ylabel('Price ($)')\nplt.legend()\nplt.xticks(rotation=45)\n\n# Volatility (rolling standard deviation)\nvolatility = returns.rolling(window=20).std() * np.sqrt(252)  # Annualized\nplt.subplot(2, 3, 5)\nplt.plot(volatility.index, volatility.values, 'orange', linewidth=2)\nplt.title('Rolling Volatility (20-day)')\nplt.ylabel('Volatility')\nplt.xticks(rotation=45)\n\n# ACF of returns\nplt.subplot(2, 3, 6)\nplot_acf(returns, ax=plt.gca(), lags=20)\nplt.title('ACF of Returns')\n\nplt.tight_layout()\nplt.show()\n\n# Test for stationarity in returns\nreturns_stationarity = test_stationarity(returns)\nprint(f\"\\nReturns Stationarity Test:\")\nprint(f\"ADF p-value: {returns_stationarity['adf_pvalue']:.4f}\")\nprint(f\"KPSS p-value: {returns_stationarity['kpss_pvalue']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sales Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_sales_data(n_months=60):\n    \"\"\"Simulate monthly sales data with trend and seasonality\"\"\"\n    np.random.seed(42)\n    \n    # Generate time index\n    months = np.arange(n_months)\n    \n    # Components\n    trend = 1000 + 50 * months  # Linear trend\n    seasonal = 200 * np.sin(2 * np.pi * months / 12) + 100 * np.cos(2 * np.pi * months / 12)\n    noise = np.random.normal(0, 100, n_months)\n    \n    # Combine components\n    sales = trend + seasonal + noise\n    \n    # Create datetime index\n    dates = pd.date_range('2019-01-01', periods=n_months, freq='M')\n    sales_series = pd.Series(sales, index=dates)\n    \n    return sales_series\n\nsales_data = simulate_sales_data()\n\nprint(\"Sales Forecasting Analysis\")\nprint(f\"Data shape: {sales_data.shape}\")\nprint(f\"Date range: {sales_data.index[0]} to {sales_data.index[-1]}\")\nprint(f\"Average monthly sales: {sales_data.mean():.0f}\")\nprint(f\"Sales growth: {((sales_data.iloc[-1] / sales_data.iloc[0]) - 1) * 100:.1f}%\")\n\n# Analyze sales data\nplt.figure(figsize=(15, 10))\n\n# Sales series\nplt.subplot(2, 3, 1)\nplt.plot(sales_data.index, sales_data.values, 'b-', linewidth=1)\nplt.title('Monthly Sales')\nplt.ylabel('Sales')\nplt.xticks(rotation=45)\n\n# Seasonal decomposition\nsales_decomposition = perform_seasonal_decomposition(sales_data)\nplt.subplot(2, 3, 2)\nplt.plot(sales_decomposition.seasonal.index, sales_decomposition.seasonal.values, 'g-', linewidth=2)\nplt.title('Seasonal Component')\nplt.ylabel('Seasonal Effect')\nplt.xticks(rotation=45)\n\n# Trend\nplt.subplot(2, 3, 3)\nplt.plot(sales_decomposition.trend.index, sales_decomposition.trend.values, 'r-', linewidth=2)\nplt.title('Trend Component')\nplt.ylabel('Trend')\nplt.xticks(rotation=45)\n\n# Year-over-year growth\nyoy_growth = sales_data.pct_change(periods=12) * 100\nplt.subplot(2, 3, 4)\nplt.plot(yoy_growth.index, yoy_growth.values, 'purple', linewidth=2)\nplt.title('Year-over-Year Growth')\nplt.ylabel('Growth (%)')\nplt.xticks(rotation=45)\n\n# Monthly averages\nmonthly_avg = sales_data.groupby(sales_data.index.month).mean()\nplt.subplot(2, 3, 5)\nplt.bar(monthly_avg.index, monthly_avg.values, alpha=0.7, color='skyblue', edgecolor='black')\nplt.title('Average Sales by Month')\nplt.xlabel('Month')\nplt.ylabel('Average Sales')\nplt.xticks(range(1, 13))\n\n# ACF\nplt.subplot(2, 3, 6)\nplot_acf(sales_data, ax=plt.gca(), lags=24)\nplt.title('ACF of Sales')\n\nplt.tight_layout()\nplt.show()\n\n# Fit ARIMA model for sales forecasting\nsales_arima = ARIMA(sales_data, order=(1, 1, 1)).fit()\nsales_forecast, sales_conf = perform_forecasting(sales_arima, steps=12)\n\nprint(f\"\\nSales Forecast (Next 12 Months):\")\nforecast_df = pd.DataFrame({\n    'Forecast': sales_forecast.values,\n    'Lower_CI': sales_conf.iloc[:, 0],\n    'Upper_CI': sales_conf.iloc[:, 1]\n}, index=sales_forecast.index)\n\nprint(forecast_df.round(0))\n\n# Visualize sales forecast\nplt.figure(figsize=(12, 6))\n\nplt.plot(sales_data.index, sales_data.values, 'b-', linewidth=2, label='Historical Sales')\nplt.plot(sales_forecast.index, sales_forecast.values, 'r-', linewidth=2, label='Forecast')\nplt.fill_between(sales_forecast.index, \n                 sales_conf.iloc[:, 0], \n                 sales_conf.iloc[:, 1], \n                 alpha=0.3, color='red', label='95% Confidence Interval')\n\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.title('Sales Forecast')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Problems\n\n1. **Trend Analysis**: Create functions to detect and analyze different types of trends (linear, exponential, polynomial).\n\n2. **Seasonality Detection**: Implement methods to automatically detect seasonal patterns and their periods.\n\n3. **Forecast Evaluation**: Build comprehensive forecast evaluation frameworks with multiple metrics.\n\n4. **Anomaly Detection**: Develop time series anomaly detection methods using statistical and machine learning approaches.\n\n## Further Reading\n\n- \"Time Series Analysis: Forecasting and Control\" by Box, Jenkins, Reinsel, and Ljung\n- \"Forecasting: Principles and Practice\" by Rob J. Hyndman and George Athanasopoulos\n- \"Time Series Analysis: Univariate and Multivariate Methods\" by William W.S. Wei\n- \"Applied Time Series Analysis\" by Wayne A. Woodward, Henry L. Gray, and Alan C. Elliott\n\n## Key Takeaways\n\n- **Time series components** include trend, seasonality, cyclical, and random components\n- **Stationarity** is crucial for many time series models and can be tested using ADF and KPSS tests\n- **Autocorrelation** analysis helps identify patterns and guide model selection\n- **Moving averages** provide smoothing and trend identification\n- **ARIMA models** are powerful for forecasting stationary time series\n- **Seasonal decomposition** separates time series into interpretable components\n- **Forecasting** requires careful model selection and evaluation\n- **Real-world applications** include stock prices, sales forecasting, and economic indicators\n\nIn the next chapter, we'll explore multivariate statistics, including principal component analysis, factor analysis, and clustering techniques."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}