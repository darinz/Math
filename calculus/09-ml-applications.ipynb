{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Calculus in Machine Learning\n",
       "\n",
       "[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)\n",
       "[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n",
       "[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-orange.svg)](https://matplotlib.org/)\n",
       "[![SymPy](https://img.shields.io/badge/SymPy-1.10+-purple.svg)](https://www.sympy.org/)\n",
       "\n",
       "## Introduction\n",
       "\n",
       "Calculus is fundamental to machine learning, particularly in optimization, neural networks, and understanding model behavior."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "from sklearn.linear_model import LinearRegression\n",
       "from sklearn.metrics import mean_squared_error\n",
       "\n",
       "plt.style.use('seaborn-v0_8')\n",
       "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 9.1 Linear Regression with Gradient Descent\n",
       "\n",
       "Implement linear regression using gradient descent to demonstrate calculus in action."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Generate synthetic data\n",
       "np.random.seed(42)\n",
       "X = np.random.randn(100, 1) * 2\n",
       "y = 3 * X + 2 + np.random.randn(100, 1) * 0.5\n",
       "\n",
       "# Manual gradient descent\n",
       "def linear_regression_gd(X, y, learning_rate=0.01, epochs=1000):\n",
       "    n_samples = X.shape[0]\n",
       "    w = np.random.randn(1, 1)\n",
       "    b = np.random.randn(1, 1)\n",
       "    history = []\n",
       "    \n",
       "    for epoch in range(epochs):\n",
       "        y_pred = X @ w + b\n",
       "        dw = (2/n_samples) * X.T @ (y_pred - y)\n",
       "        db = (2/n_samples) * np.sum(y_pred - y)\n",
       "        \n",
       "        w -= learning_rate * dw\n",
       "        b -= learning_rate * db\n",
       "        \n",
       "        if epoch % 100 == 0:\n",
       "            mse = mean_squared_error(y, y_pred)\n",
       "            history.append((epoch, w[0,0], b[0,0], mse))\n",
       "    \n",
       "    return w, b, history\n",
       "\n",
       "# Run optimization\n",
       "w_gd, b_gd, history = linear_regression_gd(X, y)\n",
       "print(f\"Gradient Descent: w = {w_gd[0,0]:.4f}, b = {b_gd[0,0]:.4f}\")\n",
       "\n",
       "# Compare with sklearn\n",
       "lr = LinearRegression()\n",
       "lr.fit(X, y)\n",
       "print(f\"Sklearn: w = {lr.coef_[0,0]:.4f}, b = {lr.intercept_[0]:.4f}\")"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }