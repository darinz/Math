{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calculus in Machine Learning\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-orange.svg)](https://matplotlib.org/)\n[![SymPy](https://img.shields.io/badge/SymPy-1.10+-purple.svg)](https://www.sympy.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-red.svg)](https://scipy.org/)\n[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.0+-blue.svg)](https://scikit-learn.org/)\n\n## Introduction\n\nCalculus is the mathematical foundation of machine learning, providing the theoretical framework and computational tools for optimization, model training, and understanding complex systems. Every machine learning algorithm relies on calculus concepts:\n\n- **Derivatives** drive optimization algorithms like gradient descent\n- **Partial derivatives** enable backpropagation in neural networks\n- **Integration** appears in probability, statistics, and model evaluation\n- **Multivariable calculus** handles high-dimensional optimization landscapes\n\nThis section explores how calculus principles translate into practical machine learning algorithms, with rigorous mathematical foundations and detailed implementation insights.\n\n## 9.1 Gradient Descent and Optimization\n\n### Mathematical Foundations\n\nGradient descent is an iterative optimization algorithm that finds local minima of differentiable functions. For a function \\( f: \\mathbb{R}^n \\to \\mathbb{R} \\), the update rule is:\n\\[\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)\n\\]\nwhere \\( \\alpha > 0 \\) is the learning rate and \\( \\nabla f(\\mathbf{x}_k) \\) is the gradient at the current point.\n\n**Key Properties:**\n- The gradient \\( \\nabla f \\) points in the direction of steepest ascent\n- Moving in the opposite direction (descent) reduces the function value\n- The learning rate controls the step size and convergence behavior\n- Convergence depends on the function's smoothness and convexity\n\n**Relevance to ML:**\n- Loss functions in ML are typically differentiable and often convex\n- Gradient descent scales to high-dimensional parameter spaces\n- Understanding convergence helps tune hyperparameters and diagnose training issues\n\n### Python Implementation: Basic Gradient Descent\n\nThe following implementation demonstrates the core principles of gradient descent with detailed commentary on each step and convergence analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\n\n# Simple gradient descent implementation\ndef gradient_descent(f, grad_f, x0, learning_rate=0.1, max_iter=1000, tol=1e-6):\n    \"\"\"\n    Gradient descent optimization algorithm.\n    \n    Mathematical foundation:\n    - Uses the update rule: x_{k+1} = x_k - α∇f(x_k)\n    - Converges to local minima for convex functions\n    - Learning rate α controls convergence speed and stability\n    \n    Parameters:\n    f: objective function (scalar-valued)\n    grad_f: gradient function (vector-valued)\n    x0: initial point\n    learning_rate: step size (α)\n    max_iter: maximum iterations\n    tol: tolerance for convergence\n    \n    Returns:\n    optimal_x: approximate minimizer\n    history: optimization trajectory\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    history = [x.copy()]\n    \n    for i in range(max_iter):\n        gradient = grad_f(x)\n        \n        # Update rule: x_new = x - α∇f(x)\n        x_new = x - learning_rate * gradient\n        \n        # Check convergence: ||x_new - x|| < tolerance\n        if np.linalg.norm(x_new - x) < tol:\n            print(f\"Converged after {i+1} iterations\")\n            break\n            \n        x = x_new\n        history.append(x.copy())\n    \n    return x, np.array(history)\n\n# Example: Minimize f(x) = x² + 2x + 1\n# This is a convex quadratic function with minimum at x = -1\ndef objective_function(x):\n    \"\"\"\n    Objective function: f(x) = x² + 2x + 1\n    - Convex function (second derivative = 2 > 0)\n    - Global minimum at x = -1\n    - f(-1) = 0\n    \"\"\"\n    return x**2 + 2*x + 1\n\ndef gradient_function(x):\n    \"\"\"\n    Gradient: f'(x) = 2x + 2\n    - Linear function\n    - Zero at x = -1 (critical point)\n    \"\"\"\n    return 2*x + 2\n\n# Run gradient descent\nx0 = 5.0  # Start far from the minimum\noptimal_x, history = gradient_descent(objective_function, gradient_function, x0, learning_rate=0.1)\n\nprint(f\"Optimal x: {optimal_x:.6f}\")\nprint(f\"Optimal value: {objective_function(optimal_x):.6f}\")\nprint(f\"Number of iterations: {len(history)}\")\nprint(f\"Gradient at optimum: {gradient_function(optimal_x):.6f}\")\n\n# Visualize optimization\nx_vals = np.linspace(-2, 6, 100)\ny_vals = objective_function(x_vals)\n\nplt.figure(figsize=(12, 8))\n\n# Plot objective function\nplt.subplot(2, 1, 1)\nplt.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = x² + 2x + 1')\nplt.plot(history, [objective_function(x) for x in history], 'ro-', \n         markersize=4, label='Optimization path')\nplt.scatter(optimal_x, objective_function(optimal_x), c='red', s=100, \n           label=f'Optimum: ({optimal_x:.3f}, {objective_function(optimal_x):.3f})')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Gradient Descent Optimization')\nplt.legend()\nplt.grid(True)\n\n# Plot gradient and convergence\nplt.subplot(2, 1, 2)\ngrad_vals = gradient_function(x_vals)\nplt.plot(x_vals, grad_vals, 'g-', linewidth=2, label=\"f'(x) = 2x + 2\")\nplt.plot(history, [gradient_function(x) for x in history], 'mo-', \n         markersize=4, label='Gradient along path')\nplt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel(\"f'(x)\")\nplt.title('Gradient Function and Convergence')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:**\n- The algorithm implements the fundamental gradient descent update rule\n- Convergence is checked using the norm of the parameter update\n- The visualization shows both the objective function and the gradient\n- The gradient approaches zero as the algorithm converges to the minimum\n- This demonstrates the core principle: gradient descent follows the direction of steepest descent\n\n### Stochastic Gradient Descent (SGD)\n\n#### Mathematical Foundations\n\nSGD extends gradient descent to handle large datasets by using noisy gradient estimates. For a loss function \\( L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n L_i(\\theta) \\), the update rule is:\n\\[\n\\theta_{k+1} = \\theta_k - \\alpha \\nabla L_i(\\theta_k)\n\\]\nwhere \\( i \\) is randomly sampled from the dataset.\n\n**Key Properties:**\n- Uses mini-batches to estimate gradients\n- Introduces noise that can help escape local minima\n- Scales to large datasets with limited memory\n- Convergence is probabilistic rather than deterministic\n\n**Relevance to ML:**\n- Essential for training deep neural networks on large datasets\n- Noise can improve generalization by preventing overfitting\n- Batch size affects the trade-off between speed and stability\n\n### Python Implementation: SGD for Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stochastic Gradient Descent for linear regression\ndef sgd_linear_regression(X, y, learning_rate=0.01, epochs=100, batch_size=32):\n    \"\"\"\n    SGD for linear regression: y = Xw + b\n    \n    Mathematical foundation:\n    - Loss function: L(w,b) = (1/n)∑(y_i - (X_i^T w + b))²\n    - Gradients: ∇_w L = -(2/n)X^T(y - Xw - b), ∇_b L = -(2/n)∑(y - Xw - b)\n    - Update rule: w = w - α∇_w L, b = b - α∇_b L\n    \n    Parameters:\n    X: feature matrix (n_samples, n_features)\n    y: target vector (n_samples,)\n    learning_rate: step size\n    epochs: number of passes through the dataset\n    batch_size: size of mini-batches\n    \"\"\"\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)  # Initialize weights to zero\n    b = 0.0                   # Initialize bias to zero\n    \n    history = []\n    \n    for epoch in range(epochs):\n        # Shuffle data for stochastic sampling\n        indices = np.random.permutation(n_samples)\n        X_shuffled = X[indices]\n        y_shuffled = y[indices]\n        \n        for i in range(0, n_samples, batch_size):\n            # Mini-batch: sample a subset of data\n            X_batch = X_shuffled[i:i+batch_size]\n            y_batch = y_shuffled[i:i+batch_size]\n            \n            # Forward pass: compute predictions\n            y_pred = X_batch @ w + b\n            \n            # Compute gradients using the mini-batch\n            error = y_pred - y_batch\n            dw = (2/batch_size) * X_batch.T @ error  # Gradient w.r.t. weights\n            db = (2/batch_size) * np.sum(error)      # Gradient w.r.t. bias\n            \n            # Update parameters using gradient descent\n            w -= learning_rate * dw\n            b -= learning_rate * db\n        \n        # Record loss for monitoring\n        y_pred_full = X @ w + b\n        loss = np.mean((y_pred_full - y)**2)\n        history.append(loss)\n    \n    return w, b, history\n\n# Generate synthetic data\nnp.random.seed(42)\nn_samples = 1000\nX = np.random.randn(n_samples, 2)  # 2 features\ntrue_w = np.array([2.0, -1.5])     # True weights\ntrue_b = 1.0                       # True bias\ny = X @ true_w + true_b + 0.1 * np.random.randn(n_samples)  # Add noise\n\n# Run SGD\nw_learned, b_learned, loss_history = sgd_linear_regression(X, y, learning_rate=0.01, epochs=50)\n\nprint(f\"True weights: {true_w}\")\nprint(f\"Learned weights: {w_learned}\")\nprint(f\"True bias: {true_b}\")\nprint(f\"Learned bias: {b_learned}\")\nprint(f\"Final loss: {loss_history[-1]:.6f}\")\n\n# Plot training loss\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(loss_history, 'b-', linewidth=2)\nplt.xlabel('Epoch')\nplt.ylabel('Mean Squared Error')\nplt.title('SGD Training Loss')\nplt.grid(True)\n\n# Visualize convergence in parameter space\nplt.subplot(1, 2, 2)\nplt.scatter(true_w[0], true_w[1], c='red', s=100, label='True weights', zorder=5)\nplt.scatter(w_learned[0], w_learned[1], c='blue', s=100, label='Learned weights', zorder=5)\nplt.xlabel('w₁')\nplt.ylabel('w₂')\nplt.title('Weight Convergence')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:**\n- SGD processes data in mini-batches, making it memory-efficient for large datasets\n- The gradient computation uses only a subset of the data, introducing stochasticity\n- Parameter updates follow the same gradient descent principle but with noisy gradients\n- The loss history shows convergence behavior, which can help diagnose training issues\n\n## 9.2 Backpropagation in Neural Networks\n\n### Mathematical Foundations\n\nBackpropagation is an algorithm for computing gradients in neural networks using the chain rule. For a network with parameters \\( \\theta \\), the gradient of the loss \\( L \\) is:\n\\[\n\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial a_L} \\frac{\\partial a_L}{\\partial z_L} \\frac{\\partial z_L}{\\partial a_{L-1}} \\cdots \\frac{\\partial a_1}{\\partial z_1} \\frac{\\partial z_1}{\\partial \\theta}\n\\]\nwhere \\( a_l \\) are activations and \\( z_l \\) are pre-activations.\n\n**Key Properties:**\n- Uses the chain rule to compute gradients efficiently\n- Computes gradients layer by layer, from output to input\n- Enables training of deep networks with many parameters\n- The algorithm is automatic and can be implemented using computational graphs\n\n**Relevance to ML:**\n- Essential for training deep neural networks\n- Enables automatic differentiation in modern frameworks\n- Understanding backpropagation helps debug and optimize networks\n\n### Python Implementation: Simple Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleNeuralNetwork:\n    \"\"\"\n    Simple neural network with backpropagation.\n    \n    Mathematical foundation:\n    - Forward pass: z_l = W_l a_{l-1} + b_l, a_l = σ(z_l)\n    - Loss: L = (1/m)∑(y_pred - y)²\n    - Backward pass: δ_l = (∂L/∂a_l) ⊙ σ'(z_l)\n    - Gradients: ∂L/∂W_l = δ_l a_{l-1}^T, ∂L/∂b_l = δ_l\n    \"\"\"\n    def __init__(self, input_size, hidden_size, output_size):\n        # Initialize weights and biases with small random values\n        # Xavier initialization: scale by 1/sqrt(input_size)\n        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n        self.b1 = np.zeros((1, hidden_size))\n        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n        self.b2 = np.zeros((1, output_size))\n    \n    def sigmoid(self, x):\n        \"\"\"\n        Sigmoid activation function: σ(x) = 1/(1 + e^(-x))\n        - Range: (0, 1)\n        - Derivative: σ'(x) = σ(x)(1 - σ(x))\n        \"\"\"\n        return 1 / (1 + np.exp(-x))\n    \n    def sigmoid_derivative(self, x):\n        \"\"\"\n        Derivative of sigmoid: σ'(x) = σ(x)(1 - σ(x))\n        - Used in backpropagation for gradient computation\n        \"\"\"\n        return x * (1 - x)\n    \n    def forward(self, X):\n        \"\"\"\n        Forward pass through the network.\n        - Computes activations for each layer\n        - Stores intermediate values for backpropagation\n        \"\"\"\n        # Layer 1: z1 = XW1 + b1, a1 = σ(z1)\n        self.z1 = X @ self.W1 + self.b1\n        self.a1 = self.sigmoid(self.z1)\n        \n        # Layer 2: z2 = a1W2 + b2, a2 = σ(z2)\n        self.z2 = self.a1 @ self.W2 + self.b2\n        self.a2 = self.sigmoid(self.z2)\n        \n        return self.a2\n    \n    def backward(self, X, y, learning_rate=0.1):\n        \"\"\"\n        Backward pass (backpropagation).\n        - Computes gradients using chain rule\n        - Updates parameters using gradient descent\n        \"\"\"\n        m = X.shape[0]  # Number of samples\n        \n        # Output layer gradients\n        # δ2 = ∂L/∂a2 = (a2 - y)\n        dz2 = self.a2 - y\n        \n        # ∂L/∂W2 = (1/m) a1^T δ2\n        dW2 = (1/m) * self.a1.T @ dz2\n        # ∂L/∂b2 = (1/m) ∑δ2\n        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n        \n        # Hidden layer gradients\n        # δ1 = δ2 W2^T ⊙ σ'(a1)\n        dz1 = dz2 @ self.W2.T * self.sigmoid_derivative(self.a1)\n        \n        # ∂L/∂W1 = (1/m) X^T δ1\n        dW1 = (1/m) * X.T @ dz1\n        # ∂L/∂b1 = (1/m) ∑δ1\n        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n        \n        # Update parameters using gradient descent\n        self.W2 -= learning_rate * dW2\n        self.b2 -= learning_rate * db2\n        self.W1 -= learning_rate * dW1\n        self.b1 -= learning_rate * db1\n    \n    def train(self, X, y, epochs=1000, learning_rate=0.1):\n        \"\"\"\n        Train the neural network.\n        - Performs forward and backward passes for each epoch\n        - Monitors loss for convergence\n        \"\"\"\n        losses = []\n        \n        for epoch in range(epochs):\n            # Forward pass\n            y_pred = self.forward(X)\n            \n            # Compute loss: MSE\n            loss = np.mean((y_pred - y)**2)\n            losses.append(loss)\n            \n            # Backward pass\n            self.backward(X, y, learning_rate)\n            \n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n        \n        return losses\n\n# XOR problem: non-linear classification task\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([[0], [1], [1], [0]])\n\n# Create and train network\nnn = SimpleNeuralNetwork(input_size=2, hidden_size=4, output_size=1)\nlosses = nn.train(X, y, epochs=10000, learning_rate=0.1)\n\n# Test predictions\npredictions = nn.forward(X)\nprint(\"\\nPredictions:\")\nfor i, (x, pred, true) in enumerate(zip(X, predictions, y)):\n    print(f\"Input: {x}, Prediction: {pred[0]:.3f}, True: {true[0]}\")\n\n# Plot training loss\nplt.figure(figsize=(10, 6))\nplt.plot(losses, 'b-', linewidth=2)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Neural Network Training Loss')\nplt.yscale('log')  # Log scale to see convergence\nplt.grid(True)\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:**\n- The forward pass computes activations layer by layer using the sigmoid activation function\n- The backward pass uses the chain rule to compute gradients efficiently\n- Gradients are computed for both weights and biases at each layer\n- The XOR problem demonstrates the network's ability to learn non-linear patterns\n- The loss plot shows convergence behavior, which is crucial for understanding training dynamics\n\n## 9.3 Loss Functions and Their Derivatives\n\n### Common Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse_loss(y_pred, y_true):\n    \"\"\"Mean Squared Error loss\"\"\"\n    return np.mean((y_pred - y_true)**2)\n\ndef mse_derivative(y_pred, y_true):\n    \"\"\"Derivative of MSE with respect to predictions\"\"\"\n    return 2 * (y_pred - y_true) / len(y_pred)\n\ndef cross_entropy_loss(y_pred, y_true):\n    \"\"\"Binary cross-entropy loss\"\"\"\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\ndef cross_entropy_derivative(y_pred, y_true):\n    \"\"\"Derivative of binary cross-entropy with respect to predictions\"\"\"\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    return (y_pred - y_true) / (y_pred * (1 - y_pred))\n\ndef huber_loss(y_pred, y_true, delta=1.0):\n    \"\"\"Huber loss - combines MSE and MAE\"\"\"\n    error = y_pred - y_true\n    abs_error = np.abs(error)\n    quadratic = np.minimum(abs_error, delta)\n    linear = abs_error - quadratic\n    return np.mean(0.5 * quadratic**2 + delta * linear)\n\ndef huber_derivative(y_pred, y_true, delta=1.0):\n    \"\"\"Derivative of Huber loss\"\"\"\n    error = y_pred - y_true\n    abs_error = np.abs(error)\n    return np.where(abs_error <= delta, error, delta * np.sign(error))\n\n# Demonstrate loss functions\ny_true = np.array([0, 1, 0, 1])\ny_pred = np.array([0.1, 0.8, 0.3, 0.9])\n\nprint(\"Loss Function Comparison:\")\nprint(f\"MSE Loss: {mse_loss(y_pred, y_true):.4f}\")\nprint(f\"Cross-Entropy Loss: {cross_entropy_loss(y_pred, y_true):.4f}\")\nprint(f\"Huber Loss: {huber_loss(y_pred, y_true):.4f}\")\n\nprint(\"\\nDerivatives:\")\nprint(f\"MSE Derivative: {mse_derivative(y_pred, y_true)}\")\nprint(f\"Cross-Entropy Derivative: {cross_entropy_derivative(y_pred, y_true)}\")\nprint(f\"Huber Derivative: {huber_derivative(y_pred, y_true)}\")\n\n# Visualize loss functions\nx_vals = np.linspace(0, 1, 100)\ny_true_fixed = 1.0\n\nmse_vals = [(x - y_true_fixed)**2 for x in x_vals]\nce_vals = [-y_true_fixed * np.log(x) - (1 - y_true_fixed) * np.log(1 - x) for x in x_vals]\nhuber_vals = [huber_loss(x, y_true_fixed) for x in x_vals]\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.plot(x_vals, mse_vals, 'b-', linewidth=2)\nplt.xlabel('Prediction')\nplt.ylabel('MSE Loss')\nplt.title('Mean Squared Error')\nplt.grid(True)\n\nplt.subplot(1, 3, 2)\nplt.plot(x_vals, ce_vals, 'r-', linewidth=2)\nplt.xlabel('Prediction')\nplt.ylabel('Cross-Entropy Loss')\nplt.title('Binary Cross-Entropy')\nplt.grid(True)\n\nplt.subplot(1, 3, 3)\nplt.plot(x_vals, huber_vals, 'g-', linewidth=2)\nplt.xlabel('Prediction')\nplt.ylabel('Huber Loss')\nplt.title('Huber Loss')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.4 Regularization and Gradient Clipping\n\n### L1 and L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def l1_regularization(weights, lambda_param=0.01):\n    \"\"\"L1 regularization (Lasso)\"\"\"\n    return lambda_param * np.sum(np.abs(weights))\n\ndef l2_regularization(weights, lambda_param=0.01):\n    \"\"\"L2 regularization (Ridge)\"\"\"\n    return lambda_param * np.sum(weights**2)\n\ndef l1_derivative(weights, lambda_param=0.01):\n    \"\"\"Derivative of L1 regularization\"\"\"\n    return lambda_param * np.sign(weights)\n\ndef l2_derivative(weights, lambda_param=0.01):\n    \"\"\"Derivative of L2 regularization\"\"\"\n    return 2 * lambda_param * weights\n\n# Demonstrate regularization effects\nweights = np.array([0.5, -0.3, 0.8, -0.1])\n\nprint(\"Regularization Comparison:\")\nprint(f\"L1 regularization: {l1_regularization(weights):.4f}\")\nprint(f\"L2 regularization: {l2_regularization(weights):.4f}\")\nprint(f\"L1 derivative: {l1_derivative(weights)}\")\nprint(f\"L2 derivative: {l2_derivative(weights)}\")\n\n# Visualize regularization effects\nx_vals = np.linspace(-2, 2, 100)\ny_vals = np.linspace(-2, 2, 100)\nX, Y = np.meshgrid(x_vals, y_vals)\n\n# Original loss function: f(x,y) = x² + y²\nZ_original = X**2 + Y**2\n\n# With L2 regularization\nlambda_param = 0.1\nZ_l2 = Z_original + lambda_param * (X**2 + Y**2)\n\nfig = plt.figure(figsize=(12, 5))\n\nax1 = fig.add_subplot(121, projection='3d')\nsurf1 = ax1.plot_surface(X, Y, Z_original, cmap='viridis')\nax1.set_title('Original Loss Function')\nax1.set_xlabel('w1')\nax1.set_ylabel('w2')\nax1.set_zlabel('Loss')\n\nax2 = fig.add_subplot(122, projection='3d')\nsurf2 = ax2.plot_surface(X, Y, Z_l2, cmap='viridis')\nax2.set_title('Loss with L2 Regularization')\nax2.set_xlabel('w1')\nax2.set_ylabel('w2')\nax2.set_zlabel('Loss')\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradient Clipping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_clipping(gradients, max_norm=1.0):\n    \"\"\"Clip gradients to prevent exploding gradients\"\"\"\n    norm = np.linalg.norm(gradients)\n    if norm > max_norm:\n        gradients = gradients * max_norm / norm\n    return gradients\n\n# Demonstrate gradient clipping\ndef demonstrate_gradient_clipping():\n    # Simulate large gradients\n    large_gradients = np.array([10.0, -15.0, 8.0, -12.0])\n    \n    print(\"Original gradients:\", large_gradients)\n    print(\"Original norm:\", np.linalg.norm(large_gradients))\n    \n    clipped_gradients = gradient_clipping(large_gradients, max_norm=5.0)\n    print(\"Clipped gradients:\", clipped_gradients)\n    print(\"Clipped norm:\", np.linalg.norm(clipped_gradients))\n\ndemonstrate_gradient_clipping()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.5 Advanced Optimization Algorithms\n\n### Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdamOptimizer:\n    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.m = None  # First moment\n        self.v = None  # Second moment\n        self.t = 0     # Time step\n    \n    def update(self, params, gradients):\n        if self.m is None:\n            self.m = np.zeros_like(params)\n            self.v = np.zeros_like(params)\n        \n        self.t += 1\n        \n        # Update biased first moment estimate\n        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients\n        \n        # Update biased second moment estimate\n        self.v = self.beta2 * self.v + (1 - self.beta2) * (gradients**2)\n        \n        # Compute bias-corrected first moment estimate\n        m_hat = self.m / (1 - self.beta1**self.t)\n        \n        # Compute bias-corrected second moment estimate\n        v_hat = self.v / (1 - self.beta2**self.t)\n        \n        # Update parameters\n        params -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n        \n        return params\n\n# Test Adam optimizer\ndef test_adam_optimizer():\n    # Rosenbrock function\n    def rosenbrock(x):\n        return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n    \n    def rosenbrock_gradient(x):\n        dx = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)\n        dy = 200 * (x[1] - x[0]**2)\n        return np.array([dx, dy])\n    \n    # Initialize\n    x = np.array([-1.0, -1.0])\n    optimizer = AdamOptimizer(learning_rate=0.01)\n    history = [x.copy()]\n    \n    # Optimize\n    for i in range(1000):\n        gradient = rosenbrock_gradient(x)\n        x = optimizer.update(x, gradient)\n        history.append(x.copy())\n    \n    history = np.array(history)\n    \n    print(f\"Final point: {x}\")\n    print(f\"Final value: {rosenbrock(x):.6f}\")\n    \n    # Visualize optimization path\n    x_vals = np.linspace(-2, 2, 100)\n    y_vals = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(x_vals, y_vals)\n    Z = rosenbrock([X, Y])\n    \n    plt.figure(figsize=(10, 8))\n    contour = plt.contour(X, Y, Z, levels=20)\n    plt.clabel(contour, inline=True, fontsize=8)\n    plt.plot(history[:, 0], history[:, 1], 'r-', linewidth=2, label='Adam optimization')\n    plt.scatter(history[0, 0], history[0, 1], c='red', s=100, label='Start')\n    plt.scatter(history[-1, 0], history[-1, 1], c='green', s=100, label='End')\n    plt.title('Adam Optimizer on Rosenbrock Function')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ntest_adam_optimizer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.6 Calculus in Deep Learning\n\n### Automatic Differentiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple automatic differentiation example\nclass AutoDiff:\n    def __init__(self, value, derivative=1.0):\n        self.value = value\n        self.derivative = derivative\n    \n    def __add__(self, other):\n        if isinstance(other, AutoDiff):\n            return AutoDiff(self.value + other.value, self.derivative + other.derivative)\n        else:\n            return AutoDiff(self.value + other, self.derivative)\n    \n    def __mul__(self, other):\n        if isinstance(other, AutoDiff):\n            return AutoDiff(self.value * other.value, \n                          self.derivative * other.value + self.value * other.derivative)\n        else:\n            return AutoDiff(self.value * other, self.derivative * other)\n    \n    def __pow__(self, power):\n        return AutoDiff(self.value**power, power * self.value**(power-1) * self.derivative)\n\n# Test automatic differentiation\ndef test_autodiff():\n    # f(x) = x² + 2x + 1\n    x = AutoDiff(3.0)  # x = 3, dx/dx = 1\n    f = x**2 + 2*x + 1\n    \n    print(f\"f(3) = {f.value}\")\n    print(f\"f'(3) = {f.derivative}\")\n    \n    # Verify with symbolic differentiation\n    # f'(x) = 2x + 2\n    # f'(3) = 2*3 + 2 = 8\n    print(f\"Expected f'(3) = 8\")\n\ntest_autodiff()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\n- **Gradient descent** is the foundation of most ML optimization algorithms\n- **Backpropagation** uses the chain rule to compute gradients in neural networks\n- **Loss functions** and their derivatives are crucial for model training\n- **Regularization** helps prevent overfitting and improves generalization\n- **Advanced optimizers** like Adam combine momentum and adaptive learning rates\n- **Automatic differentiation** enables efficient gradient computation in deep learning frameworks\n\n## Next Steps\n\nUnderstanding calculus in machine learning enables you to implement custom loss functions, design new optimization algorithms, and debug training issues. The next section covers numerical methods for when analytical solutions are not available."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}