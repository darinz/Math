{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Limits and Continuity\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-orange.svg)](https://matplotlib.org/)\n[![SymPy](https://img.shields.io/badge/SymPy-1.10+-purple.svg)](https://www.sympy.org/)\n\n## Introduction\n\nLimits and continuity form the foundation of calculus. Understanding these concepts is crucial for grasping derivatives, integrals, and their applications in machine learning and data science.\n\n### Why Limits Matter in AI/ML\n\nLimits are fundamental to calculus and form the foundation for derivatives and integrals. In AI/ML, understanding limits helps with:\n\n1. **Convergence Analysis**: Understanding whether optimization algorithms will converge to a solution\n2. **Optimization Algorithms**: Gradient descent, Newton's method, and other iterative methods rely on limit concepts\n3. **Model Behavior**: Understanding how models behave as parameters approach certain values\n4. **Numerical Stability**: Avoiding division by zero and other numerical issues\n5. **Asymptotic Analysis**: Understanding algorithm complexity and performance bounds\n\n### Mathematical Foundation\n\nThe concept of a limit formalizes the intuitive idea of \"approaching\" a value. Formally, we say that the limit of f(x) as x approaches a is L, written as:\n\n$$\\lim_{x \\to a} f(x) = L$$\n\nif for every ε > 0, there exists a δ > 0 such that whenever 0 < |x - a| < δ, we have |f(x) - L| < ε.\n\nThis ε-δ definition is the rigorous foundation that makes calculus mathematically sound.\n\n## 1.1 Definition of Limits\n\nA limit describes the behavior of a function as the input approaches a specific value. The limit captures what happens to the function's output as the input gets arbitrarily close to a target value, without necessarily reaching it.\n\n### Key Concepts:\n- **Approach**: The input gets closer and closer to a target value\n- **Behavior**: We observe what happens to the function's output\n- **Existence**: The limit may or may not exist\n- **Uniqueness**: If a limit exists, it is unique\n\n### Example: Removable Discontinuity\n\nConsider the function f(x) = (x² - 1)/(x - 1). At x = 1, the function is undefined (division by zero), but we can analyze its behavior as x approaches 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport sympy as sp\nfrom sympy import symbols, limit, simplify\n\n# Define a function with removable discontinuity\ndef f(x):\n    return (x**2 - 1) / (x - 1)\n\n# Calculate limit using SymPy\nx = sp.Symbol('x')\nlimit_expr = (x**2 - 1) / (x - 1)\n\n# Simplify the expression to understand the limit\nsimplified_expr = sp.simplify(limit_expr)\nprint(f\"Original expression: {limit_expr}\")\nprint(f\"Simplified expression: {simplified_expr}\")\n\n# Calculate the limit\nlimit_value = sp.limit(limit_expr, x, 1)\nprint(f\"Limit as x approaches 1: {limit_value}\")\n\n# Visualize the function with detailed analysis\nx_vals = np.linspace(0.5, 1.5, 1000)\ny_vals = [f(x) for x in x_vals if x != 1]\n\nplt.figure(figsize=(12, 8))\n\n# Main plot\nplt.subplot(2, 1, 1)\nplt.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = (x²-1)/(x-1)')\nplt.axhline(y=2, color='r', linestyle='--', linewidth=2, label='Limit = 2')\nplt.axvline(x=1, color='g', linestyle='--', linewidth=2, label='x = 1')\nplt.scatter(1, 2, color='red', s=100, zorder=5, label='Limit point')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Limit Example: (x²-1)/(x-1) as x → 1')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Zoomed view around x = 1\nplt.subplot(2, 1, 2)\nx_zoom = np.linspace(0.9, 1.1, 200)\ny_zoom = [f(x) for x in x_zoom if x != 1]\nplt.plot(x_zoom, y_zoom, 'b-', linewidth=2)\nplt.axhline(y=2, color='r', linestyle='--', linewidth=2)\nplt.axvline(x=1, color='g', linestyle='--', linewidth=2)\nplt.scatter(1, 2, color='red', s=100, zorder=5)\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Zoomed View Around x = 1')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate limit calculation numerically\nprint(\"\\nNumerical verification:\")\nfor h in [0.1, 0.01, 0.001, 0.0001]:\n    left_val = f(1 - h)\n    right_val = f(1 + h)\n    print(f\"f(1-{h}) = {left_val:.6f}, f(1+{h}) = {right_val:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mathematical Insight\n\nThe function f(x) = (x² - 1)/(x - 1) has a removable discontinuity at x = 1. We can factor the numerator:\n\n$$f(x) = \\frac{x^2 - 1}{x - 1} = \\frac{(x + 1)(x - 1)}{x - 1} = x + 1$$\n\nfor all x ≠ 1. Therefore, as x approaches 1, f(x) approaches 2. The discontinuity is \"removable\" because we can define f(1) = 2 to make the function continuous.\n\n## 1.2 One-Sided Limits\n\nOne-sided limits are crucial for understanding functions that behave differently from the left and right sides of a point. This is common in piecewise functions and functions with jumps or vertical asymptotes.\n\n### Mathematical Definition\n\n- **Left-hand limit**: $\\lim_{x \\to a^-} f(x) = L$ means f(x) approaches L as x approaches a from the left\n- **Right-hand limit**: $\\lim_{x \\to a^+} f(x) = L$ means f(x) approaches L as x approaches a from the right\n\nA two-sided limit exists if and only if both one-sided limits exist and are equal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-sided limits with detailed analysis\ndef step_function(x):\n    \"\"\"Heaviside step function: returns -1 for x < 0, 1 for x ≥ 0\"\"\"\n    return np.where(x < 0, -1, 1)\n\ndef sign_function(x):\n    \"\"\"Sign function: returns -1 for x < 0, 0 for x = 0, 1 for x > 0\"\"\"\n    return np.where(x < 0, -1, np.where(x > 0, 1, 0))\n\n# Create visualization\nx_vals = np.linspace(-2, 2, 1000)\ny_step = step_function(x_vals)\ny_sign = sign_function(x_vals)\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n\n# Step function\nax1.plot(x_vals, y_step, 'b-', linewidth=3, label='Step Function')\nax1.axvline(x=0, color='r', linestyle='--', linewidth=2, label='x = 0')\nax1.scatter(0, -1, color='red', s=100, zorder=5, label='Left limit = -1')\nax1.scatter(0, 1, color='green', s=100, zorder=5, label='Right limit = 1')\nax1.set_xlabel('x')\nax1.set_ylabel('f(x)')\nax1.set_title('One-Sided Limits: Step Function')\nax1.legend()\nax1.grid(True, alpha=0.3)\nax1.set_ylim(-1.5, 1.5)\n\n# Sign function\nax2.plot(x_vals, y_sign, 'b-', linewidth=3, label='Sign Function')\nax2.axvline(x=0, color='r', linestyle='--', linewidth=2, label='x = 0')\nax2.scatter(0, -1, color='red', s=100, zorder=5, label='Left limit = -1')\nax2.scatter(0, 1, color='green', s=100, zorder=5, label='Right limit = 1')\nax2.scatter(0, 0, color='purple', s=100, zorder=5, label='f(0) = 0')\nax2.set_xlabel('x')\nax2.set_ylabel('f(x)')\nax2.set_title('One-Sided Limits: Sign Function')\nax2.legend()\nax2.grid(True, alpha=0.3)\nax2.set_ylim(-1.5, 1.5)\n\nplt.tight_layout()\nplt.show()\n\n# Calculate one-sided limits using SymPy\nprint(\"One-sided limits analysis:\")\nprint(\"Step function:\")\nprint(f\"  Left limit: {sp.limit(sp.Piecewise((-1, x < 0), (1, True)), x, 0, dir='-')}\")\nprint(f\"  Right limit: {sp.limit(sp.Piecewise((-1, x < 0), (1, True)), x, 0, dir='+')}\")\nprint(f\"  Two-sided limit exists: {sp.limit(sp.Piecewise((-1, x < 0), (1, True)), x, 0) == sp.limit(sp.Piecewise((-1, x < 0), (1, True)), x, 0, dir='-') == sp.limit(sp.Piecewise((-1, x < 0), (1, True)), x, 0, dir='+')}\")\n\n# Numerical verification\nprint(\"\\nNumerical verification:\")\nfor h in [0.1, 0.01, 0.001]:\n    left_val = step_function(-h)\n    right_val = step_function(h)\n    print(f\"f(-{h}) = {left_val}, f({h}) = {right_val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Applications in AI/ML\n\nOne-sided limits are important in:\n- **Activation functions**: ReLU, Leaky ReLU, and other piecewise functions\n- **Loss functions**: Hinge loss, absolute error\n- **Optimization**: Understanding behavior at boundaries\n- **Neural networks**: Analyzing gradient flow through different activation regions\n\n## 1.3 Continuity\n\nContinuity is a fundamental property that ensures smooth behavior of functions. A function is continuous at a point if there are no jumps, breaks, or holes in its graph at that point.\n\n### Mathematical Definition\n\nA function f is continuous at a point a if:\n1. f(a) is defined\n2. $\\lim_{x \\to a} f(x)$ exists\n3. $\\lim_{x \\to a} f(x) = f(a)$\n\n### Types of Discontinuities\n\n1. **Removable discontinuity**: The limit exists but doesn't equal the function value\n2. **Jump discontinuity**: One-sided limits exist but are different\n3. **Infinite discontinuity**: The function approaches ±∞\n4. **Essential discontinuity**: The limit doesn't exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive continuity analysis\ndef continuous_func(x):\n    \"\"\"Continuous function: f(x) = x²\"\"\"\n    return x**2\n\ndef removable_discontinuity(x):\n    \"\"\"Function with removable discontinuity at x = 0\"\"\"\n    return np.where(x != 0, np.sin(x)/x, 1)\n\ndef jump_discontinuity(x):\n    \"\"\"Function with jump discontinuity at x = 0\"\"\"\n    return np.where(x < 0, x, x + 1)\n\ndef infinite_discontinuity(x):\n    \"\"\"Function with infinite discontinuity at x = 0\"\"\"\n    return np.where(x != 0, 1/x, 0)\n\n# Create comprehensive visualization\nx_vals = np.linspace(-2, 2, 1000)\ny1_vals = continuous_func(x_vals)\ny2_vals = removable_discontinuity(x_vals)\ny3_vals = jump_discontinuity(x_vals)\ny4_vals = infinite_discontinuity(x_vals)\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# Continuous function\nax1.plot(x_vals, y1_vals, 'b-', linewidth=2)\nax1.set_title('Continuous Function: f(x) = x²')\nax1.set_xlabel('x')\nax1.set_ylabel('f(x)')\nax1.grid(True, alpha=0.3)\n\n# Removable discontinuity\nax2.plot(x_vals, y2_vals, 'g-', linewidth=2)\nax2.scatter(0, 1, color='red', s=100, zorder=5, label='f(0) = 1')\nax2.set_title('Removable Discontinuity: f(x) = sin(x)/x')\nax2.set_xlabel('x')\nax2.set_ylabel('f(x)')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Jump discontinuity\nax3.plot(x_vals, y3_vals, 'r-', linewidth=2)\nax3.scatter(0, 0, color='red', s=100, zorder=5, label='Left limit = 0')\nax3.scatter(0, 1, color='blue', s=100, zorder=5, label='Right limit = 1')\nax3.set_title('Jump Discontinuity: f(x) = x for x < 0, x+1 for x ≥ 0')\nax3.set_xlabel('x')\nax3.set_ylabel('f(x)')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Infinite discontinuity\nax4.plot(x_vals, y4_vals, 'purple', linewidth=2)\nax4.set_title('Infinite Discontinuity: f(x) = 1/x')\nax4.set_xlabel('x')\nax4.set_ylabel('f(x)')\nax4.grid(True, alpha=0.3)\nax4.set_ylim(-5, 5)\n\nplt.tight_layout()\nplt.show()\n\n# Mathematical analysis of continuity\nprint(\"Continuity Analysis:\")\nprint(\"1. Continuous function: f(x) = x²\")\nprint(f\"   f(0) = {continuous_func(0)}\")\nprint(f\"   lim(x→0) f(x) = {sp.limit(x**2, x, 0)}\")\nprint(f\"   Continuous at x = 0: {continuous_func(0) == sp.limit(x**2, x, 0)}\")\n\nprint(\"\\n2. Removable discontinuity: f(x) = sin(x)/x\")\nprint(f\"   f(0) = 1 (defined)\")\nprint(f\"   lim(x→0) sin(x)/x = {sp.limit(sp.sin(x)/x, x, 0)}\")\nprint(f\"   Continuous at x = 0: {1 == sp.limit(sp.sin(x)/x, x, 0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Continuity in AI/ML Context\n\nContinuity is crucial for:\n- **Gradient-based optimization**: Continuous functions have well-defined gradients\n- **Neural network training**: Continuous activation functions ensure smooth gradient flow\n- **Loss functions**: Continuous loss functions allow for stable optimization\n- **Model interpretability**: Continuous models are easier to understand and debug\n\n## 1.4 Limits at Infinity\n\nLimits at infinity describe the long-term behavior of functions and are essential for understanding asymptotic behavior in algorithms and models.\n\n### Mathematical Significance\n\n- **Horizontal asymptotes**: Functions that approach a constant value\n- **Growth rates**: Understanding which functions grow faster\n- **Algorithm complexity**: Analyzing time and space complexity\n- **Model convergence**: Understanding training behavior over many epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive analysis of limits at infinity\ndef rational_func(x):\n    \"\"\"Rational function: (3x² + 2x + 1)/(x² + 1)\"\"\"\n    return (3*x**2 + 2*x + 1) / (x**2 + 1)\n\ndef exponential_func(x):\n    \"\"\"Exponential function: e^x\"\"\"\n    return np.exp(x)\n\ndef logarithmic_func(x):\n    \"\"\"Logarithmic function: ln(x)\"\"\"\n    return np.where(x > 0, np.log(x), np.nan)\n\ndef polynomial_func(x):\n    \"\"\"Polynomial function: x³ - 2x² + x\"\"\"\n    return x**3 - 2*x**2 + x\n\n# Create comprehensive visualization\nx_vals = np.linspace(0, 10, 1000)\ny1_vals = rational_func(x_vals)\ny2_vals = exponential_func(x_vals)\ny3_vals = logarithmic_func(x_vals)\ny4_vals = polynomial_func(x_vals)\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# Rational function\nax1.plot(x_vals, y1_vals, 'b-', linewidth=2)\nax1.axhline(y=3, color='r', linestyle='--', linewidth=2, label='Horizontal asymptote y = 3')\nax1.set_title('Rational Function: (3x²+2x+1)/(x²+1)')\nax1.set_xlabel('x')\nax1.set_ylabel('f(x)')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Exponential function\nax2.plot(x_vals, y2_vals, 'g-', linewidth=2)\nax2.set_title('Exponential Function: e^x')\nax2.set_xlabel('x')\nax2.set_ylabel('f(x)')\nax2.grid(True, alpha=0.3)\n\n# Logarithmic function\nax3.plot(x_vals, y3_vals, 'r-', linewidth=2)\nax3.set_title('Logarithmic Function: ln(x)')\nax3.set_xlabel('x')\nax3.set_ylabel('f(x)')\nax3.grid(True, alpha=0.3)\n\n# Polynomial function\nax4.plot(x_vals, y4_vals, 'purple', linewidth=2)\nax4.set_title('Polynomial Function: x³ - 2x² + x')\nax4.set_xlabel('x')\nax4.set_ylabel('f(x)')\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Calculate limits at infinity\nprint(\"Limits at Infinity Analysis:\")\nprint(\"1. Rational function: (3x² + 2x + 1)/(x² + 1)\")\nprint(f\"   lim(x→∞) = {sp.limit((3*x**2 + 2*x + 1) / (x**2 + 1), x, sp.oo)}\")\n\nprint(\"\\n2. Exponential function: e^x\")\nprint(f\"   lim(x→∞) = {sp.limit(sp.exp(x), x, sp.oo)}\")\n\nprint(\"\\n3. Logarithmic function: ln(x)\")\nprint(f\"   lim(x→∞) = {sp.limit(sp.log(x), x, sp.oo)}\")\n\nprint(\"\\n4. Polynomial function: x³ - 2x² + x\")\nprint(f\"   lim(x→∞) = {sp.limit(x**3 - 2*x**2 + x, x, sp.oo)}\")\n\n# Growth rate comparison\nprint(\"\\nGrowth Rate Comparison (numerical):\")\nx_large = 1000\nprint(f\"At x = {x_large}:\")\nprint(f\"  Rational: {rational_func(x_large):.2f}\")\nprint(f\"  Polynomial: {polynomial_func(x_large):.2e}\")\nprint(f\"  Exponential: {exponential_func(x_large):.2e}\")\nprint(f\"  Logarithmic: {logarithmic_func(x_large):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Asymptotic Analysis in AI/ML\n\nUnderstanding limits at infinity helps with:\n- **Algorithm complexity**: O(n), O(n²), O(2ⁿ) growth rates\n- **Model scaling**: How performance changes with data size\n- **Training convergence**: Long-term behavior of loss functions\n- **Memory usage**: Space complexity analysis\n\n## 1.5 Applications in AI/ML\n\n### Convergence Analysis\n\nConvergence analysis is fundamental to understanding whether optimization algorithms will find a solution and how quickly they will do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced convergence analysis\ndef gradient_descent_convergence(learning_rate=0.1, iterations=100, starting_point=2.0):\n    \"\"\"\n    Analyze gradient descent convergence for f(x) = x²\n    This function has a global minimum at x = 0\n    \"\"\"\n    x = starting_point\n    history = [x]\n    gradients = []\n    \n    for i in range(iterations):\n        # Gradient of f(x) = x² is f'(x) = 2x\n        gradient = 2 * x\n        gradients.append(gradient)\n        \n        # Update rule: x = x - α * ∇f(x)\n        x = x - learning_rate * gradient\n        history.append(x)\n    \n    return history, gradients\n\n# Analyze convergence with different learning rates\nlearning_rates = [0.01, 0.1, 0.5, 1.0]\niterations = 50\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\nfor i, lr in enumerate(learning_rates):\n    history, gradients = gradient_descent_convergence(learning_rate=lr, iterations=iterations)\n    \n    row = i // 2\n    col = i % 2\n    \n    ax = [ax1, ax2, ax3, ax4][i]\n    \n    # Plot convergence\n    ax.plot(range(iterations + 1), history, 'b-', linewidth=2, label=f'x values')\n    ax.axhline(y=0, color='r', linestyle='--', linewidth=2, label='Optimal value = 0')\n    ax.set_xlabel('Iteration')\n    ax.set_ylabel('x value')\n    ax.set_title(f'Gradient Descent (α = {lr})')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    print(f\"Learning rate {lr}: Final value = {history[-1]:.6f}\")\n\nplt.tight_layout()\nplt.show()\n\n# Mathematical analysis of convergence\nprint(\"\\nMathematical Analysis:\")\nprint(\"For f(x) = x²:\")\nprint(\"  - Gradient: f'(x) = 2x\")\nprint(\"  - Update rule: x_{n+1} = x_n - α * 2x_n = x_n(1 - 2α)\")\nprint(\"  - Convergence condition: |1 - 2α| < 1\")\nprint(\"  - Optimal learning rate: α = 0.5\")\n\nfor lr in learning_rates:\n    convergence_rate = abs(1 - 2 * lr)\n    print(f\"  α = {lr}: convergence rate = {convergence_rate:.3f} {'(converges)' if convergence_rate < 1 else '(diverges)'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function Behavior\n\nUnderstanding the behavior of loss functions is crucial for training neural networks and other machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive loss function analysis\ndef mse_loss(predictions, targets):\n    \"\"\"Mean Squared Error loss\"\"\"\n    return np.mean((predictions - targets)**2)\n\ndef cross_entropy_loss(predictions, targets):\n    \"\"\"Cross-entropy loss (simplified)\"\"\"\n    epsilon = 1e-15  # Avoid log(0)\n    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n    return -np.mean(targets * np.log(predictions) + (1 - targets) * np.log(1 - predictions))\n\ndef hinge_loss(predictions, targets):\n    \"\"\"Hinge loss for binary classification\"\"\"\n    return np.mean(np.maximum(0, 1 - targets * predictions))\n\n# Simulate different training scenarios\nepochs = 100\n\n# Scenario 1: Well-behaved convergence\nloss_history_1 = []\nfor epoch in range(epochs):\n    # Exponential decay with noise\n    base_loss = 10 * np.exp(-epoch / 20) + 0.1\n    noise = np.random.normal(0, 0.01)\n    loss = max(0.1, base_loss + noise)\n    loss_history_1.append(loss)\n\n# Scenario 2: Oscillating convergence\nloss_history_2 = []\nfor epoch in range(epochs):\n    base_loss = 10 * np.exp(-epoch / 30) + 0.2\n    oscillation = 0.1 * np.sin(epoch * 0.5)\n    loss = max(0.2, base_loss + oscillation)\n    loss_history_2.append(loss)\n\n# Scenario 3: Plateau then convergence\nloss_history_3 = []\nfor epoch in range(epochs):\n    if epoch < 30:\n        loss = 5.0  # Plateau\n    else:\n        loss = 5.0 * np.exp(-(epoch - 30) / 15) + 0.1\n    loss_history_3.append(loss)\n\n# Visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Loss curves\nax1.plot(range(epochs), loss_history_1, 'b-', linewidth=2, label='Smooth convergence')\nax1.plot(range(epochs), loss_history_2, 'g-', linewidth=2, label='Oscillating convergence')\nax1.plot(range(epochs), loss_history_3, 'r-', linewidth=2, label='Plateau then convergence')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Training Loss Behavior')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Log scale for better visualization\nax2.semilogy(range(epochs), loss_history_1, 'b-', linewidth=2, label='Smooth convergence')\nax2.semilogy(range(epochs), loss_history_2, 'g-', linewidth=2, label='Oscillating convergence')\nax2.semilogy(range(epochs), loss_history_3, 'r-', linewidth=2, label='Plateau then convergence')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Loss (log scale)')\nax2.set_title('Training Loss Behavior (Log Scale)')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Mathematical analysis\nprint(\"Loss Function Analysis:\")\nprint(\"1. Smooth convergence:\")\nprint(f\"   Initial loss: {loss_history_1[0]:.3f}\")\nprint(f\"   Final loss: {loss_history_1[-1]:.3f}\")\nprint(f\"   Convergence rate: {loss_history_1[0]/loss_history_1[-1]:.1f}x reduction\")\n\nprint(\"\\n2. Oscillating convergence:\")\nprint(f\"   Average loss (last 20 epochs): {np.mean(loss_history_2[-20:]):.3f}\")\nprint(f\"   Standard deviation (last 20 epochs): {np.std(loss_history_2[-20:]):.3f}\")\n\nprint(\"\\n3. Plateau then convergence:\")\nprint(f\"   Plateau duration: 30 epochs\")\nprint(f\"   Final convergence rate: {loss_history_3[30]/loss_history_3[-1]:.1f}x reduction\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numerical Stability\n\nNumerical stability is crucial for reliable computations in machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numerical stability examples\ndef unstable_division(x, y):\n    \"\"\"Demonstrate numerical instability in division\"\"\"\n    return x / y\n\ndef stable_division(x, y, epsilon=1e-15):\n    \"\"\"Stable division with protection against division by zero\"\"\"\n    return x / (y + epsilon)\n\n# Test numerical stability\nx_vals = np.linspace(0.1, 1.0, 10)\ny_vals = np.linspace(1e-15, 1e-10, 10)\n\nprint(\"Numerical Stability Analysis:\")\nprint(\"Testing division near zero:\")\n\nfor x, y in zip(x_vals, y_vals):\n    try:\n        unstable_result = unstable_division(x, y)\n        stable_result = stable_division(x, y)\n        print(f\"x={x:.1e}, y={y:.1e}: unstable={unstable_result:.6f}, stable={stable_result:.6f}\")\n    except:\n        print(f\"x={x:.1e}, y={y:.1e}: unstable=ERROR, stable={stable_division(x, y):.6f}\")\n\n# Demonstrate limit concepts in numerical computation\nprint(\"\\nLimit concepts in numerical computation:\")\nprint(\"Computing lim(x→0) sin(x)/x numerically:\")\n\nfor h in [0.1, 0.01, 0.001, 0.0001, 0.00001]:\n    result = np.sin(h) / h\n    print(f\"h = {h:.1e}: sin({h})/{h} = {result:.10f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nLimits and continuity provide the mathematical foundation for understanding:\n\n1. **Function behavior**: How functions behave near specific points and at infinity\n2. **Convergence**: Whether sequences and algorithms converge to solutions\n3. **Numerical stability**: Avoiding computational errors in machine learning\n4. **Optimization**: Understanding gradient-based optimization methods\n5. **Model training**: Analyzing loss function behavior during training\n\nThese concepts are essential for developing robust, efficient, and mathematically sound machine learning algorithms and understanding their behavior in practice."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}