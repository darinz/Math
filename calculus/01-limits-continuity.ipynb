{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limits and Continuity\n",
    "\n",
    "[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)\n",
    "[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n",
    "[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-orange.svg)](https://matplotlib.org/)\n",
    "[![SymPy](https://img.shields.io/badge/SymPy-1.10+-purple.svg)](https://www.sympy.org/)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Limits and continuity form the foundation of calculus. Understanding these concepts is crucial for grasping derivatives, integrals, and their applications in machine learning and data science.\n",
    "\n",
    "### Why Limits Matter in AI/ML\n",
    "\n",
    "Limits are fundamental to calculus and form the foundation for derivatives and integrals. In AI/ML, understanding limits helps with:\n",
    "\n",
    "1. **Convergence Analysis**: Understanding whether optimization algorithms will converge to a solution\n",
    "2. **Optimization Algorithms**: Gradient descent, Newton's method, and other iterative methods rely on limit concepts\n",
    "3. **Model Behavior**: Understanding how models behave as parameters approach certain values\n",
    "4. **Numerical Stability**: Avoiding division by zero and other numerical issues\n",
    "5. **Asymptotic Analysis**: Understanding algorithm complexity and performance bounds\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "The concept of a limit formalizes the intuitive idea of \"approaching\" a value. Formally, we say that the limit of f(x) as x approaches a is L, written as:\n",
    "\n",
    "$$\\lim_{x \\to a} f(x) = L$$\n",
    "\n",
    "if for every ε > 0, there exists a δ > 0 such that whenever 0 < |x - a| < δ, we have |f(x) - L| < ε.\n",
    "\n",
    "This ε-δ definition is the rigorous foundation that makes calculus mathematically sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Definition of Limits\n",
    "\n",
    "A limit describes the behavior of a function as the input approaches a specific value. The limit captures what happens to the function's output as the input gets arbitrarily close to a target value, without necessarily reaching it.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Approach**: The input gets closer and closer to a target value\n",
    "- **Behavior**: We observe what happens to the function's output\n",
    "- **Existence**: The limit may or may not exist\n",
    "- **Uniqueness**: If a limit exists, it is unique\n",
    "\n",
    "### Example: Removable Discontinuity\n",
    "\n",
    "Consider the function f(x) = (x² - 1)/(x - 1). At x = 1, the function is undefined (division by zero), but we can analyze its behavior as x approaches 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "from sympy import symbols, limit, simplify\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Define a function with removable discontinuity\n",
    "def f(x):\n",
    "    return (x**2 - 1) / (x - 1)\n",
    "\n",
    "# Calculate limit using SymPy\n",
    "x = sp.Symbol('x')\n",
    "limit_expr = (x**2 - 1) / (x - 1)\n",
    "\n",
    "# Simplify the expression to understand the limit\n",
    "simplified_expr = sp.simplify(limit_expr)\n",
    "print(f\"Original expression: {limit_expr}\")\n",
    "print(f\"Simplified expression: {simplified_expr}\")\n",
    "\n",
    "# Calculate the limit\n",
    "limit_value = sp.limit(limit_expr, x, 1)\n",
    "print(f\"Limit as x approaches 1: {limit_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the function with detailed analysis\n",
    "x_vals = np.linspace(0.5, 1.5, 1000)\n",
    "y_vals = [f(x) for x in x_vals if x != 1]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Main plot\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = (x²-1)/(x-1)')\n",
    "plt.axhline(y=2, color='r', linestyle='--', linewidth=2, label='Limit = 2')\n",
    "plt.axvline(x=1, color='g', linestyle='--', linewidth=2, label='x = 1')\n",
    "plt.scatter(1, 2, color='red', s=100, zorder=5, label='Limit point')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Limit Example: (x²-1)/(x-1) as x → 1')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Zoomed view around x = 1\n",
    "plt.subplot(2, 1, 2)\n",
    "x_zoom = np.linspace(0.9, 1.1, 200)\n",
    "y_zoom = [f(x) for x in x_zoom if x != 1]\n",
    "plt.plot(x_zoom, y_zoom, 'b-', linewidth=2)\n",
    "plt.axhline(y=2, color='r', linestyle='--', linewidth=2)\n",
    "plt.axvline(x=1, color='g', linestyle='--', linewidth=2)\n",
    "plt.scatter(1, 2, color='red', s=100, zorder=5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Zoomed View Around x = 1')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate limit calculation numerically\n",
    "print(\"Numerical verification:\")\n",
    "for h in [0.1, 0.01, 0.001, 0.0001]:\n",
    "    left_val = f(1 - h)\n",
    "    right_val = f(1 + h)\n",
    "    print(f\"f(1-{h}) = {left_val:.6f}, f(1+{h}) = {right_val:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Insight\n",
    "\n",
    "The function f(x) = (x² - 1)/(x - 1) has a removable discontinuity at x = 1. We can factor the numerator:\n",
    "\n",
    "$$f(x) = \\frac{x^2 - 1}{x - 1} = \\frac{(x + 1)(x - 1)}{x - 1} = x + 1$$\n",
    "\n",
    "for all x ≠ 1. Therefore, as x approaches 1, f(x) approaches 2. The discontinuity is \"removable\" because we can define f(1) = 2 to make the function continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 One-Sided Limits\n",
    "\n",
    "One-sided limits are crucial for understanding functions that behave differently from the left and right sides of a point. This is common in piecewise functions and functions with jumps or vertical asymptotes.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "- **Left-hand limit**: $\\lim_{x \\to a^-} f(x) = L$ means f(x) approaches L as x approaches a from the left\n",
    "- **Right-hand limit**: $\\lim_{x \\to a^+} f(x) = L$ means f(x) approaches L as x approaches a from the right\n",
    "\n",
    "A two-sided limit exists if and only if both one-sided limits exist and are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-sided limits with detailed analysis\n",
    "def step_function(x):\n",
    "    \"\"\"Heaviside step function: returns -1 for x < 0, 1 for x ≥ 0\"\"\"\n",
    "    return np.where(x < 0, -1, 1)\n",
    "\n",
    "def sign_function(x):\n",
    "    \"\"\"Sign function: returns -1 for x < 0, 0 for x = 0, 1 for x > 0\"\"\"\n",
    "    return np.where(x < 0, -1, np.where(x > 0, 1, 0))\n",
    "\n",
    "# Create visualization\n",
    "x_vals = np.linspace(-2, 2, 1000)\n",
    "y_step = step_function(x_vals)\n",
    "y_sign = sign_function(x_vals)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Step function\n",
    "ax1.plot(x_vals, y_step, 'b-', linewidth=3, label='Step Function')\n",
    "ax1.axvline(x=0, color='r', linestyle='--', linewidth=2, label='x = 0')\n",
    "ax1.scatter(0, -1, color='red', s=100, zorder=5, label='Left limit = -1')\n",
    "ax1.scatter(0, 1, color='green', s=100, zorder=5, label='Right limit = 1')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('f(x)')\n",
    "ax1.set_title('One-Sided Limits: Step Function')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(-1.5, 1.5)\n",
    "\n",
    "# Sign function\n",
    "ax2.plot(x_vals, y_sign, 'b-', linewidth=3, label='Sign Function')\n",
    "ax2.axvline(x=0, color='r', linestyle='--', linewidth=2, label='x = 0')\n",
    "ax2.scatter(0, -1, color='red', s=100, zorder=5, label='Left limit = -1')\n",
    "ax2.scatter(0, 1, color='green', s=100, zorder=5, label='Right limit = 1')\n",
    "ax2.scatter(0, 0, color='purple', s=100, zorder=5, label='f(0) = 0')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('f(x)')\n",
    "ax2.set_title('One-Sided Limits: Sign Function')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(-1.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate one-sided limits using SymPy\n",
    "print(\"One-sided limits analysis:\")\n",
    "print(\"Step function:\")\n",
    "print(f\"  Left limit: {sp.limit(sp.Piecewise((-1, x < 0), (1, True)), x, 0, dir='-')}\")\n",
    "print(f\"  Right limit: {sp.limit(sp.Piecewise((-1, x < 0), (1, True)), x, 0, dir='+')}\")\n",
    "print(f\"  Two-sided limit exists: {sp.limit(sp.Piecewise((-1, x < 0), (1, True)), x, 0) == sp.limit(sp.Piecewise((-1, x < 0), (1, True)), x, 0, dir='-') == sp.limit(sp.Piecewise((-1, x < 0), (1, True)), x, 0, dir='+')}\")\n",
    "\n",
    "# Numerical verification\n",
    "print(\"\\nNumerical verification:\")\n",
    "for h in [0.1, 0.01, 0.001]:\n",
    "    left_val = step_function(-h)\n",
    "    right_val = step_function(h)\n",
    "    print(f\"f(-{h}) = {left_val}, f({h}) = {right_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications in AI/ML\n",
    "\n",
    "One-sided limits are important in:\n",
    "- **Activation functions**: ReLU, Leaky ReLU, and other piecewise functions\n",
    "- **Loss functions**: Hinge loss, absolute error\n",
    "- **Optimization**: Understanding behavior at boundaries\n",
    "- **Neural networks**: Analyzing gradient flow through different activation regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Continuity\n",
    "\n",
    "Continuity is a fundamental property that ensures smooth behavior of functions. A function is continuous at a point if there are no jumps, breaks, or holes in its graph at that point.\n",
    "\n",
    "### Definition of Continuity\n",
    "\n",
    "A function f(x) is continuous at a point x = a if:\n",
    "\n",
    "1. f(a) is defined\n",
    "2. $\\lim_{x \\to a} f(x)$ exists\n",
    "3. $\\lim_{x \\to a} f(x) = f(a)$\n",
    "\n",
    "### Types of Discontinuities\n",
    "\n",
    "1. **Removable Discontinuity**: The limit exists but f(a) is not defined or not equal to the limit\n",
    "2. **Jump Discontinuity**: The left and right limits exist but are different\n",
    "3. **Infinite Discontinuity**: The limit approaches ±∞\n",
    "4. **Essential Discontinuity**: The limit does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of different types of discontinuities\n",
    "def removable_discontinuity(x):\n",
    "    \"\"\"f(x) = (x²-1)/(x-1) with removable discontinuity at x=1\"\"\"\n",
    "    return np.where(x != 1, (x**2 - 1) / (x - 1), 2)\n",
    "\n",
    "def jump_discontinuity(x):\n",
    "    \"\"\"f(x) = floor(x) with jump discontinuities at integers\"\"\"\n",
    "    return np.floor(x)\n",
    "\n",
    "def infinite_discontinuity(x):\n",
    "    \"\"\"f(x) = 1/x with infinite discontinuity at x=0\"\"\"\n",
    "    return np.where(x != 0, 1/x, np.nan)\n",
    "\n",
    "# Create visualization\n",
    "x_vals = np.linspace(-2, 4, 1000)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Removable discontinuity\n",
    "y_removable = removable_discontinuity(x_vals)\n",
    "ax1.plot(x_vals, y_removable, 'b-', linewidth=2, label='f(x) = (x²-1)/(x-1)')\n",
    "ax1.scatter(1, 2, color='red', s=100, zorder=5, label='Removable discontinuity')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('f(x)')\n",
    "ax1.set_title('Removable Discontinuity')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Jump discontinuity\n",
    "y_jump = jump_discontinuity(x_vals)\n",
    "ax2.plot(x_vals, y_jump, 'b-', linewidth=2, label='f(x) = floor(x)')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('f(x)')\n",
    "ax2.set_title('Jump Discontinuity')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Infinite discontinuity\n",
    "x_infinite = np.linspace(-2, 2, 1000)\n",
    "y_infinite = infinite_discontinuity(x_infinite)\n",
    "ax3.plot(x_infinite, y_infinite, 'b-', linewidth=2, label='f(x) = 1/x')\n",
    "ax3.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Vertical asymptote')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('f(x)')\n",
    "ax3.set_title('Infinite Discontinuity')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim(-5, 5)\n",
    "\n",
    "# Continuous function for comparison\n",
    "y_continuous = x_vals**2\n",
    "ax4.plot(x_vals, y_continuous, 'g-', linewidth=2, label='f(x) = x²')\n",
    "ax4.set_xlabel('x')\n",
    "ax4.set_ylabel('f(x)')\n",
    "ax4.set_title('Continuous Function')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuity in AI/ML Applications\n",
    "\n",
    "Continuity is crucial in machine learning for:\n",
    "\n",
    "1. **Activation Functions**: Most activation functions (sigmoid, tanh, ReLU) are continuous\n",
    "2. **Loss Functions**: Continuous loss functions enable gradient-based optimization\n",
    "3. **Optimization**: Continuous functions have well-defined derivatives\n",
    "4. **Model Stability**: Continuous functions provide stable predictions\n",
    "\n",
    "### Example: Activation Functions\n",
    "\n",
    "Let's examine the continuity of common activation functions used in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common activation functions and their continuity\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Visualize activation functions\n",
    "x_vals = np.linspace(-5, 5, 1000)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sigmoid\n",
    "ax1.plot(x_vals, sigmoid(x_vals), 'b-', linewidth=2, label='Sigmoid')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('f(x)')\n",
    "ax1.set_title('Sigmoid Activation Function')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ReLU\n",
    "ax2.plot(x_vals, relu(x_vals), 'r-', linewidth=2, label='ReLU')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('f(x)')\n",
    "ax2.set_title('ReLU Activation Function')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Tanh\n",
    "ax3.plot(x_vals, tanh(x_vals), 'g-', linewidth=2, label='Tanh')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('f(x)')\n",
    "ax3.set_title('Tanh Activation Function')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Leaky ReLU\n",
    "ax4.plot(x_vals, leaky_relu(x_vals), 'm-', linewidth=2, label='Leaky ReLU')\n",
    "ax4.set_xlabel('x')\n",
    "ax4.set_ylabel('f(x)')\n",
    "ax4.set_title('Leaky ReLU Activation Function')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check continuity at x = 0 for ReLU and Leaky ReLU\n",
    "print(\"Continuity analysis at x = 0:\")\n",
    "print(f\"ReLU: f(0) = {relu(0)}, lim(x→0⁻) = {relu(-0.001)}, lim(x→0⁺) = {relu(0.001)}\")\n",
    "print(f\"Leaky ReLU: f(0) = {leaky_relu(0)}, lim(x→0⁻) = {leaky_relu(-0.001)}, lim(x→0⁺) = {leaky_relu(0.001)}\")\n",
    "print(\"Both functions are continuous at x = 0!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
