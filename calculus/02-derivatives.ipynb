{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Derivatives and Differentiation\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-orange.svg)](https://matplotlib.org/)\n[![SymPy](https://img.shields.io/badge/SymPy-1.10+-purple.svg)](https://www.sympy.org/)\n\n## Introduction\n\nDerivatives measure how a function changes as its input changes. This concept is fundamental to optimization, which is central to machine learning algorithms.\n\n### Why Derivatives Matter in AI/ML\n\nDerivatives are the cornerstone of optimization in machine learning. They enable us to:\n\n1. **Find Optimal Solutions**: Locate minima and maxima of loss functions\n2. **Gradient-Based Optimization**: Implement algorithms like gradient descent, Adam, and RMSprop\n3. **Neural Network Training**: Compute gradients for backpropagation\n4. **Model Sensitivity Analysis**: Understand how changes in inputs affect outputs\n5. **Feature Importance**: Determine which features contribute most to predictions\n\n### Mathematical Foundation\n\nThe derivative of a function f(x) at a point x₀ is defined as the limit of the difference quotient:\n\n$$f'(x_0) = \\lim_{h \\to 0} \\frac{f(x_0 + h) - f(x_0)}{h}$$\n\nThis represents the instantaneous rate of change of the function at that point, or geometrically, the slope of the tangent line to the curve at that point.\n\n### Physical and Geometric Interpretation\n\n- **Rate of Change**: How quickly the function value changes with respect to the input\n- **Slope**: The steepness of the function at a particular point\n- **Velocity**: In physics, the derivative of position with respect to time\n- **Sensitivity**: How sensitive the output is to small changes in the input\n\n## 2.1 Definition of Derivatives\n\nThe derivative captures the instantaneous rate of change of a function. It's the foundation for understanding how functions behave locally and globally.\n\n### Key Concepts:\n\n- **Instantaneous Rate**: The rate of change at a specific point, not over an interval\n- **Tangent Line**: The line that best approximates the function near a point\n- **Local Linearity**: Functions behave approximately linearly near any point\n- **Differentiability**: A function is differentiable if its derivative exists at a point\n\n### Example: Understanding the Difference Quotient\n\nThe difference quotient $\\frac{f(x + h) - f(x)}{h}$ represents the average rate of change over the interval [x, x+h]. As h approaches 0, this becomes the instantaneous rate of change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport sympy as sp\nfrom scipy.misc import derivative\n\n# Define a function for detailed analysis\ndef f(x):\n    \"\"\"Function to analyze: f(x) = x²\"\"\"\n    return x**2\n\n# Numerical derivative using finite differences with detailed analysis\ndef numerical_derivative_detailed(f, x, h=1e-7):\n    \"\"\"\n    Compute numerical derivative using central difference method\n    This is more accurate than forward difference for most functions\n    \"\"\"\n    return (f(x + h) - f(x - h)) / (2 * h)\n\ndef forward_difference(f, x, h):\n    \"\"\"Forward difference approximation\"\"\"\n    return (f(x + h) - f(x)) / h\n\ndef backward_difference(f, x, h):\n    \"\"\"Backward difference approximation\"\"\"\n    return (f(x) - f(x - h)) / h\n\ndef central_difference(f, x, h):\n    \"\"\"Central difference approximation (most accurate)\"\"\"\n    return (f(x + h) - f(x - h)) / (2 * h)\n\n# Symbolic derivative using SymPy\nx = sp.Symbol('x')\nf_sym = x**2\nf_prime = sp.diff(f_sym, x)\nprint(f\"Symbolic derivative of x²: {f_prime}\")\n\n# Compare different numerical methods\nx_test = 2.0\nh_values = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n\nprint(\"\\nNumerical Derivative Comparison at x = 2:\")\nprint(\"h\\t\\tForward\\t\\tBackward\\tCentral\\t\\tExact\")\nprint(\"-\" * 70)\n\nfor h in h_values:\n    forward = forward_difference(f, x_test, h)\n    backward = backward_difference(f, x_test, h)\n    central = central_difference(f, x_test, h)\n    exact = 2 * x_test  # f'(x) = 2x\n    \n    print(f\"{h:.1e}\\t{forward:.6f}\\t{backward:.6f}\\t{central:.6f}\\t{exact:.6f}\")\n\n# Visualize the derivative concept\nx_vals = np.linspace(-2, 2, 100)\ny_vals = [f(x) for x in x_vals]\n\n# Create comprehensive visualization\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# Original function\nax1.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = x²')\nax1.set_xlabel('x')\nax1.set_ylabel('f(x)')\nax1.set_title('Original Function')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Tangent line at x = 1\nx_tangent = 1.0\ny_tangent = f(x_tangent)\nslope = 2 * x_tangent  # f'(x) = 2x\ntangent_x = np.linspace(x_tangent - 0.5, x_tangent + 0.5, 100)\ntangent_y = y_tangent + slope * (tangent_x - x_tangent)\n\nax2.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = x²')\nax2.plot(tangent_x, tangent_y, 'r--', linewidth=2, label=f'Tangent at x=1 (slope={slope})')\nax2.scatter(x_tangent, y_tangent, color='red', s=100, zorder=5)\nax2.set_xlabel('x')\nax2.set_ylabel('f(x)')\nax2.set_title('Tangent Line')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Secant lines approaching tangent\nx_point = 1.0\nh_values_vis = [0.5, 0.2, 0.1]\ncolors = ['green', 'orange', 'purple']\n\nax3.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = x²')\nax3.scatter(x_point, f(x_point), color='red', s=100, zorder=5, label='Point of interest')\n\nfor i, h in enumerate(h_values_vis):\n    x1, x2 = x_point, x_point + h\n    y1, y2 = f(x1), f(x2)\n    slope_secant = (y2 - y1) / (x2 - x1)\n    \n    # Plot secant line\n    secant_x = np.linspace(x1 - 0.3, x2 + 0.3, 100)\n    secant_y = y1 + slope_secant * (secant_x - x1)\n    ax3.plot(secant_x, secant_y, '--', color=colors[i], linewidth=2, \n             label=f'Secant h={h} (slope={slope_secant:.2f})')\n\nax3.set_xlabel('x')\nax3.set_ylabel('f(x)')\nax3.set_title('Secant Lines Approaching Tangent')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Derivatives comparison\nnumerical_derivatives = [numerical_derivative_detailed(f, x) for x in x_vals]\nsymbolic_derivatives = [2*x for x in x_vals]  # f'(x) = 2x\n\nax4.plot(x_vals, numerical_derivatives, 'r-', linewidth=2, label='Numerical f\\'(x)')\nax4.plot(x_vals, symbolic_derivatives, 'g--', linewidth=2, label='Symbolic f\\'(x) = 2x')\nax4.set_xlabel('x')\nax4.set_ylabel('f\\'(x)')\nax4.set_title('Derivatives Comparison')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Error analysis\nprint(\"\\nError Analysis:\")\nprint(\"Comparing numerical vs symbolic derivatives:\")\nx_test_points = [0.5, 1.0, 1.5, 2.0]\nh_optimal = 1e-7\n\nfor x_test in x_test_points:\n    numerical_val = numerical_derivative_detailed(f, x_test, h_optimal)\n    symbolic_val = 2 * x_test\n    error = abs(numerical_val - symbolic_val)\n    print(f\"x = {x_test}: numerical = {numerical_val:.10f}, symbolic = {symbolic_val:.10f}, error = {error:.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mathematical Insight: Why the Central Difference is Better\n\nThe central difference formula $\\frac{f(x+h) - f(x-h)}{2h}$ is generally more accurate than the forward difference $\\frac{f(x+h) - f(x)}{h}$ because:\n\n1. **Taylor Series Analysis**: The central difference eliminates the first-order error term\n2. **Symmetry**: It uses points equally spaced on both sides of x\n3. **Error Reduction**: The truncation error is O(h²) instead of O(h)\n\n### Applications in Machine Learning\n\nUnderstanding derivatives is crucial for:\n- **Gradient Descent**: Finding the direction of steepest descent\n- **Backpropagation**: Computing gradients through neural networks\n- **Optimization**: Locating minima of loss functions\n- **Sensitivity Analysis**: Understanding model behavior\n\n## 2.2 Basic Differentiation Rules\n\nUnderstanding differentiation rules is essential for computing derivatives efficiently. These rules form the foundation for automatic differentiation systems used in modern machine learning frameworks.\n\n### Fundamental Rules\n\nThe basic differentiation rules provide systematic methods for computing derivatives of common function combinations:\n\n1. **Power Rule**: $\\frac{d}{dx}(x^n) = nx^{n-1}$\n2. **Constant Rule**: $\\frac{d}{dx}(c) = 0$\n3. **Constant Multiple Rule**: $\\frac{d}{dx}(cf(x)) = c\\frac{d}{dx}f(x)$\n4. **Sum Rule**: $\\frac{d}{dx}(f(x) + g(x)) = \\frac{d}{dx}f(x) + \\frac{d}{dx}g(x)$\n5. **Product Rule**: $\\frac{d}{dx}(f(x)g(x)) = f(x)\\frac{d}{dx}g(x) + g(x)\\frac{d}{dx}f(x)$\n6. **Quotient Rule**: $\\frac{d}{dx}(\\frac{f(x)}{g(x)}) = \\frac{g(x)\\frac{d}{dx}f(x) - f(x)\\frac{d}{dx}g(x)}{g(x)^2}$\n\n### Mathematical Justification\n\nThese rules can be derived from the limit definition of the derivative. For example, the product rule follows from:\n\n$$\\frac{d}{dx}(f(x)g(x)) = \\lim_{h \\to 0} \\frac{f(x+h)g(x+h) - f(x)g(x)}{h}$$\n\nBy adding and subtracting $f(x+h)g(x)$ in the numerator and using the limit properties, we obtain the product rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive demonstration of differentiation rules\ndef demonstrate_rules_comprehensive():\n    x = sp.Symbol('x')\n    \n    print(\"=== BASIC DIFFERENTIATION RULES ===\\n\")\n    \n    # Power rule: d/dx(x^n) = n*x^(n-1)\n    print(\"1. POWER RULE\")\n    power_examples = [x**2, x**3, x**0.5, x**(-1), x**(-2)]\n    for expr in power_examples:\n        deriv = sp.diff(expr, x)\n        print(f\"   d/dx({expr}) = {deriv}\")\n    \n    # Constant rule: d/dx(c) = 0\n    print(\"\\n2. CONSTANT RULE\")\n    const_expr = 5\n    const_deriv = sp.diff(const_expr, x)\n    print(f\"   d/dx({const_expr}) = {const_deriv}\")\n    \n    # Constant multiple rule: d/dx(cf(x)) = c*d/dx(f(x))\n    print(\"\\n3. CONSTANT MULTIPLE RULE\")\n    const_mult_expr = 3 * x**2\n    const_mult_deriv = sp.diff(const_mult_expr, x)\n    print(f\"   d/dx({const_mult_expr}) = {const_mult_deriv}\")\n    \n    # Sum rule: d/dx(f + g) = d/dx(f) + d/dx(g)\n    print(\"\\n4. SUM RULE\")\n    sum_expr = x**2 + 3*x + 1\n    sum_deriv = sp.diff(sum_expr, x)\n    print(f\"   d/dx({sum_expr}) = {sum_deriv}\")\n    \n    # Product rule: d/dx(f*g) = f*dg + g*df\n    print(\"\\n5. PRODUCT RULE\")\n    product_expr = x**2 * sp.sin(x)\n    product_deriv = sp.diff(product_expr, x)\n    print(f\"   d/dx({product_expr}) = {product_deriv}\")\n    \n    # Quotient rule: d/dx(f/g) = (g*df - f*dg)/g²\n    print(\"\\n6. QUOTIENT RULE\")\n    quotient_expr = x**2 / (x + 1)\n    quotient_deriv = sp.diff(quotient_expr, x)\n    print(f\"   d/dx({quotient_expr}) = {quotient_deriv}\")\n    \n    # Chain rule preview\n    print(\"\\n7. CHAIN RULE PREVIEW\")\n    chain_expr = sp.sin(x**2)\n    chain_deriv = sp.diff(chain_expr, x)\n    print(f\"   d/dx({chain_expr}) = {chain_deriv}\")\n\ndemonstrate_rules_comprehensive()\n\n# Visualize the rules with practical examples\ndef visualize_differentiation_rules():\n    x_vals = np.linspace(-2, 2, 1000)\n    \n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Power rule visualization\n    y1 = x_vals**2\n    dy1 = 2 * x_vals\n    \n    ax1.plot(x_vals, y1, 'b-', linewidth=2, label='f(x) = x²')\n    ax1.plot(x_vals, dy1, 'r--', linewidth=2, label='f\\'(x) = 2x')\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_title('Power Rule: d/dx(x²) = 2x')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Product rule visualization\n    y2 = x_vals**2 * np.sin(x_vals)\n    dy2 = 2 * x_vals * np.sin(x_vals) + x_vals**2 * np.cos(x_vals)\n    \n    ax2.plot(x_vals, y2, 'b-', linewidth=2, label='f(x) = x²sin(x)')\n    ax2.plot(x_vals, dy2, 'r--', linewidth=2, label='f\\'(x) = 2xsin(x) + x²cos(x)')\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.set_title('Product Rule')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Quotient rule visualization\n    y3 = x_vals**2 / (x_vals + 1)\n    dy3 = (2 * x_vals * (x_vals + 1) - x_vals**2) / (x_vals + 1)**2\n    \n    # Handle division by zero\n    mask = np.abs(x_vals + 1) > 1e-10\n    ax3.plot(x_vals[mask], y3[mask], 'b-', linewidth=2, label='f(x) = x²/(x+1)')\n    ax3.plot(x_vals[mask], dy3[mask], 'r--', linewidth=2, label='f\\'(x)')\n    ax3.set_xlabel('x')\n    ax3.set_ylabel('y')\n    ax3.set_title('Quotient Rule')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    # Sum rule visualization\n    y4 = x_vals**2 + 3*x_vals + 1\n    dy4 = 2*x_vals + 3\n    \n    ax4.plot(x_vals, y4, 'b-', linewidth=2, label='f(x) = x² + 3x + 1')\n    ax4.plot(x_vals, dy4, 'r--', linewidth=2, label='f\\'(x) = 2x + 3')\n    ax4.set_xlabel('x')\n    ax4.set_ylabel('y')\n    ax4.set_title('Sum Rule')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_differentiation_rules()\n\n# Advanced examples with error analysis\ndef advanced_differentiation_examples():\n    x = sp.Symbol('x')\n    \n    print(\"\\n=== ADVANCED DIFFERENTIATION EXAMPLES ===\\n\")\n    \n    # Exponential and logarithmic functions\n    print(\"1. EXPONENTIAL AND LOGARITHMIC FUNCTIONS\")\n    exp_expr = sp.exp(x)\n    log_expr = sp.log(x)\n    print(f\"   d/dx(e^x) = {sp.diff(exp_expr, x)}\")\n    print(f\"   d/dx(ln(x)) = {sp.diff(log_expr, x)}\")\n    \n    # Trigonometric functions\n    print(\"\\n2. TRIGONOMETRIC FUNCTIONS\")\n    trig_expr = sp.sin(x)\n    trig_expr2 = sp.cos(x)\n    trig_expr3 = sp.tan(x)\n    print(f\"   d/dx(sin(x)) = {sp.diff(trig_expr, x)}\")\n    print(f\"   d/dx(cos(x)) = {sp.diff(trig_expr2, x)}\")\n    print(f\"   d/dx(tan(x)) = {sp.diff(trig_expr3, x)}\")\n    \n    # Complex combinations\n    print(\"\\n3. COMPLEX COMBINATIONS\")\n    complex_expr = sp.exp(x**2) * sp.sin(x)\n    complex_deriv = sp.diff(complex_expr, x)\n    print(f\"   d/dx(e^(x²) * sin(x)) = {complex_deriv}\")\n    \n    # Implicit differentiation example\n    print(\"\\n4. IMPLICIT DIFFERENTIATION\")\n    y = sp.Symbol('y')\n    implicit_expr = x**2 + y**2 - 1  # Circle: x² + y² = 1\n    # Solve for dy/dx: 2x + 2y*dy/dx = 0\n    dy_dx = -x/y\n    print(f\"   For x² + y² = 1: dy/dx = {dy_dx}\")\n\nadvanced_differentiation_examples()\n\n# Numerical verification of rules\ndef numerical_verification():\n    print(\"\\n=== NUMERICAL VERIFICATION ===\\n\")\n    \n    def f1(x): return x**2  # Power rule\n    def f2(x): return x**2 * np.sin(x)  # Product rule\n    def f3(x): return x**2 / (x + 1)  # Quotient rule\n    \n    x_test = 1.5\n    h = 1e-7\n    \n    # Test power rule\n    numerical_power = numerical_derivative_detailed(f1, x_test, h)\n    symbolic_power = 2 * x_test\n    print(f\"Power rule at x = {x_test}:\")\n    print(f\"  Numerical: {numerical_power:.6f}\")\n    print(f\"  Symbolic:  {symbolic_power:.6f}\")\n    print(f\"  Error:     {abs(numerical_power - symbolic_power):.2e}\")\n    \n    # Test product rule\n    numerical_product = numerical_derivative_detailed(f2, x_test, h)\n    symbolic_product = 2 * x_test * np.sin(x_test) + x_test**2 * np.cos(x_test)\n    print(f\"\\nProduct rule at x = {x_test}:\")\n    print(f\"  Numerical: {numerical_product:.6f}\")\n    print(f\"  Symbolic:  {symbolic_product:.6f}\")\n    print(f\"  Error:     {abs(numerical_product - symbolic_product):.2e}\")\n\nnumerical_verification()\n\n### Applications in Machine Learning\n\nThese differentiation rules are fundamental to:\n\n1. **Automatic Differentiation**: Modern frameworks like TensorFlow and PyTorch use these rules to compute gradients automatically\n2. **Loss Function Derivatives**: Computing gradients of complex loss functions\n3. **Activation Function Derivatives**: Derivatives of sigmoid, ReLU, tanh, etc.\n4. **Optimization Algorithms**: All gradient-based optimization methods rely on these rules\n\n## 2.3 Chain Rule\n\nThe chain rule is one of the most important differentiation rules, especially in machine learning. It allows us to compute derivatives of composite functions, which are ubiquitous in neural networks and other complex models.\n\n### Mathematical Foundation\n\nThe chain rule states that if y = f(u) and u = g(x), then:\n\n$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} = f'(g(x)) \\cdot g'(x)$$\n\nThis can be extended to longer chains: if y = f(g(h(x))), then:\n\n$$\\frac{dy}{dx} = f'(g(h(x))) \\cdot g'(h(x)) \\cdot h'(x)$$\n\n### Why the Chain Rule is Crucial in ML\n\n1. **Neural Networks**: Each layer applies a function to the output of the previous layer\n2. **Activation Functions**: Functions like sigmoid, tanh, ReLU are applied to linear combinations\n3. **Loss Functions**: Often involve multiple nested functions\n4. **Backpropagation**: The entire algorithm is based on the chain rule\n\n### Intuitive Understanding\n\nThink of the chain rule as \"how much does the final output change when I change the input?\" This involves:\n- How much the intermediate variable changes with respect to the input\n- How much the final output changes with respect to the intermediate variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python\n# Comprehensive chain rule demonstration\ndef chain_rule_example_comprehensive():\n    x = sp.Symbol('x')\n    \n    print(\"=== CHAIN RULE EXAMPLES ===\\n\")\n    \n    # Basic chain rule examples\n    print(\"1. BASIC CHAIN RULE EXAMPLES\")\n    \n    # f(x) = sin(x²)\n    f_expr1 = sp.sin(x**2)\n    f_deriv1 = sp.diff(f_expr1, x)\n    print(f\"   f(x) = sin(x²)\")\n    print(f\"   f'(x) = {f_deriv1}\")\n    print(f\"   Explanation: d/dx(sin(x²)) = cos(x²) * d/dx(x²) = cos(x²) * 2x\")\n    \n    # f(x) = e^(x²)\n    f_expr2 = sp.exp(x**2)\n    f_deriv2 = sp.diff(f_expr2, x)\n    print(f\"\\n   f(x) = e^(x²)\")\n    print(f\"   f'(x) = {f_deriv2}\")\n    print(f\"   Explanation: d/dx(e^(x²)) = e^(x²) * d/dx(x²) = e^(x²) * 2x\")\n    \n    # f(x) = ln(x² + 1)\n    f_expr3 = sp.log(x**2 + 1)\n    f_deriv3 = sp.diff(f_expr3, x)\n    print(f\"\\n   f(x) = ln(x² + 1)\")\n    print(f\"   f'(x) = {f_deriv3}\")\n    print(f\"   Explanation: d/dx(ln(x² + 1)) = 1/(x² + 1) * d/dx(x² + 1) = 2x/(x² + 1)\")\n    \n    # Multiple chain rule applications\n    print(\"\\n2. MULTIPLE CHAIN RULE APPLICATIONS\")\n    \n    # f(x) = sin(e^(x²))\n    f_expr4 = sp.sin(sp.exp(x**2))\n    f_deriv4 = sp.diff(f_expr4, x)\n    print(f\"   f(x) = sin(e^(x²))\")\n    print(f\"   f'(x) = {f_deriv4}\")\n    \n    # f(x) = e^(sin(x²))\n    f_expr5 = sp.exp(sp.sin(x**2))\n    f_deriv5 = sp.diff(f_expr5, x)\n    print(f\"\\n   f(x) = e^(sin(x²))\")\n    print(f\"   f'(x) = {f_deriv5}\")\n    \n    # Practical ML examples\n    print(\"\\n3. MACHINE LEARNING EXAMPLES\")\n    \n    # Sigmoid function: σ(x) = 1/(1 + e^(-x))\n    sigmoid_expr = 1 / (1 + sp.exp(-x))\n    sigmoid_deriv = sp.diff(sigmoid_expr, x)\n    print(f\"   Sigmoid: σ(x) = 1/(1 + e^(-x))\")\n    print(f\"   σ'(x) = {sigmoid_deriv}\")\n    print(f\"   Simplified: σ'(x) = σ(x)(1 - σ(x))\")\n    \n    # Tanh function: tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))\n    tanh_expr = sp.tanh(x)\n    tanh_deriv = sp.diff(tanh_expr, x)\n    print(f\"\\n   Tanh: tanh(x)\")\n    print(f\"   tanh'(x) = {tanh_deriv}\")\n    print(f\"   Simplified: tanh'(x) = 1 - tanh²(x)\")\n\nchain_rule_example_comprehensive()\n\n# Visualize chain rule with detailed analysis\ndef visualize_chain_rule():\n    x_vals = np.linspace(-2, 2, 1000)\n    \n    # Example: f(x) = sin(x²)\n    def f(x): return np.sin(x**2)\n    def f_prime(x): return 2 * x * np.cos(x**2)\n    \n    # Break down the chain: u = x², f = sin(u)\n    def u(x): return x**2\n    def u_prime(x): return 2 * x\n    def f_of_u(u_val): return np.sin(u_val)\n    def f_of_u_prime(u_val): return np.cos(u_val)\n    \n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Original function and its derivative\n    ax1.plot(x_vals, f(x_vals), 'b-', linewidth=2, label='f(x) = sin(x²)')\n    ax1.plot(x_vals, f_prime(x_vals), 'r--', linewidth=2, label='f\\'(x) = 2x*cos(x²)')\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_title('Composite Function and Its Derivative')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Inner function u(x) = x²\n    ax2.plot(x_vals, u(x_vals), 'g-', linewidth=2, label='u(x) = x²')\n    ax2.plot(x_vals, u_prime(x_vals), 'g--', linewidth=2, label='u\\'(x) = 2x')\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('u')\n    ax2.set_title('Inner Function u(x) = x²')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Outer function f(u) = sin(u)\n    u_vals = np.linspace(0, 4, 1000)  # u = x² ranges from 0 to 4 for x in [-2, 2]\n    ax3.plot(u_vals, f_of_u(u_vals), 'm-', linewidth=2, label='f(u) = sin(u)')\n    ax3.plot(u_vals, f_of_u_prime(u_vals), 'm--', linewidth=2, label='f\\'(u) = cos(u)')\n    ax3.set_xlabel('u')\n    ax3.set_ylabel('f(u)')\n    ax3.set_title('Outer Function f(u) = sin(u)')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    # Chain rule verification\n    x_test = np.linspace(-2, 2, 100)\n    chain_rule_result = f_of_u_prime(u(x_test)) * u_prime(x_test)\n    direct_derivative = f_prime(x_test)\n    \n    ax4.plot(x_test, chain_rule_result, 'b-', linewidth=2, label='Chain rule: f\\'(u)*u\\'(x)')\n    ax4.plot(x_test, direct_derivative, 'r--', linewidth=2, label='Direct derivative')\n    ax4.set_xlabel('x')\n    ax4.set_ylabel('Derivative')\n    ax4.set_title('Chain Rule Verification')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Numerical verification\n    print(\"\\nChain Rule Numerical Verification:\")\n    x_test_point = 1.0\n    h = 1e-7\n    \n    # Direct derivative\n    direct_deriv = numerical_derivative_detailed(f, x_test_point, h)\n    \n    # Chain rule calculation\n    u_val = u(x_test_point)\n    du_dx = u_prime(x_test_point)\n    df_du = f_of_u_prime(u_val)\n    chain_rule_deriv = df_du * du_dx\n    \n    print(f\"At x = {x_test_point}:\")\n    print(f\"  Direct derivative: {direct_deriv:.6f}\")\n    print(f\"  Chain rule: f'(u)*u'(x) = {df_du:.6f} * {du_dx:.6f} = {chain_rule_deriv:.6f}\")\n    print(f\"  Error: {abs(direct_deriv - chain_rule_deriv):.2e}\")\n\nvisualize_chain_rule()\n\n# Backpropagation example using chain rule\ndef backpropagation_example():\n    print(\"\\n=== BACKPROPAGATION EXAMPLE ===\\n\")\n    \n    # Simple neural network: y = σ(w*x + b)\n    # where σ is the sigmoid function\n    \n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    \n    def sigmoid_derivative(x):\n        s = sigmoid(x)\n        return s * (1 - s)\n    \n    # Forward pass\n    def forward_pass(x, w, b):\n        z = w * x + b  # Linear combination\n        y = sigmoid(z)  # Activation function\n        return y, z\n    \n    # Backward pass using chain rule\n    def backward_pass(x, w, b, target):\n        y, z = forward_pass(x, w, b)\n        \n        # Loss: L = (y - target)²\n        loss = (y - target)**2\n        \n        # Chain rule for gradients:\n        # dL/dw = dL/dy * dy/dz * dz/dw\n        # dL/db = dL/dy * dy/dz * dz/db\n        \n        dL_dy = 2 * (y - target)  # dL/dy\n        dy_dz = sigmoid_derivative(z)  # dy/dz\n        dz_dw = x  # dz/dw\n        dz_db = 1  # dz/db\n        \n        dL_dw = dL_dy * dy_dz * dz_dw\n        dL_db = dL_dy * dy_dz * dz_db\n        \n        return dL_dw, dL_db, loss\n    \n    # Test the backpropagation\n    x = 2.0\n    w = 1.0\n    b = 0.5\n    target = 0.8\n    \n    print(f\"Input: x = {x}\")\n    print(f\"Weight: w = {w}\")\n    print(f\"Bias: b = {b}\")\n    print(f\"Target: {target}\")\n    \n    y, z = forward_pass(x, w, b)\n    print(f\"Forward pass: z = {z:.4f}, y = {y:.4f}\")\n    \n    dL_dw, dL_db, loss = backward_pass(x, w, b, target)\n    print(f\"Loss: {loss:.4f}\")\n    print(f\"Gradients: dL/dw = {dL_dw:.4f}, dL/db = {dL_db:.4f}\")\n    \n    # Verify with numerical gradients\n    def loss_function(w, b):\n        y, _ = forward_pass(x, w, b)\n        return (y - target)**2\n    \n    h = 1e-7\n    numerical_dw = (loss_function(w + h, b) - loss_function(w - h, b)) / (2 * h)\n    numerical_db = (loss_function(w, b + h) - loss_function(w, b - h)) / (2 * h)\n    \n    print(f\"\\nNumerical verification:\")\n    print(f\"  Numerical dL/dw: {numerical_dw:.4f}\")\n    print(f\"  Analytical dL/dw: {dL_dw:.4f}\")\n    print(f\"  Error: {abs(numerical_dw - dL_dw):.2e}\")\n    \n    print(f\"  Numerical dL/db: {numerical_db:.4f}\")\n    print(f\"  Analytical dL/db: {dL_db:.4f}\")\n    print(f\"  Error: {abs(numerical_db - dL_db):.2e}\")\n\nbackpropagation_example()\n\n# Advanced chain rule examples\ndef advanced_chain_rule_examples():\n    x = sp.Symbol('x')\n    \n    print(\"\\n=== ADVANCED CHAIN RULE EXAMPLES ===\\n\")\n    \n    # Multiple nested functions\n    print(\"1. MULTIPLE NESTED FUNCTIONS\")\n    complex_expr = sp.sin(sp.exp(sp.log(x**2 + 1)))\n    complex_deriv = sp.diff(complex_expr, x)\n    print(f\"   f(x) = sin(e^(ln(x² + 1)))\")\n    print(f\"   f'(x) = {complex_deriv}\")\n    \n    # Parametric functions\n    print(\"\\n2. PARAMETRIC FUNCTIONS\")\n    t = sp.Symbol('t')\n    x_param = sp.cos(t)\n    y_param = sp.sin(t)\n    \n    # dy/dx = (dy/dt)/(dx/dt)\n    dy_dt = sp.diff(y_param, t)\n    dx_dt = sp.diff(x_param, t)\n    dy_dx = dy_dt / dx_dt\n    print(f\"   x = cos(t), y = sin(t)\")\n    print(f\"   dy/dx = {dy_dx}\")\n    \n    # Implicit differentiation with chain rule\n    print(\"\\n3. IMPLICIT DIFFERENTIATION\")\n    y = sp.Symbol('y')\n    implicit_expr = x**2 + y**2 - 1\n    # Differentiate both sides with respect to x\n    # 2x + 2y*dy/dx = 0\n    # dy/dx = -x/y\n    print(f\"   For x² + y² = 1:\")\n    print(f\"   dy/dx = -x/y (using chain rule on y²)\")\n\nadvanced_chain_rule_examples()\n\n### Applications in Machine Learning\n\nThe chain rule is fundamental to:\n\n1. **Backpropagation**: The core algorithm for training neural networks\n2. **Automatic Differentiation**: Modern frameworks compute gradients using the chain rule\n3. **Activation Functions**: Derivatives of sigmoid, tanh, ReLU, etc.\n4. **Loss Functions**: Gradients of complex loss functions\n5. **Optimization**: All gradient-based optimization methods\n\n## 2.4 Partial Derivatives\n\nPartial derivatives are essential for functions of multiple variables, which are ubiquitous in machine learning. They measure how a function changes with respect to one variable while holding all others constant.\n\n### Mathematical Foundation\n\nFor a function f(x₁, x₂, ..., xₙ), the partial derivative with respect to xᵢ is:\n\n$$\\frac{\\partial f}{\\partial x_i} = \\lim_{h \\to 0} \\frac{f(x_1, ..., x_i + h, ..., x_n) - f(x_1, ..., x_i, ..., x_n)}{h}$$\n\nThis represents the rate of change of f in the direction of the i-th variable.\n\n### Why Partial Derivatives Matter in ML\n\n1. **Multivariable Functions**: Most ML models have multiple parameters\n2. **Gradient Computation**: Gradients are vectors of partial derivatives\n3. **Optimization**: Gradient descent requires partial derivatives\n4. **Feature Sensitivity**: Understanding how each feature affects the output\n\n### Geometric Interpretation\n\n- **Directional Sensitivity**: How much the function changes in a specific direction\n- **Tangent Planes**: Partial derivatives define the tangent plane to a surface\n- **Gradient Vector**: The vector of all partial derivatives points in the direction of steepest ascent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive partial derivatives demonstration\ndef partial_derivatives_comprehensive():\n    x, y = sp.symbols('x y')\n    \n    print(\"=== PARTIAL DERIVATIVES EXAMPLES ===\\n\")\n    \n    # Basic examples\n    print(\"1. BASIC EXAMPLES\")\n    \n    # f(x,y) = x² + y²\n    f_expr1 = x**2 + y**2\n    df_dx1 = sp.diff(f_expr1, x)\n    df_dy1 = sp.diff(f_expr1, y)\n    \n    print(f\"   f(x,y) = x² + y²\")\n    print(f\"   ∂f/∂x = {df_dx1}\")\n    print(f\"   ∂f/∂y = {df_dy1}\")\n    print(f\"   Gradient ∇f = [{df_dx1}, {df_dy1}]\")\n    \n    # f(x,y) = x*y + sin(x)\n    f_expr2 = x*y + sp.sin(x)\n    df_dx2 = sp.diff(f_expr2, x)\n    df_dy2 = sp.diff(f_expr2, y)\n    \n    print(f\"\\n   f(x,y) = x*y + sin(x)\")\n    print(f\"   ∂f/∂x = {df_dx2}\")\n    print(f\"   ∂f/∂y = {df_dy2}\")\n    print(f\"   Gradient ∇f = [{df_dx2}, {df_dy2}]\")\n    \n    # More complex examples\n    print(\"\\n2. COMPLEX EXAMPLES\")\n    \n    # f(x,y) = e^(x² + y²)\n    f_expr3 = sp.exp(x**2 + y**2)\n    df_dx3 = sp.diff(f_expr3, x)\n    df_dy3 = sp.diff(f_expr3, y)\n    \n    print(f\"   f(x,y) = e^(x² + y²)\")\n    print(f\"   ∂f/∂x = {df_dx3}\")\n    print(f\"   ∂f/∂y = {df_dy3}\")\n    \n    # f(x,y) = sin(x*y) / (x² + y²)\n    f_expr4 = sp.sin(x*y) / (x**2 + y**2)\n    df_dx4 = sp.diff(f_expr4, x)\n    df_dy4 = sp.diff(f_expr4, y)\n    \n    print(f\"\\n   f(x,y) = sin(x*y) / (x² + y²)\")\n    print(f\"   ∂f/∂x = {df_dx4}\")\n    print(f\"   ∂f/∂y = {df_dy4}\")\n    \n    # ML-specific examples\n    print(\"\\n3. MACHINE LEARNING EXAMPLES\")\n    \n    # Linear regression: f(w,b) = (wx + b - y)²\n    w, b = sp.symbols('w b')\n    x_data, y_data = sp.symbols('x_data y_data')\n    loss_expr = (w * x_data + b - y_data)**2\n    \n    df_dw = sp.diff(loss_expr, w)\n    df_db = sp.diff(loss_expr, b)\n    \n    print(f\"   Loss function: L(w,b) = (wx + b - y)²\")\n    print(f\"   ∂L/∂w = {df_dw}\")\n    print(f\"   ∂L/∂b = {df_db}\")\n    \n    # Logistic regression: f(w,b) = -y*log(σ(wx + b)) - (1-y)*log(1-σ(wx + b))\n    sigma_expr = 1 / (1 + sp.exp(-(w * x_data + b)))\n    log_loss_expr = -y_data * sp.log(sigma_expr) - (1 - y_data) * sp.log(1 - sigma_expr)\n    \n    df_dw_log = sp.diff(log_loss_expr, w)\n    df_db_log = sp.diff(log_loss_expr, b)\n    \n    print(f\"\\n   Logistic loss: L(w,b) = -y*log(σ(wx + b)) - (1-y)*log(1-σ(wx + b))\")\n    print(f\"   ∂L/∂w = {df_dw_log}\")\n    print(f\"   ∂L/∂b = {df_db_log}\")\n\npartial_derivatives_comprehensive()\n\n# Visualize partial derivatives\ndef visualize_partial_derivatives():\n    from mpl_toolkits.mplot3d import Axes3D\n    \n    def f_3d(x, y):\n        return x**2 + y**2\n    \n    def df_dx(x, y):\n        return 2 * x\n    \n    def df_dy(x, y):\n        return 2 * y\n    \n    # Create 3D surface\n    x = np.linspace(-2, 2, 50)\n    y = np.linspace(-2, 2, 50)\n    X, Y = np.meshgrid(x, y)\n    Z = f_3d(X, Y)\n    \n    fig = plt.figure(figsize=(20, 10))\n    \n    # 3D surface plot\n    ax1 = fig.add_subplot(2, 3, 1, projection='3d')\n    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_zlabel('f(x,y)')\n    ax1.set_title('Surface: f(x,y) = x² + y²')\n    \n    # Contour plot with gradient vectors\n    ax2 = fig.add_subplot(2, 3, 2)\n    contour = ax2.contour(X, Y, Z, levels=10)\n    ax2.clabel(contour, inline=True, fontsize=8)\n    \n    # Add gradient vectors\n    skip = 5\n    ax2.quiver(X[::skip, ::skip], Y[::skip, ::skip], \n               df_dx(X[::skip, ::skip], Y[::skip, ::skip]),\n               df_dy(X[::skip, ::skip], Y[::skip, ::skip]),\n               color='red', alpha=0.6)\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.set_title('Contour Plot with Gradient Vectors')\n    ax2.grid(True, alpha=0.3)\n    \n    # ∂f/∂x as a function of x (y fixed)\n    ax3 = fig.add_subplot(2, 3, 3)\n    y_fixed = 1.0\n    x_vals = np.linspace(-2, 2, 100)\n    df_dx_vals = df_dx(x_vals, y_fixed)\n    ax3.plot(x_vals, df_dx_vals, 'b-', linewidth=2)\n    ax3.set_xlabel('x')\n    ax3.set_ylabel('∂f/∂x')\n    ax3.set_title(f'∂f/∂x (y = {y_fixed})')\n    ax3.grid(True, alpha=0.3)\n    \n    # ∂f/∂y as a function of y (x fixed)\n    ax4 = fig.add_subplot(2, 3, 4)\n    x_fixed = 1.0\n    y_vals = np.linspace(-2, 2, 100)\n    df_dy_vals = df_dy(x_fixed, y_vals)\n    ax4.plot(y_vals, df_dy_vals, 'r-', linewidth=2)\n    ax4.set_xlabel('y')\n    ax4.set_ylabel('∂f/∂y')\n    ax4.set_title(f'∂f/∂y (x = {x_fixed})')\n    ax4.grid(True, alpha=0.3)\n    \n    # Gradient magnitude\n    ax5 = fig.add_subplot(2, 3, 5)\n    grad_magnitude = np.sqrt(df_dx(X, Y)**2 + df_dy(X, Y)**2)\n    grad_contour = ax5.contour(X, Y, grad_magnitude, levels=10)\n    ax5.clabel(grad_contour, inline=True, fontsize=8)\n    ax5.set_xlabel('x')\n    ax5.set_ylabel('y')\n    ax5.set_title('Gradient Magnitude |∇f|')\n    ax5.grid(True, alpha=0.3)\n    \n    # Direction of steepest ascent\n    ax6 = fig.add_subplot(2, 3, 6)\n    # Normalize gradient vectors\n    grad_norm = np.sqrt(df_dx(X, Y)**2 + df_dy(X, Y)**2)\n    df_dx_norm = df_dx(X, Y) / (grad_norm + 1e-10)\n    df_dy_norm = df_dy(X, Y) / (grad_norm + 1e-10)\n    \n    ax6.quiver(X[::skip, ::skip], Y[::skip, ::skip], \n               df_dx_norm[::skip, ::skip], df_dy_norm[::skip, ::skip],\n               color='purple', alpha=0.6)\n    ax6.set_xlabel('x')\n    ax6.set_ylabel('y')\n    ax6.set_title('Direction of Steepest Ascent')\n    ax6.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_partial_derivatives()\n\n# Numerical computation of partial derivatives\ndef numerical_partial_derivatives():\n    def f(x, y):\n        return x**2 + y**2\n    \n    def numerical_partial_x(f, x, y, h=1e-7):\n        return (f(x + h, y) - f(x - h, y)) / (2 * h)\n    \n    def numerical_partial_y(f, x, y, h=1e-7):\n        return (f(x, y + h) - f(x, y - h)) / (2 * h)\n    \n    print(\"\\n=== NUMERICAL PARTIAL DERIVATIVES ===\\n\")\n    \n    test_points = [(0, 0), (1, 1), (-1, 2), (0.5, -0.5)]\n    \n    for x_test, y_test in test_points:\n        # Numerical derivatives\n        num_df_dx = numerical_partial_x(f, x_test, y_test)\n        num_df_dy = numerical_partial_y(f, x_test, y_test)\n        \n        # Analytical derivatives\n        ana_df_dx = 2 * x_test\n        ana_df_dy = 2 * y_test\n        \n        print(f\"At point ({x_test}, {y_test}):\")\n        print(f\"  ∂f/∂x: numerical = {num_df_dx:.6f}, analytical = {ana_df_dx:.6f}\")\n        print(f\"  ∂f/∂y: numerical = {num_df_dy:.6f}, analytical = {ana_df_dy:.6f}\")\n        print(f\"  Errors: dx = {abs(num_df_dx - ana_df_dx):.2e}, dy = {abs(num_df_dy - ana_df_dy):.2e}\")\n        print()\n\nnumerical_partial_derivatives()\n\n# Gradient descent with partial derivatives\ndef gradient_descent_2d():\n    print(\"\\n=== GRADIENT DESCENT IN 2D ===\\n\")\n    \n    def f(x, y):\n        return x**2 + y**2\n    \n    def df_dx(x, y):\n        return 2 * x\n    \n    def df_dy(x, y):\n        return 2 * y\n    \n    def gradient_descent_2d_algorithm(f, df_dx, df_dy, x0, y0, learning_rate=0.1, iterations=50):\n        x, y = x0, y0\n        history = [(x, y)]\n        \n        for i in range(iterations):\n            # Compute gradients\n            grad_x = df_dx(x, y)\n            grad_y = df_dy(x, y)\n            \n            # Update parameters\n            x = x - learning_rate * grad_x\n            y = y - learning_rate * grad_y\n            \n            history.append((x, y))\n        \n        return history\n    \n    # Run gradient descent\n    x0, y0 = 2.0, 1.5\n    history = gradient_descent_2d_algorithm(f, df_dx, df_dy, x0, y0, learning_rate=0.1, iterations=20)\n    \n    # Extract coordinates for plotting\n    x_coords = [point[0] for point in history]\n    y_coords = [point[1] for point in history]\n    z_coords = [point[2] for point in history]\n    \n    # Create visualization\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Contour plot with optimization path\n    x = np.linspace(-2.5, 2.5, 100)\n    y = np.linspace(-2.5, 2.5, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = f(X, Y)\n    \n    ax1.contour(X, Y, Z, levels=20)\n    ax1.plot(x_coords, y_coords, 'r-o', linewidth=2, markersize=4, label='Optimization path')\n    ax1.scatter(x_coords[0], y_coords[0], color='green', s=100, label='Start')\n    ax1.scatter(x_coords[-1], y_coords[-1], color='red', s=100, label='End')\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_title('Gradient Descent Path')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Function value over iterations\n    ax2.plot(range(len(z_coords)), z_coords, 'b-o', linewidth=2, markersize=4)\n    ax2.set_xlabel('Iteration')\n    ax2.set_ylabel('f(x,y)')\n    ax2.set_title('Function Value vs Iteration')\n    ax2.grid(True, alpha=0.3)\n    \n    # x and y coordinates over iterations\n    ax3.plot(range(len(x_coords)), x_coords, 'g-o', linewidth=2, markersize=4, label='x')\n    ax3.plot(range(len(y_coords)), y_coords, 'm-o', linewidth=2, markersize=4, label='y')\n    ax3.set_xlabel('Iteration')\n    ax3.set_ylabel('Coordinate Value')\n    ax3.set_title('Parameter Values vs Iteration')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    # Gradient magnitude over iterations\n    grad_magnitudes = []\n    for i, (x_val, y_val) in enumerate(zip(x_coords, y_coords)):\n        grad_mag = np.sqrt(df_dx(x_val, y_val)**2 + df_dy(x_val, y_val)**2)\n        grad_magnitudes.append(grad_mag)\n    \n    ax4.plot(range(len(grad_magnitudes)), grad_magnitudes, 'c-o', linewidth=2, markersize=4)\n    ax4.set_xlabel('Iteration')\n    ax4.set_ylabel('|∇f|')\n    ax4.set_title('Gradient Magnitude vs Iteration')\n    ax4.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print results\n    print(f\"Starting point: ({x0}, {y0})\")\n    print(f\"Final point: ({x_coords[-1]:.6f}, {y_coords[-1]:.6f})\")\n    print(f\"Final function value: {z_coords[-1]:.6f}\")\n    print(f\"Final gradient magnitude: {grad_magnitudes[-1]:.6f}\")\n\ngradient_descent_2d()\n\n# Higher-order partial derivatives\ndef higher_order_partials():\n    x, y = sp.symbols('x y')\n    \n    print(\"\\n=== HIGHER-ORDER PARTIAL DERIVATIVES ===\\n\")\n    \n    # f(x,y) = x³ + y³ + x*y\n    f_expr = x**3 + y**3 + x*y\n    \n    # First-order partials\n    df_dx = sp.diff(f_expr, x)\n    df_dy = sp.diff(f_expr, y)\n    \n    # Second-order partials\n    d2f_dx2 = sp.diff(df_dx, x)\n    d2f_dy2 = sp.diff(df_dy, y)\n    d2f_dxdy = sp.diff(df_dx, y)\n    d2f_dydx = sp.diff(df_dy, x)\n    \n    print(f\"Function: f(x,y) = {f_expr}\")\n    print(f\"\\nFirst-order partials:\")\n    print(f\"  ∂f/∂x = {df_dx}\")\n    print(f\"  ∂f/∂y = {df_dy}\")\n    \n    print(f\"\\nSecond-order partials:\")\n    print(f\"  ∂²f/∂x² = {d2f_dx2}\")\n    print(f\"  ∂²f/∂y² = {d2f_dy2}\")\n    print(f\"  ∂²f/∂x∂y = {d2f_dxdy}\")\n    print(f\"  ∂²f/∂y∂x = {d2f_dydx}\")\n    \n    # Verify Clairaut's theorem (equality of mixed partials)\n    print(f\"\\nClairaut's theorem verification:\")\n    print(f\"  ∂²f/∂x∂y = ∂²f/∂y∂x: {d2f_dxdy == d2f_dydx}\")\n    \n    # Hessian matrix\n    print(f\"\\nHessian matrix:\")\n    print(f\"  H = [∂²f/∂x²    ∂²f/∂x∂y]\")\n    print(f\"      [∂²f/∂y∂x   ∂²f/∂y²]\")\n    print(f\"  H = [{d2f_dx2}    {d2f_dxdy}]\")\n    print(f\"      [{d2f_dydx}   {d2f_dy2}]\")\n\nhigher_order_partials()\n\n### Applications in Machine Learning\n\nPartial derivatives are fundamental to:\n\n1. **Gradient Computation**: Computing gradients for optimization\n2. **Neural Networks**: Backpropagation through multiple layers\n3. **Feature Importance**: Understanding how each feature affects predictions\n4. **Optimization**: Gradient descent, Newton's method, etc.\n5. **Sensitivity Analysis**: Understanding model behavior\n\n## 2.5 Gradient and Directional Derivatives\n\nThe gradient is a vector that contains all the partial derivatives of a function. It points in the direction of steepest ascent and is fundamental to optimization algorithms.\n\n### Mathematical Foundation\n\nFor a function f(x₁, x₂, ..., xₙ), the gradient is:\n\n$$\\nabla f = \\left[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, ..., \\frac{\\partial f}{\\partial x_n}\\right]$$\n\nThe directional derivative in the direction of unit vector **u** is:\n\n$$D_{\\mathbf{u}}f = \\nabla f \\cdot \\mathbf{u}$$\n\n### Key Properties of the Gradient\n\n1. **Direction of Steepest Ascent**: ∇f points in the direction of maximum increase\n2. **Magnitude**: |∇f| gives the rate of change in the direction of steepest ascent\n3. **Orthogonality**: ∇f is perpendicular to level curves/surfaces\n4. **Linearity**: ∇(af + bg) = a∇f + b∇g\n\n### Why Gradients Matter in ML\n\n1. **Optimization**: Gradient descent follows the negative gradient\n2. **Feature Importance**: Gradient magnitude indicates sensitivity\n3. **Convergence**: Gradient magnitude helps determine convergence\n4. **Regularization**: Gradient-based regularization methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python\n# Comprehensive gradient analysis\ndef gradient_analysis_comprehensive():\n    print(\"=== GRADIENT ANALYSIS ===\\n\")\n    \n    def f(x, y):\n        return x**2 + y**2\n    \n    def gradient_f(x, y):\n        return np.array([2*x, 2*y])\n    \n    def gradient_magnitude(x, y):\n        return np.sqrt((2*x)**2 + (2*y)**2)\n    \n    # Test points\n    test_points = [(0, 0), (1, 1), (-1, 0), (0.5, -0.5)]\n    \n    print(\"1. GRADIENT CALCULATION\")\n    for point in test_points:\n        x, y = point\n        grad = gradient_f(x, y)\n        mag = gradient_magnitude(x, y)\n        print(f\"   Point ({x}, {y}):\")\n        print(f\"     Gradient: [{grad[0]:.4f}, {grad[1]:.4f}]\")\n        print(f\"     Magnitude: {mag:.4f}\")\n        print(f\"     Unit direction: [{grad[0]/mag:.4f}, {grad[1]/mag:.4f}]\")\n        print()\n    \n    # Directional derivatives\n    print(\"2. DIRECTIONAL DERIVATIVES\")\n    x, y = 1.0, 1.0\n    grad = gradient_f(x, y)\n    \n    # Test different directions\n    directions = [\n        np.array([1, 0]),      # x-direction\n        np.array([0, 1]),      # y-direction\n        np.array([1, 1]),      # diagonal\n        np.array([1, -1])      # opposite diagonal\n    ]\n    \n    for i, direction in enumerate(directions):\n        # Normalize direction\n        unit_direction = direction / np.linalg.norm(direction)\n        directional_deriv = np.dot(grad, unit_direction)\n        print(f\"   Direction {i+1}: {unit_direction}\")\n        print(f\"   Directional derivative: {directional_deriv:.4f}\")\n        print()\n\ngradient_analysis_comprehensive()\n\n# Advanced gradient calculation with error analysis\ndef gradient_2d_advanced(f, x, y, h=1e-7):\n    \"\"\"Calculate gradient of 2D function using central differences\"\"\"\n    df_dx = (f(x + h, y) - f(x - h, y)) / (2 * h)\n    df_dy = (f(x, y + h) - f(x, y - h)) / (2 * h)\n    return np.array([df_dx, df_dy])\n\ndef f_example(x, y):\n    return x**2 + y**2\n\ndef f_complex(x, y):\n    return np.sin(x*y) + np.exp(x**2 + y**2)\n\n# Compare analytical vs numerical gradients\nprint(\"=== GRADIENT COMPARISON ===\\n\")\n\ndef analytical_gradient_f(x, y):\n    return np.array([2*x, 2*y])\n\ntest_points = [(0.5, 0.5), (1.0, 1.0), (-0.5, 1.0)]\n\nfor point in test_points:\n    x, y = point\n    analytical = analytical_gradient_f(x, y)\n    numerical = gradient_2d_advanced(f_example, x, y)\n    error = np.linalg.norm(analytical - numerical)\n    \n    print(f\"Point ({x}, {y}):\")\n    print(f\"  Analytical: [{analytical[0]:.6f}, {analytical[1]:.6f}]\")\n    print(f\"  Numerical:  [{numerical[0]:.6f}, {numerical[1]:.6f}]\")\n    print(f\"  Error:      {error:.2e}\")\n    print()\n\n# Comprehensive gradient field visualization\ndef visualize_gradient_field():\n    x = np.linspace(-2, 2, 20)\n    y = np.linspace(-2, 2, 20)\n    X, Y = np.meshgrid(x, y)\n    \n    # Function values\n    Z = f_example(X, Y)\n    \n    # Gradients\n    gradients = np.zeros((len(x), len(y), 2))\n    for i in range(len(x)):\n        for j in range(len(y)):\n            gradients[i, j] = gradient_2d_advanced(f_example, X[i, j], Y[i, j])\n    \n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Contour plot with gradient vectors\n    contour = ax1.contour(X, Y, Z, levels=15)\n    ax1.clabel(contour, inline=True, fontsize=8)\n    ax1.quiver(X, Y, gradients[:, :, 0], gradients[:, :, 1], \n               angles='xy', scale_units='xy', scale=1, alpha=0.6)\n    ax1.set_title('Gradient Field of f(x,y) = x² + y²')\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.grid(True, alpha=0.3)\n    \n    # Gradient magnitude\n    grad_magnitude = np.sqrt(gradients[:, :, 0]**2 + gradients[:, :, 1]**2)\n    mag_contour = ax2.contour(X, Y, grad_magnitude, levels=15)\n    ax2.clabel(mag_contour, inline=True, fontsize=8)\n    ax2.set_title('Gradient Magnitude |∇f|')\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.grid(True, alpha=0.3)\n    \n    # 3D surface with gradient\n    ax3 = fig.add_subplot(2, 2, 3, projection='3d')\n    surf = ax3.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n    ax3.set_xlabel('x')\n    ax3.set_ylabel('y')\n    ax3.set_zlabel('f(x,y)')\n    ax3.set_title('3D Surface with Gradient')\n    \n    # Gradient direction heatmap\n    grad_direction = np.arctan2(gradients[:, :, 1], gradients[:, :, 0])\n    im = ax4.imshow(grad_direction, extent=[-2, 2, -2, 2], origin='lower', cmap='hsv')\n    ax4.set_title('Gradient Direction (angle)')\n    ax4.set_xlabel('x')\n    ax4.set_ylabel('y')\n    plt.colorbar(im, ax=ax4)\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_gradient_field()\n\n# Directional derivative analysis\ndef directional_derivative_analysis():\n    print(\"\\n=== DIRECTIONAL DERIVATIVE ANALYSIS ===\\n\")\n    \n    def f(x, y):\n        return x**2 + y**2\n    \n    def gradient_f(x, y):\n        return np.array([2*x, 2*y])\n    \n    def directional_derivative(f, gradient_f, x, y, direction, h=1e-7):\n        \"\"\"Compute directional derivative numerically\"\"\"\n        unit_direction = direction / np.linalg.norm(direction)\n        \n        # Numerical approach\n        f_current = f(x, y)\n        f_forward = f(x + h * unit_direction[0], y + h * unit_direction[1])\n        numerical_deriv = (f_forward - f_current) / h\n        \n        # Analytical approach\n        grad = gradient_f(x, y)\n        analytical_deriv = np.dot(grad, unit_direction)\n        \n        return numerical_deriv, analytical_deriv\n    \n    # Test point\n    x, y = 1.0, 1.0\n    print(f\"Test point: ({x}, {y})\")\n    print(f\"Gradient: {gradient_f(x, y)}\")\n    print()\n    \n    # Test different directions\n    directions = [\n        np.array([1, 0]),      # x-direction\n        np.array([0, 1]),      # y-direction\n        np.array([1, 1]),      # diagonal\n        np.array([1, -1]),     # opposite diagonal\n        np.array([np.cos(np.pi/4), np.sin(np.pi/4)])  # 45 degrees\n    ]\n    \n    for i, direction in enumerate(directions):\n        unit_direction = direction / np.linalg.norm(direction)\n        num_deriv, ana_deriv = directional_derivative(f, gradient_f, x, y, direction)\n        \n        print(f\"Direction {i+1}: {unit_direction}\")\n        print(f\"  Numerical derivative:  {num_deriv:.6f}\")\n        print(f\"  Analytical derivative: {ana_deriv:.6f}\")\n        print(f\"  Error:                 {abs(num_deriv - ana_deriv):.2e}\")\n        print()\n\ndirectional_derivative_analysis()\n\n## 2.6 Applications in Machine Learning\n\n### Gradient Descent Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced gradient descent analysis\ndef gradient_descent_analysis():\n    print(\"=== GRADIENT DESCENT ANALYSIS ===\\n\")\n    \n    def rosenbrock(x, y):\n        \"\"\"Rosenbrock function: challenging optimization landscape\"\"\"\n        return (1 - x)**2 + 100 * (y - x**2)**2\n    \n    def rosenbrock_gradient(x, y):\n        \"\"\"Gradient of Rosenbrock function\"\"\"\n        dx = -2 * (1 - x) - 400 * x * (y - x**2)\n        dy = 200 * (y - x**2)\n        return np.array([dx, dy])\n    \n    def gradient_descent_advanced(f, grad_f, start_point, learning_rate=0.001, \n                                 iterations=1000, momentum=0.0):\n        \"\"\"Gradient descent with momentum\"\"\"\n        x, y = start_point\n        history = [(x, y, f(x, y))]\n        velocity = np.array([0.0, 0.0])\n        \n        for i in range(iterations):\n            gradient = grad_f(x, y)\n            \n            # Update with momentum\n            velocity = momentum * velocity - learning_rate * gradient\n            x, y = x + velocity[0], y + velocity[1]\n            \n            history.append((x, y, f(x, y)))\n            \n            # Early stopping if gradient is very small\n            if np.linalg.norm(gradient) < 1e-6:\n                break\n        \n        return np.array(history)\n    \n    # Test different starting points and learning rates\n    start_points = [(-1, -1), (0, 0), (1, 1)]\n    learning_rates = [0.0001, 0.001, 0.01]\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    for i, start_point in enumerate(start_points):\n        for j, lr in enumerate(learning_rates):\n            if i * 2 + j < 4:  # Only plot first 4 combinations\n                path = gradient_descent_advanced(rosenbrock, rosenbrock_gradient, \n                                               start_point, learning_rate=lr, iterations=1000)\n                \n                ax = axes[i, j] if i < 2 else axes[1, j-2]\n                \n                # Create contour plot\n                x = np.linspace(-2, 2, 100)\n                y = np.linspace(-2, 2, 100)\n                X, Y = np.meshgrid(x, y)\n                Z = rosenbrock(X, Y)\n                \n                contour = ax.contour(X, Y, Z, levels=20)\n                ax.plot(path[:, 0], path[:, 1], 'r-', linewidth=2, alpha=0.7)\n                ax.scatter(path[0, 0], path[0, 1], c='red', s=100, label='Start')\n                ax.scatter(path[-1, 0], path[-1, 1], c='green', s=100, label='End')\n                ax.set_title(f'Start: {start_point}, LR: {lr}')\n                ax.set_xlabel('x')\n                ax.set_ylabel('y')\n                ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Analyze convergence\n    print(\"Convergence Analysis:\")\n    start_point = (-1, -1)\n    lr = 0.001\n    \n    path = gradient_descent_advanced(rosenbrock, rosenbrock_gradient, \n                                   start_point, learning_rate=lr, iterations=1000)\n    \n    print(f\"Starting point: {start_point}\")\n    print(f\"Learning rate: {lr}\")\n    print(f\"Final point: ({path[-1, 0]:.6f}, {path[-1, 1]:.6f})\")\n    print(f\"Final function value: {path[-1, 2]:.6f}\")\n    print(f\"Number of iterations: {len(path)}\")\n    \n    # Plot convergence\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 3, 1)\n    plt.plot(path[:, 2])\n    plt.xlabel('Iteration')\n    plt.ylabel('Function Value')\n    plt.title('Convergence')\n    plt.grid(True, alpha=0.3)\n    \n    plt.subplot(1, 3, 2)\n    plt.plot(path[:, 0], label='x')\n    plt.plot(path[:, 1], label='y')\n    plt.xlabel('Iteration')\n    plt.ylabel('Parameter Value')\n    plt.title('Parameter Evolution')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.subplot(1, 3, 3)\n    gradients = [np.linalg.norm(rosenbrock_gradient(x, y)) for x, y in path[:, :2]]\n    plt.plot(gradients)\n    plt.xlabel('Iteration')\n    plt.ylabel('Gradient Magnitude')\n    plt.title('Gradient Magnitude')\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\ngradient_descent_analysis()\n\n### Loss Function Derivatives\n\nUnderstanding loss function derivatives is crucial for training neural networks and other machine learning models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python\n# Comprehensive loss function analysis\ndef loss_function_analysis():\n    print(\"=== LOSS FUNCTION ANALYSIS ===\\n\")\n    \n    def mse_loss(y_pred, y_true):\n        \"\"\"Mean Squared Error loss\"\"\"\n        return np.mean((y_pred - y_true)**2)\n    \n    def mse_derivative(y_pred, y_true):\n        \"\"\"Derivative of MSE with respect to predictions\"\"\"\n        return 2 * (y_pred - y_true) / len(y_pred)\n    \n    def cross_entropy_loss(y_pred, y_true):\n        \"\"\"Cross-entropy loss for binary classification\"\"\"\n        epsilon = 1e-15  # Avoid log(0)\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    \n    def cross_entropy_derivative(y_pred, y_true):\n        \"\"\"Derivative of cross-entropy with respect to predictions\"\"\"\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        return (y_pred - y_true) / (y_pred * (1 - y_pred))\n    \n    def huber_loss(y_pred, y_true, delta=1.0):\n        \"\"\"Huber loss: combines MSE and MAE\"\"\"\n        error = y_pred - y_true\n        abs_error = np.abs(error)\n        quadratic = np.minimum(abs_error, delta)\n        linear = abs_error - quadratic\n        return np.mean(0.5 * quadratic**2 + delta * linear)\n    \n    def huber_derivative(y_pred, y_true, delta=1.0):\n        \"\"\"Derivative of Huber loss\"\"\"\n        error = y_pred - y_true\n        abs_error = np.abs(error)\n        derivative = np.where(abs_error <= delta, error, delta * np.sign(error))\n        return derivative / len(y_pred)\n    \n    # Test data\n    y_true = np.array([0, 1, 0, 1])\n    y_pred = np.array([0.1, 0.8, 0.3, 0.9])\n    \n    # Calculate losses and derivatives\n    losses = {\n        'MSE': mse_loss(y_pred, y_true),\n        'Cross-Entropy': cross_entropy_loss(y_pred, y_true),\n        'Huber': huber_loss(y_pred, y_true)\n    }\n    \n    derivatives = {\n        'MSE': mse_derivative(y_pred, y_true),\n        'Cross-Entropy': cross_entropy_derivative(y_pred, y_true),\n        'Huber': huber_derivative(y_pred, y_true)\n    }\n    \n    print(\"Loss Function Comparison:\")\n    for name, loss in losses.items():\n        print(f\"  {name}: {loss:.4f}\")\n    \n    print(\"\\nDerivatives:\")\n    for name, deriv in derivatives.items():\n        print(f\"  {name}: {deriv}\")\n    \n    # Visualize loss functions\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # MSE loss landscape\n    y_true_single = 1.0\n    y_pred_range = np.linspace(-2, 4, 100)\n    mse_values = [(y_pred - y_true_single)**2 for y_pred in y_pred_range]\n    mse_derivatives = [2 * (y_pred - y_true_single) for y_pred in y_pred_range]\n    \n    ax1.plot(y_pred_range, mse_values, 'b-', linewidth=2, label='MSE Loss')\n    ax1.plot(y_pred_range, mse_derivatives, 'r--', linewidth=2, label='MSE Derivative')\n    ax1.axvline(y_true_single, color='g', linestyle=':', label='True Value')\n    ax1.set_xlabel('Prediction')\n    ax1.set_ylabel('Value')\n    ax1.set_title('MSE Loss and Derivative')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Cross-entropy loss landscape\n    y_pred_range_ce = np.linspace(0.01, 0.99, 100)\n    ce_values = [-y_true_single * np.log(y_pred) - (1 - y_true_single) * np.log(1 - y_pred) \n                 for y_pred in y_pred_range_ce]\n    ce_derivatives = [(y_pred - y_true_single) / (y_pred * (1 - y_pred)) \n                      for y_pred in y_pred_range_ce]\n    \n    ax2.plot(y_pred_range_ce, ce_values, 'b-', linewidth=2, label='Cross-Entropy Loss')\n    ax2.plot(y_pred_range_ce, ce_derivatives, 'r--', linewidth=2, label='CE Derivative')\n    ax2.axvline(y_true_single, color='g', linestyle=':', label='True Value')\n    ax2.set_xlabel('Prediction')\n    ax2.set_ylabel('Value')\n    ax2.set_title('Cross-Entropy Loss and Derivative')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Huber loss landscape\n    huber_values = [huber_loss(np.array([y_pred]), np.array([y_true_single])) \n                    for y_pred in y_pred_range]\n    huber_derivatives = [huber_derivative(np.array([y_pred]), np.array([y_true_single]))[0] \n                         for y_pred in y_pred_range]\n    \n    ax3.plot(y_pred_range, huber_values, 'b-', linewidth=2, label='Huber Loss')\n    ax3.plot(y_pred_range, huber_derivatives, 'r--', linewidth=2, label='Huber Derivative')\n    ax3.axvline(y_true_single, color='g', linestyle=':', label='True Value')\n    ax3.set_xlabel('Prediction')\n    ax3.set_ylabel('Value')\n    ax3.set_title('Huber Loss and Derivative')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    # Comparison of derivatives\n    ax4.plot(y_pred_range, mse_derivatives, 'b-', linewidth=2, label='MSE')\n    ax4.plot(y_pred_range, huber_derivatives, 'r-', linewidth=2, label='Huber')\n    ax4.axvline(y_true_single, color='g', linestyle=':', label='True Value')\n    ax4.set_xlabel('Prediction')\n    ax4.set_ylabel('Derivative')\n    ax4.set_title('Derivative Comparison')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\nloss_function_analysis()\n\n# Neural network gradient analysis\ndef neural_network_gradient_analysis():\n    print(\"\\n=== NEURAL NETWORK GRADIENT ANALYSIS ===\\n\")\n    \n    # Simple neural network: y = σ(w₂ * σ(w₁ * x + b₁) + b₂)\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    \n    def sigmoid_derivative(x):\n        s = sigmoid(x)\n        return s * (1 - s)\n    \n    def forward_pass(x, w1, b1, w2, b2):\n        \"\"\"Forward pass through the network\"\"\"\n        z1 = w1 * x + b1\n        a1 = sigmoid(z1)\n        z2 = w2 * a1 + b2\n        y = sigmoid(z2)\n        return y, z1, a1, z2\n    \n    def backward_pass(x, y_true, w1, b1, w2, b2):\n        \"\"\"Backward pass using chain rule\"\"\"\n        y, z1, a1, z2 = forward_pass(x, w1, b1, w2, b2)\n        \n        # Loss: L = (y - y_true)²\n        loss = (y - y_true)**2\n        \n        # Chain rule for gradients\n        # dL/dy = 2(y - y_true)\n        # dy/dz2 = σ'(z2)\n        # dz2/da1 = w2\n        # da1/dz1 = σ'(z1)\n        # dz1/dw1 = x, dz1/db1 = 1\n        # dz2/dw2 = a1, dz2/db2 = 1\n        \n        dL_dy = 2 * (y - y_true)\n        dy_dz2 = sigmoid_derivative(z2)\n        dz2_da1 = w2\n        da1_dz1 = sigmoid_derivative(z1)\n        \n        # Gradients\n        dL_dw2 = dL_dy * dy_dz2 * a1\n        dL_db2 = dL_dy * dy_dz2\n        dL_dw1 = dL_dy * dy_dz2 * dz2_da1 * da1_dz1 * x\n        dL_db1 = dL_dy * dy_dz2 * dz2_da1 * da1_dz1\n        \n        return dL_dw1, dL_db1, dL_dw2, dL_db2, loss\n    \n    # Test the network\n    x = 2.0\n    y_true = 0.8\n    w1, b1, w2, b2 = 1.0, 0.5, 0.8, 0.2\n    \n    print(f\"Input: x = {x}\")\n    print(f\"Target: y = {y_true}\")\n    print(f\"Parameters: w1={w1}, b1={b1}, w2={w2}, b2={b2}\")\n    \n    y, z1, a1, z2 = forward_pass(x, w1, b1, w2, b2)\n    print(f\"Forward pass: y = {y:.4f}\")\n    \n    dL_dw1, dL_db1, dL_dw2, dL_db2, loss = backward_pass(x, y_true, w1, b1, w2, b2)\n    print(f\"Loss: {loss:.4f}\")\n    print(f\"Gradients: dL/dw1={dL_dw1:.4f}, dL/db1={dL_db1:.4f}, dL/dw2={dL_dw2:.4f}, dL/db2={dL_db2:.4f}\")\n    \n    # Verify with numerical gradients\n    def loss_function(w1, b1, w2, b2):\n        y, _, _, _ = forward_pass(x, w1, b1, w2, b2)\n        return (y - y_true)**2\n    \n    h = 1e-7\n    numerical_gradients = []\n    \n    for param, param_name in [(w1, 'w1'), (b1, 'b1'), (w2, 'w2'), (b2, 'b2')]:\n        # Create parameter list for numerical gradient\n        params = [w1, b1, w2, b2]\n        param_idx = ['w1', 'b1', 'w2', 'b2'].index(param_name)\n        \n        # Numerical gradient\n        params_plus = params.copy()\n        params_plus[param_idx] += h\n        params_minus = params.copy()\n        params_minus[param_idx] -= h\n        \n        num_grad = (loss_function(*params_plus) - loss_function(*params_minus)) / (2 * h)\n        analytical_grad = [dL_dw1, dL_db1, dL_dw2, dL_db2][param_idx]\n        \n        print(f\"\\n{param_name}:\")\n        print(f\"  Numerical:  {num_grad:.6f}\")\n        print(f\"  Analytical: {analytical_grad:.6f}\")\n        print(f\"  Error:      {abs(num_grad - analytical_grad):.2e}\")\n\nneural_network_gradient_analysis()\n```\n\n## Summary\n\nDerivatives are the mathematical foundation of optimization and machine learning. This comprehensive exploration covered:\n\n### Key Concepts:\n1. **Definition and Interpretation**: Derivatives measure instantaneous rate of change\n2. **Basic Rules**: Power, sum, product, quotient rules for efficient computation\n3. **Chain Rule**: Essential for composite functions and backpropagation\n4. **Partial Derivatives**: Handle multivariable functions\n5. **Gradient**: Vector of partial derivatives pointing in direction of steepest ascent\n6. **Directional Derivatives**: Rate of change in specific directions\n\n### Applications in Machine Learning:\n1. **Gradient Descent**: Foundation of most optimization algorithms\n2. **Backpropagation**: Chain rule applied to neural network training\n3. **Loss Functions**: Derivatives guide parameter updates\n4. **Activation Functions**: Derivatives of sigmoid, ReLU, tanh, etc.\n5. **Convergence Analysis**: Understanding optimization behavior\n\n### Mathematical Insights:\n- Derivatives provide local linear approximations\n- Gradients point in direction of maximum increase\n- Chain rule enables automatic differentiation\n- Partial derivatives handle high-dimensional spaces\n- Numerical methods provide verification of analytical results\n\nThese concepts form the mathematical backbone of modern machine learning, enabling the training of complex models through gradient-based optimization.\n\n## Next Steps\n\nUnderstanding derivatives enables us to explore their applications in optimization, curve sketching, and machine learning algorithms in the next section."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}