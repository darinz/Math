{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Derivatives and Differentiation\n",
       "\n",
       "[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)\n",
       "[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n",
       "[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-orange.svg)](https://matplotlib.org/)\n",
       "[![SymPy](https://img.shields.io/badge/SymPy-1.10+-purple.svg)](https://www.sympy.org/)\n",
       "\n",
       "## Introduction\n",
       "\n",
       "Derivatives are the cornerstone of calculus and are essential for understanding how functions change. In machine learning and data science, derivatives are used extensively for optimization, gradient descent, and understanding model behavior.\n",
       "\n",
       "### Why Derivatives Matter in AI/ML\n",
       "\n",
       "1. **Gradient Descent**: The most fundamental optimization algorithm in machine learning\n",
       "2. **Backpropagation**: Computing gradients through neural networks\n",
       "3. **Model Training**: Understanding how parameters affect the loss function\n",
       "4. **Feature Importance**: Understanding how input changes affect output\n",
       "5. **Optimization**: Finding minima and maxima of functions\n",
       "\n",
       "### Mathematical Definition\n",
       "\n",
       "The derivative of a function f(x) at a point x = a is defined as:\n",
       "\n",
       "$$f'(a) = \\lim_{h \\to 0} \\frac{f(a + h) - f(a)}{h}$$\n",
       "\n",
       "This represents the instantaneous rate of change of the function at that point."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import sympy as sp\n",
       "from sympy import symbols, diff, limit\n",
       "from scipy.misc import derivative\n",
       "\n",
       "# Set up plotting style\n",
       "plt.style.use('seaborn-v0_8')\n",
       "plt.rcParams['figure.figsize'] = (12, 8)\n",
       "plt.rcParams['font.size'] = 12\n",
       "\n",
       "# Enable LaTeX rendering\n",
       "plt.rcParams['text.usetex'] = True"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2.1 Understanding the Derivative\n",
       "\n",
       "The derivative represents the slope of the tangent line to a function at a given point. Let's visualize this concept."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Visualizing the derivative as the slope of the tangent line\n",
       "def f(x):\n",
       "    return x**2\n",
       "\n",
       "def f_prime(x):\n",
       "    return 2*x\n",
       "\n",
       "def tangent_line(x, x0, y0, slope):\n",
       "    return slope * (x - x0) + y0\n",
       "\n",
       "# Create visualization\n",
       "x_vals = np.linspace(-3, 3, 1000)\n",
       "y_vals = f(x_vals)\n",
       "\n",
       "# Points where we'll draw tangent lines\n",
       "tangent_points = [-2, -1, 0, 1, 2]\n",
       "\n",
       "plt.figure(figsize=(15, 10))\n",
       "\n",
       "# Main plot with function and tangent lines\n",
       "plt.subplot(2, 2, 1)\n",
       "plt.plot(x_vals, y_vals, 'b-', linewidth=3, label='f(x) = x²')\n",
       "\n",
       "colors = ['red', 'orange', 'green', 'purple', 'brown']\n",
       "for i, x0 in enumerate(tangent_points):\n",
       "    y0 = f(x0)\n",
       "    slope = f_prime(x0)\n",
       "    \n",
       "    # Draw tangent line\n",
       "    x_tangent = np.linspace(x0 - 1, x0 + 1, 100)\n",
       "    y_tangent = tangent_line(x_tangent, x0, y0, slope)\n",
       "    \n",
       "    plt.plot(x_tangent, y_tangent, color=colors[i], linestyle='--', linewidth=2, \n",
       "             label=f'Tangent at x={x0}, slope={slope}')\n",
       "    plt.scatter(x0, y0, color=colors[i], s=100, zorder=5)\n",
       "\n",
       "plt.xlabel('x')\n",
       "plt.ylabel('f(x)')\n",
       "plt.title('Derivative as Slope of Tangent Line')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "# Derivative function\n",
       "plt.subplot(2, 2, 2)\n",
       "y_prime_vals = f_prime(x_vals)\n",
       "plt.plot(x_vals, y_prime_vals, 'r-', linewidth=3, label=\"f'(x) = 2x\")\n",
       "plt.xlabel('x')\n",
       "plt.ylabel(\"f'(x)\")\n",
       "plt.title('Derivative Function')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "# Secant lines approaching tangent\n",
       "plt.subplot(2, 2, 3)\n",
       "x0 = 1\n",
       "y0 = f(x0)\n",
       "plt.plot(x_vals, y_vals, 'b-', linewidth=3, label='f(x) = x²')\n",
       "plt.scatter(x0, y0, color='red', s=100, zorder=5, label=f'Point ({x0}, {y0})')\n",
       "\n",
       "# Draw secant lines with different h values\n",
       "h_values = [0.5, 0.2, 0.1, 0.05]\n",
       "colors_secant = ['orange', 'green', 'purple', 'brown']\n",
       "\n",
       "for i, h in enumerate(h_values):\n",
       "    x1 = x0 + h\n",
       "    y1 = f(x1)\n",
       "    slope = (y1 - y0) / h\n",
       "    \n",
       "    x_secant = np.linspace(x0 - 0.5, x0 + 0.5, 100)\n",
       "    y_secant = tangent_line(x_secant, x0, y0, slope)\n",
       "    \n",
       "    plt.plot(x_secant, y_secant, color=colors_secant[i], linestyle='--', linewidth=2,\n",
       "             label=f'Secant h={h}, slope={slope:.2f}')\n",
       "    plt.scatter(x1, y1, color=colors_secant[i], s=50, zorder=5)\n",
       "\n",
       "plt.xlabel('x')\n",
       "plt.ylabel('f(x)')\n",
       "plt.title('Secant Lines Approaching Tangent')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "plt.xlim(0.5, 1.5)\n",
       "plt.ylim(0.5, 2.5)\n",
       "\n",
       "# Numerical approximation of derivative\n",
       "plt.subplot(2, 2, 4)\n",
       "h_values = np.logspace(-5, 0, 100)\n",
       "x0 = 1\n",
       "exact_derivative = f_prime(x0)\n",
       "\n",
       "numerical_derivatives = []\n",
       "for h in h_values:\n",
       "    approx = (f(x0 + h) - f(x0)) / h\n",
       "    numerical_derivatives.append(abs(approx - exact_derivative))\n",
       "\n",
       "plt.loglog(h_values, numerical_derivatives, 'b-', linewidth=2, label='Error')\n",
       "plt.axhline(y=1e-10, color='r', linestyle='--', label='Machine precision')\n",
       "plt.xlabel('h (step size)')\n",
       "plt.ylabel('Absolute Error')\n",
       "plt.title('Numerical Derivative Error vs Step Size')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2.2 Basic Differentiation Rules\n",
       "\n",
       "Understanding the fundamental rules of differentiation is crucial for computing derivatives efficiently."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Demonstrate basic differentiation rules using SymPy\n",
       "x = sp.Symbol('x')\n",
       "\n",
       "# Power rule\n",
       "print(\"=== Power Rule ===\")\n",
       "functions = [x**2, x**3, x**0.5, 1/x, 1/x**2]\n",
       "for func in functions:\n",
       "    derivative = sp.diff(func, x)\n",
       "    print(f\"d/dx({func}) = {derivative}\")\n",
       "\n",
       "print(\"\\n=== Constant Multiple Rule ===\")\n",
       "constants = [2, 3, -1, 0.5]\n",
       "for c in constants:\n",
       "    derivative = sp.diff(c * x**2, x)\n",
       "    print(f\"d/dx({c} * x²) = {derivative}\")\n",
       "\n",
       "print(\"\\n=== Sum Rule ===\")\n",
       "sum_func = x**2 + 3*x + 1\n",
       "derivative = sp.diff(sum_func, x)\n",
       "print(f\"d/dx({sum_func}) = {derivative}\")\n",
       "\n",
       "print(\"\\n=== Product Rule ===\")\n",
       "product_func = x**2 * sp.sin(x)\n",
       "derivative = sp.diff(product_func, x)\n",
       "print(f\"d/dx({product_func}) = {derivative}\")\n",
       "\n",
       "print(\"\\n=== Quotient Rule ===\")\n",
       "quotient_func = x**2 / (x + 1)\n",
       "derivative = sp.diff(quotient_func, x)\n",
       "print(f\"d/dx({quotient_func}) = {derivative}\")\n",
       "\n",
       "print(\"\\n=== Chain Rule ===\")\n",
       "chain_func = sp.sin(x**2)\n",
       "derivative = sp.diff(chain_func, x)\n",
       "print(f\"d/dx({chain_func}) = {derivative}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2.3 Chain Rule and Its Importance in ML\n",
       "\n",
       "The chain rule is fundamental to backpropagation in neural networks. It allows us to compute derivatives of composite functions."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Chain rule visualization and application\n",
       "def simple_chain_function(x):\n",
       "    \"\"\"f(x) = sin(x²)\"\"\"\n",
       "    return np.sin(x**2)\n",
       "\n",
       "def chain_derivative(x):\n",
       "    \"\"\"f'(x) = 2x * cos(x²) by chain rule\"\"\"\n",
       "    return 2 * x * np.cos(x**2)\n",
       "\n",
       "# Visualize the chain rule\n",
       "x_vals = np.linspace(-3, 3, 1000)\n",
       "y_vals = simple_chain_function(x_vals)\n",
       "y_prime_vals = chain_derivative(x_vals)\n",
       "\n",
       "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
       "\n",
       "# Original function\n",
       "ax1.plot(x_vals, y_vals, 'b-', linewidth=3, label='f(x) = sin(x²)')\n",
       "ax1.set_xlabel('x')\n",
       "ax1.set_ylabel('f(x)')\n",
       "ax1.set_title('Composite Function')\n",
       "ax1.legend()\n",
       "ax1.grid(True, alpha=0.3)\n",
       "\n",
       "# Derivative\n",
       "ax2.plot(x_vals, y_prime_vals, 'r-', linewidth=3, label=\"f'(x) = 2x * cos(x²)\")\n",
       "ax2.set_xlabel('x')\n",
       "ax2.set_ylabel(\"f'(x)\")\n",
       "ax2.set_title('Derivative by Chain Rule')\n",
       "ax2.legend()\n",
       "ax2.grid(True, alpha=0.3)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Demonstrate chain rule step by step\n",
       "print(\"Chain Rule Step-by-Step:\")\n",
       "print(\"f(x) = sin(x²)\")\n",
       "print(\"Let u = x², then f(x) = sin(u)\")\n",
       "print(\"By chain rule: f'(x) = d/dx[sin(u)] * d/dx[x²]\")\n",
       "print(\"f'(x) = cos(u) * 2x = cos(x²) * 2x = 2x * cos(x²)\")\n",
       "\n",
       "# Verify with SymPy\n",
       "x = sp.Symbol('x')\n",
       "chain_func = sp.sin(x**2)\n",
       "derivative = sp.diff(chain_func, x)\n",
       "print(f\"\\nSymPy verification: d/dx(sin(x²)) = {derivative}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2.4 Partial Derivatives and Gradients\n",
       "\n",
       "In machine learning, we often work with functions of multiple variables. Partial derivatives and gradients are essential for multivariate optimization."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Partial derivatives and gradients\n",
       "from mpl_toolkits.mplot3d import Axes3D\n",
       "\n",
       "def multivariate_function(x, y):\n",
       "    \"\"\"f(x,y) = x² + y²\"\"\"\n",
       "    return x**2 + y**2\n",
       "\n",
       "def gradient_x(x, y):\n",
       "    \"\"\"∂f/∂x = 2x\"\"\"\n",
       "    return 2 * x\n",
       "\n",
       "def gradient_y(x, y):\n",
       "    \"\"\"∂f/∂y = 2y\"\"\"\n",
       "    return 2 * y\n",
       "\n",
       "# Create 3D visualization\n",
       "x = np.linspace(-3, 3, 50)\n",
       "y = np.linspace(-3, 3, 50)\n",
       "X, Y = np.meshgrid(x, y)\n",
       "Z = multivariate_function(X, Y)\n",
       "\n",
       "fig = plt.figure(figsize=(15, 10))\n",
       "\n",
       "# 3D surface plot\n",
       "ax1 = fig.add_subplot(2, 2, 1, projection='3d')\n",
       "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
       "ax1.set_xlabel('x')\n",
       "ax1.set_ylabel('y')\n",
       "ax1.set_zlabel('f(x,y)')\n",
       "ax1.set_title('Multivariate Function: f(x,y) = x² + y²')\n",
       "\n",
       "# Contour plot with gradient vectors\n",
       "ax2 = fig.add_subplot(2, 2, 2)\n",
       "contour = ax2.contour(X, Y, Z, levels=20, cmap='viridis')\n",
       "ax2.clabel(contour, inline=True, fontsize=8)\n",
       "\n",
       "# Add gradient vectors at selected points\n",
       "sample_points = [(-2, -2), (-1, -1), (0, 0), (1, 1), (2, 2)]\n",
       "for x0, y0 in sample_points:\n",
       "    grad_x = gradient_x(x0, y0)\n",
       "    grad_y = gradient_y(x0, y0)\n",
       "    ax2.arrow(x0, y0, grad_x * 0.1, grad_y * 0.1, \n",
       "              head_width=0.1, head_length=0.1, fc='red', ec='red', alpha=0.7)\n",
       "    ax2.scatter(x0, y0, color='red', s=50, zorder=5)\n",
       "\n",
       "ax2.set_xlabel('x')\n",
       "ax2.set_ylabel('y')\n",
       "ax2.set_title('Contour Plot with Gradient Vectors')\n",
       "ax2.grid(True, alpha=0.3)\n",
       "\n",
       "# Partial derivative with respect to x\n",
       "ax3 = fig.add_subplot(2, 2, 3)\n",
       "Z_dx = gradient_x(X, Y)\n",
       "contour_dx = ax3.contour(X, Y, Z_dx, levels=20, cmap='Reds')\n",
       "ax3.clabel(contour_dx, inline=True, fontsize=8)\n",
       "ax3.set_xlabel('x')\n",
       "ax3.set_ylabel('y')\n",
       "ax3.set_title('Partial Derivative ∂f/∂x = 2x')\n",
       "ax3.grid(True, alpha=0.3)\n",
       "\n",
       "# Partial derivative with respect to y\n",
       "ax4 = fig.add_subplot(2, 2, 4)\n",
       "Z_dy = gradient_y(X, Y)\n",
       "contour_dy = ax4.contour(X, Y, Z_dy, levels=20, cmap='Blues')\n",
       "ax4.clabel(contour_dy, inline=True, fontsize=8)\n",
       "ax4.set_xlabel('x')\n",
       "ax4.set_ylabel('y')\n",
       "ax4.set_title('Partial Derivative ∂f/∂y = 2y')\n",
       "ax4.grid(True, alpha=0.3)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Verify with SymPy\n",
       "x, y = sp.symbols('x y')\n",
       "f = x**2 + y**2\n",
       "print(\"Partial derivatives:\")\n",
       "print(f\"∂f/∂x = {sp.diff(f, x)}\")\n",
       "print(f\"∂f/∂y = {sp.diff(f, y)}\")\n",
       "print(f\"Gradient ∇f = [{sp.diff(f, x)}, {sp.diff(f, y)}]\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2.5 Gradient Descent: The Foundation of ML Optimization\n",
       "\n",
       "Gradient descent is the most fundamental optimization algorithm in machine learning. It uses derivatives to find the minimum of a function."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Gradient descent implementation\n",
       "def simple_function(x):\n",
       "    \"\"\"f(x) = x² + 2x + 1\"\"\"\n",
       "    return x**2 + 2*x + 1\n",
       "\n",
       "def derivative_function(x):\n",
       "    \"\"\"f'(x) = 2x + 2\"\"\"\n",
       "    return 2*x + 2\n",
       "\n",
       "def gradient_descent(start_x, learning_rate, num_iterations):\n",
       "    \"\"\"Simple gradient descent for 1D function\"\"\"\n",
       "    x = start_x\n",
       "    history = []\n",
       "    \n",
       "    for i in range(num_iterations):\n",
       "        # Compute gradient\n",
       "        grad = derivative_function(x)\n",
       "        \n",
       "        # Update parameter\n",
       "        x_new = x - learning_rate * grad\n",
       "        \n",
       "        # Store history\n",
       "        history.append((x, simple_function(x), grad))\n",
       "        \n",
       "        # Update x\n",
       "        x = x_new\n",
       "    \n",
       "    return x, history\n",
       "\n",
       "# Run gradient descent\n",
       "start_x = 5.0\n",
       "learning_rate = 0.1\n",
       "num_iterations = 20\n",
       "\n",
       "optimal_x, history = gradient_descent(start_x, learning_rate, num_iterations)\n",
       "\n",
       "print(f\"Starting point: x = {start_x}\")\n",
       "print(f\"Optimal point: x = {optimal_x:.6f}\")\n",
       "print(f\"Optimal value: f(x) = {simple_function(optimal_x):.6f}\")\n",
       "print(f\"Analytical minimum: x = -1, f(-1) = 0\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Visualize gradient descent\n",
       "x_vals = np.linspace(-2, 6, 1000)\n",
       "y_vals = simple_function(x_vals)\n",
       "\n",
       "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
       "\n",
       "# Function and optimization path\n",
       "ax1.plot(x_vals, y_vals, 'b-', linewidth=3, label='f(x) = x² + 2x + 1')\n",
       "ax1.axvline(x=-1, color='g', linestyle='--', linewidth=2, label='Analytical minimum')\n",
       "\n",
       "# Plot optimization path\n",
       "x_history = [h[0] for h in history]\n",
       "y_history = [h[1] for h in history]\n",
       "ax1.plot(x_history, y_history, 'ro-', linewidth=2, markersize=8, label='Gradient descent path')\n",
       "ax1.scatter(x_history[0], y_history[0], color='red', s=200, zorder=5, label='Start')\n",
       "ax1.scatter(x_history[-1], y_history[-1], color='green', s=200, zorder=5, label='End')\n",
       "\n",
       "ax1.set_xlabel('x')\n",
       "ax1.set_ylabel('f(x)')\n",
       "ax1.set_title('Gradient Descent Optimization')\n",
       "ax1.legend()\n",
       "ax1.grid(True, alpha=0.3)\n",
       "\n",
       "# Convergence plot\n",
       "iterations = range(len(history))\n",
       "function_values = [h[1] for h in history]\n",
       "gradients = [abs(h[2]) for h in history]\n",
       "\n",
       "ax2.plot(iterations, function_values, 'b-', linewidth=2, label='Function value')\n",
       "ax2.set_xlabel('Iteration')\n",
       "ax2.set_ylabel('f(x)')\n",
       "ax2.set_title('Convergence of Function Value')\n",
       "ax2.legend()\n",
       "ax2.grid(True, alpha=0.3)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Print optimization history\n",
       "print(\"\\nOptimization History:\")\n",
       "print(\"Iteration | x | f(x) | f'(x)\")\n",
       "print(\"-\" * 35)\n",
       "for i, (x, fx, grad) in enumerate(history):\n",
       "    print(f\"{i:9d} | {x:6.4f} | {fx:6.4f} | {grad:6.4f}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2.6 Higher-Order Derivatives\n",
       "\n",
       "Higher-order derivatives provide information about the curvature and behavior of functions. They are important for optimization algorithms like Newton's method."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Higher-order derivatives\n",
       "def polynomial_function(x):\n",
       "    \"\"\"f(x) = x³ - 3x² + 2x\"\"\"\n",
       "    return x**3 - 3*x**2 + 2*x\n",
       "\n",
       "def first_derivative(x):\n",
       "    \"\"\"f'(x) = 3x² - 6x + 2\"\"\"\n",
       "    return 3*x**2 - 6*x + 2\n",
       "\n",
       "def second_derivative(x):\n",
       "    \"\"\"f''(x) = 6x - 6\"\"\"\n",
       "    return 6*x - 6\n",
       "\n",
       "def third_derivative(x):\n",
       "    \"\"\"f'''(x) = 6\"\"\"\n",
       "    return 6\n",
       "\n",
       "# Visualize higher-order derivatives\n",
       "x_vals = np.linspace(-1, 3, 1000)\n",
       "y_vals = polynomial_function(x_vals)\n",
       "y_prime_vals = first_derivative(x_vals)\n",
       "y_double_prime_vals = second_derivative(x_vals)\n",
       "y_triple_prime_vals = third_derivative(x_vals)\n",
       "\n",
       "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
       "\n",
       "# Original function\n",
       "ax1.plot(x_vals, y_vals, 'b-', linewidth=3, label='f(x) = x³ - 3x² + 2x')\n",
       "ax1.set_xlabel('x')\n",
       "ax1.set_ylabel('f(x)')\n",
       "ax1.set_title('Original Function')\n",
       "ax1.legend()\n",
       "ax1.grid(True, alpha=0.3)\n",
       "\n",
       "# First derivative\n",
       "ax2.plot(x_vals, y_prime_vals, 'r-', linewidth=3, label=\"f'(x) = 3x² - 6x + 2\")\n",
       "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
       "ax2.set_xlabel('x')\n",
       "ax2.set_ylabel(\"f'(x)\")\n",
       "ax2.set_title('First Derivative (Critical Points)')\n",
       "ax2.legend()\n",
       "ax2.grid(True, alpha=0.3)\n",
       "\n",
       "# Second derivative\n",
       "ax3.plot(x_vals, y_double_prime_vals, 'g-', linewidth=3, label=\"f''(x) = 6x - 6\")\n",
       "ax3.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
       "ax3.set_xlabel('x')\n",
       "ax3.set_ylabel(\"f''(x)\")\n",
       "ax3.set_title('Second Derivative (Concavity)')\n",
       "ax3.legend()\n",
       "ax3.grid(True, alpha=0.3)\n",
       "\n",
       "# Third derivative\n",
       "ax4.plot(x_vals, y_triple_prime_vals, 'm-', linewidth=3, label=\"f'''(x) = 6\")\n",
       "ax4.set_xlabel('x')\n",
       "ax4.set_ylabel(\"f'''(x)\")\n",
       "ax4.set_title('Third Derivative (Constant)')\n",
       "ax4.legend()\n",
       "ax4.grid(True, alpha=0.3)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Find critical points and analyze them\n",
       "from scipy.optimize import fsolve\n",
       "\n",
       "def find_critical_points():\n",
       "    \"\"\"Find where f'(x) = 0\"\"\"\n",
       "    def equation(x):\n",
       "        return first_derivative(x)\n",
       "    \n",
       "    # Try different starting points\n",
       "    critical_points = []\n",
       "    for start in [0, 1, 2]:\n",
       "        try:\n",
       "            root = fsolve(equation, start)[0]\n",
       "            if abs(equation(root)) < 1e-10:  # Check if it's actually a root\n",
       "                critical_points.append(root)\n",
       "        except:\n",
       "            continue\n",
       "    \n",
       "    return list(set([round(x, 6) for x in critical_points]))\n",
       "\n",
       "critical_points = find_critical_points()\n",
       "print(\"Critical points (where f'(x) = 0):\")\n",
       "for x in critical_points:\n",
       "    f_val = polynomial_function(x)\n",
       "    f_double_prime = second_derivative(x)\n",
       "    point_type = \"Local minimum\" if f_double_prime > 0 else \"Local maximum\" if f_double_prime < 0 else \"Saddle point\"\n",
       "    print(f\"x = {x:.4f}: f(x) = {f_val:.4f}, f''(x) = {f_double_prime:.4f} ({point_type})\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2.7 Applications in Machine Learning\n",
       "\n",
       "Derivatives are fundamental to many machine learning algorithms and concepts."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Machine learning applications of derivatives\n",
       "from sklearn.linear_model import LinearRegression\n",
       "from sklearn.metrics import mean_squared_error\n",
       "import pandas as pd\n",
       "\n",
       "# Generate synthetic data for linear regression\n",
       "np.random.seed(42)\n",
       "X = np.random.randn(100, 1) * 2\n",
       "y = 3 * X + 2 + np.random.randn(100, 1) * 0.5\n",
       "\n",
       "# Manual gradient descent for linear regression\n",
       "def linear_regression_gradient_descent(X, y, learning_rate=0.01, epochs=1000):\n",
       "    \"\"\"Implement linear regression using gradient descent\"\"\"\n",
       "    n_samples = X.shape[0]\n",
       "    \n",
       "    # Initialize parameters\n",
       "    w = np.random.randn(1, 1)\n",
       "    b = np.random.randn(1, 1)\n",
       "    \n",
       "    # Store history\n",
       "    history = []\n",
       "    \n",
       "    for epoch in range(epochs):\n",
       "        # Forward pass\n",
       "        y_pred = X @ w + b\n",
       "        \n",
       "        # Compute gradients\n",
       "        dw = (2/n_samples) * X.T @ (y_pred - y)\n",
       "        db = (2/n_samples) * np.sum(y_pred - y)\n",
       "        \n",
       "        # Update parameters\n",
       "        w -= learning_rate * dw\n",
       "        b -= learning_rate * db\n",
       "        \n",
       "        # Store history every 100 epochs\n",
       "        if epoch % 100 == 0:\n",
       "            mse = mean_squared_error(y, y_pred)\n",
       "            history.append((epoch, w[0,0], b[0,0], mse))\n",
       "    \n",
       "    return w, b, history\n",
       "\n",
       "# Run gradient descent\n",
       "w_gd, b_gd, history = linear_regression_gradient_descent(X, y)\n",
       "\n",
       "# Compare with sklearn\n",
       "lr = LinearRegression()\n",
       "lr.fit(X, y)\n",
       "w_sklearn = lr.coef_[0, 0]\n",
       "b_sklearn = lr.intercept_[0]\n",
       "\n",
       "print(\"Linear Regression Results:\")\n",
       "print(f\"Gradient Descent: w = {w_gd[0,0]:.4f}, b = {b_gd[0,0]:.4f}\")\n",
       "print(f\"Sklearn: w = {w_sklearn:.4f}, b = {b_sklearn:.4f}\")\n",
       "print(f\"Difference: w = {abs(w_gd[0,0] - w_sklearn):.6f}, b = {abs(b_gd[0,0] - b_sklearn):.6f}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Visualize the optimization process\n",
       "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
       "\n",
       "# Data and fitted lines\n",
       "ax1.scatter(X, y, alpha=0.6, label='Data')\n",
       "\n",
       "# Plot fitted lines\n",
       "X_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
       "y_gd = X_line * w_gd[0,0] + b_gd[0,0]\n",
       "y_sklearn = X_line * w_sklearn + b_sklearn\n",
       "\n",
       "ax1.plot(X_line, y_gd, 'r-', linewidth=2, label=f'Gradient Descent: y = {w_gd[0,0]:.3f}x + {b_gd[0,0]:.3f}')\n",
       "ax1.plot(X_line, y_sklearn, 'g--', linewidth=2, label=f'Sklearn: y = {w_sklearn:.3f}x + {b_sklearn:.3f}')\n",
       "\n",
       "ax1.set_xlabel('X')\n",
       "ax1.set_ylabel('y')\n",
       "ax1.set_title('Linear Regression Comparison')\n",
       "ax1.legend()\n",
       "ax1.grid(True, alpha=0.3)\n",
       "\n",
       "# Convergence plot\n",
       "epochs, ws, bs, mses = zip(*history)\n",
       "ax2.plot(epochs, mses, 'b-', linewidth=2, label='MSE Loss')\n",
       "ax2.set_xlabel('Epoch')\n",
       "ax2.set_ylabel('Mean Squared Error')\n",
       "ax2.set_title('Loss Convergence')\n",
       "ax2.legend()\n",
       "ax2.grid(True, alpha=0.3)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Print optimization history\n",
       "print(\"\\nOptimization History:\")\n",
       "print(\"Epoch | Weight | Bias | MSE\")\n",
       "print(\"-\" * 35)\n",
       "for epoch, w, b, mse in history:\n",
       "    print(f\"{epoch:5d} | {w:6.4f} | {b:6.4f} | {mse:.6f}\")"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }