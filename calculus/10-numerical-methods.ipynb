{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Numerical Methods in Calculus\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-orange.svg)](https://matplotlib.org/)\n[![SymPy](https://img.shields.io/badge/SymPy-1.10+-purple.svg)](https://www.sympy.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-red.svg)](https://scipy.org/)\n\n## Introduction\n\nNumerical methods provide computational techniques for solving calculus problems when analytical solutions are difficult or impossible to obtain. These methods are essential for practical applications in engineering, physics, machine learning, and data science where exact solutions may not be available or computationally feasible.\n\n**Key Concepts:**\n- **Approximation:** Numerical methods provide approximate solutions with controlled error\n- **Discretization:** Continuous problems are converted to discrete computational problems\n- **Convergence:** Methods improve accuracy as computational effort increases\n- **Stability:** Small errors in input don't lead to large errors in output\n\n**Relevance to AI/ML:**\n- Numerical integration appears in probability calculations and model evaluation\n- Numerical differentiation is used in gradient computation and sensitivity analysis\n- Understanding numerical methods helps choose appropriate algorithms and assess accuracy\n\n---\n\n## 10.1 Numerical Integration\n\n### Mathematical Foundations\n\nNumerical integration approximates definite integrals when analytical solutions are unavailable. The goal is to compute:\n\\[\n\\int_a^b f(x) \\, dx \\approx \\sum_{i=1}^n w_i f(x_i)\n\\]\nwhere \\( w_i \\) are weights and \\( x_i \\) are evaluation points.\n\n**Key Properties:**\n- **Rectangle Rule:** Uses function values at left endpoints of subintervals\n- **Trapezoidal Rule:** Uses linear interpolation between points\n- **Simpson's Rule:** Uses quadratic interpolation for higher accuracy\n- **Error Analysis:** Error typically decreases as \\( O(h^p) \\) where \\( h \\) is step size and \\( p \\) is order\n\n**Relevance to AI/ML:**\n- Computing expectations and probabilities in probabilistic models\n- Evaluating model performance metrics (AUC, etc.)\n- Numerical optimization and sampling methods\n\n### Python Implementation: Rectangle Rule\n\nThe rectangle rule approximates the integral by summing rectangles under the curve. For a partition \\( a = x_0 < x_1 < \\cdots < x_n = b \\):\n\\[\n\\int_a^b f(x) \\, dx \\approx h \\sum_{i=0}^{n-1} f(x_i)\n\\]\nwhere \\( h = (b-a)/n \\) is the step size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import integrate\n\ndef rectangle_rule():\n    \"\"\"\n    Implement rectangle rule for numerical integration.\n    \n    Mathematical foundation:\n    - Approximates ∫f(x)dx ≈ h∑f(x_i) where h = (b-a)/n\n    - Uses left endpoints of subintervals\n    - Error bound: |E| ≤ (b-a)²M/(2n) where M = max|f''(x)|\n    - Order of accuracy: O(h)\n    \"\"\"\n    \n    # Example: ∫x² dx from 0 to 2\n    def f(x):\n        return x**2\n    \n    a, b = 0, 2\n    exact_value = 8/3  # Exact value: x³/3 from 0 to 2\n    \n    # Rectangle rule with different numbers of subintervals\n    n_values = [5, 10, 20, 50, 100]\n    approximations = []\n    \n    print(\"Rectangle Rule for ∫x² dx from 0 to 2:\")\n    print(f\"Exact value: {exact_value:.6f}\")\n    print()\n    \n    for n in n_values:\n        h = (b - a) / n  # Step size\n        x_points = np.linspace(a, b, n+1)\n        y_points = f(x_points[:-1])  # Left endpoints\n        \n        # Rectangle rule: h * sum of function values at left endpoints\n        approximation = h * np.sum(y_points)\n        approximations.append(approximation)\n        \n        error = abs(approximation - exact_value)\n        print(f\"n = {n:3d}: Approximation = {approximation:.6f}, Error = {error:.6f}\")\n    \n    return n_values, approximations, exact_value\n\nn_values, approximations, exact_value = rectangle_rule()\n\n# Visualize rectangle rule\ndef visualize_rectangle_rule():\n    \"\"\"\n    Visualize the rectangle rule approximation.\n    Shows how rectangles approximate the area under the curve.\n    \"\"\"\n    def f(x):\n        return x**2\n    \n    a, b = 0, 2\n    n = 10\n    h = (b - a) / n\n    x_points = np.linspace(a, b, n+1)\n    y_points = f(x_points[:-1])\n    \n    x_plot = np.linspace(a, b, 1000)\n    y_plot = f(x_plot)\n    \n    plt.figure(figsize=(12, 8))\n    \n    # Plot function\n    plt.plot(x_plot, y_plot, 'b-', linewidth=2, label='f(x) = x²')\n    \n    # Plot rectangles\n    for i in range(n):\n        x_left = x_points[i]\n        x_right = x_points[i+1]\n        y_height = y_points[i]\n        \n        # Draw rectangle\n        plt.bar(x_left, y_height, width=h, alpha=0.3, color='red', align='edge')\n        \n        # Add rectangle borders\n        plt.plot([x_left, x_left, x_right, x_right, x_left], \n                [0, y_height, y_height, 0, 0], 'r-', linewidth=1)\n    \n    plt.xlabel('x')\n    plt.ylabel('f(x)')\n    plt.title('Rectangle Rule for Numerical Integration\\n(Red rectangles approximate area under curve)')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nvisualize_rectangle_rule()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:**\n- The rectangle rule approximates the integral by summing the areas of rectangles\n- Each rectangle has height equal to the function value at the left endpoint\n- The approximation improves as the number of subintervals increases\n- The visualization shows how rectangles approximate the area under the curve\n\n---\n\n### Python Implementation: Trapezoidal Rule\n\nThe trapezoidal rule uses linear interpolation between points. For a partition \\( a = x_0 < x_1 < \\cdots < x_n = b \\):\n\\[\n\\int_a^b f(x) \\, dx \\approx \\frac{h}{2} \\left(f(x_0) + 2\\sum_{i=1}^{n-1} f(x_i) + f(x_n)\\right)\n\\]\nwhere \\( h = (b-a)/n \\) is the step size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trapezoidal_rule():\n    \"\"\"\n    Implement trapezoidal rule for numerical integration.\n    \n    Mathematical foundation:\n    - Approximates ∫f(x)dx ≈ (h/2)[f(x₀) + 2∑f(x_i) + f(xₙ)]\n    - Uses linear interpolation between points\n    - Error bound: |E| ≤ (b-a)³M/(12n²) where M = max|f''(x)|\n    - Order of accuracy: O(h²) - better than rectangle rule\n    \"\"\"\n    \n    def f(x):\n        return x**2\n    \n    a, b = 0, 2\n    exact_value = 8/3\n    \n    # Trapezoidal rule with different numbers of subintervals\n    n_values = [5, 10, 20, 50, 100]\n    approximations = []\n    \n    print(\"Trapezoidal Rule for ∫x² dx from 0 to 2:\")\n    print(f\"Exact value: {exact_value:.6f}\")\n    print()\n    \n    for n in n_values:\n        h = (b - a) / n\n        x_points = np.linspace(a, b, n+1)\n        y_points = f(x_points)\n        \n        # Trapezoidal rule: h/2 * (f(x₀) + 2f(x₁) + 2f(x₂) + ... + 2f(xₙ₋₁) + f(xₙ))\n        approximation = h/2 * (y_points[0] + 2*np.sum(y_points[1:-1]) + y_points[-1])\n        approximations.append(approximation)\n        \n        error = abs(approximation - exact_value)\n        print(f\"n = {n:3d}: Approximation = {approximation:.6f}, Error = {error:.6f}\")\n    \n    return n_values, approximations, exact_value\n\nn_values, approximations, exact_value = trapezoidal_rule()\n\n# Visualize trapezoidal rule\ndef visualize_trapezoidal_rule():\n    \"\"\"\n    Visualize the trapezoidal rule approximation.\n    Shows how trapezoids approximate the area under the curve.\n    \"\"\"\n    def f(x):\n        return x**2\n    \n    a, b = 0, 2\n    n = 10\n    h = (b - a) / n\n    x_points = np.linspace(a, b, n+1)\n    y_points = f(x_points)\n    \n    x_plot = np.linspace(a, b, 1000)\n    y_plot = f(x_plot)\n    \n    plt.figure(figsize=(12, 8))\n    \n    # Plot function\n    plt.plot(x_plot, y_plot, 'b-', linewidth=2, label='f(x) = x²')\n    \n    # Plot trapezoids\n    for i in range(n):\n        x_left = x_points[i]\n        x_right = x_points[i+1]\n        y_left = y_points[i]\n        y_right = y_points[i+1]\n        \n        # Draw trapezoid\n        plt.fill([x_left, x_right, x_right, x_left], [0, 0, y_right, y_left], \n                alpha=0.3, color='red')\n        \n        # Add trapezoid borders\n        plt.plot([x_left, x_right], [y_left, y_right], 'r-', linewidth=2)\n    \n    plt.xlabel('x')\n    plt.ylabel('f(x)')\n    plt.title('Trapezoidal Rule for Numerical Integration\\n(Red trapezoids approximate area under curve)')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nvisualize_trapezoidal_rule()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:**\n- The trapezoidal rule uses linear interpolation between adjacent points\n- Each trapezoid has area \\( \\frac{h}{2}(f(x_i) + f(x_{i+1})) \\)\n- The method is more accurate than the rectangle rule (O(h²) vs O(h))\n- The visualization shows how trapezoids approximate the area under the curve\n\n---\n\n### Python Implementation: Simpson's Rule\n\nSimpson's rule uses quadratic interpolation for even higher accuracy. For an even number of subintervals:\n\\[\n\\int_a^b f(x) \\, dx \\approx \\frac{h}{3} \\left(f(x_0) + 4\\sum_{i=1,3,\\ldots}^{n-1} f(x_i) + 2\\sum_{i=2,4,\\ldots}^{n-2} f(x_i) + f(x_n)\\right)\n\\]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simpsons_rule():\n    \"\"\"\n    Implement Simpson's rule for numerical integration.\n    \n    Mathematical foundation:\n    - Approximates ∫f(x)dx ≈ (h/3)[f(x₀) + 4∑f(x_odd) + 2∑f(x_even) + f(xₙ)]\n    - Uses quadratic interpolation between points\n    - Error bound: |E| ≤ (b-a)⁵M/(180n⁴) where M = max|f⁽⁴⁾(x)|\n    - Order of accuracy: O(h⁴) - much better than trapezoidal rule\n    - Requires even number of subintervals\n    \"\"\"\n    \n    def f(x):\n        return x**2\n    \n    a, b = 0, 2\n    exact_value = 8/3\n    \n    # Simpson's rule with different numbers of subintervals (must be even)\n    n_values = [6, 10, 20, 50, 100]\n    approximations = []\n    \n    print(\"Simpson's Rule for ∫x² dx from 0 to 2:\")\n    print(f\"Exact value: {exact_value:.6f}\")\n    print()\n    \n    for n in n_values:\n        h = (b - a) / n\n        x_points = np.linspace(a, b, n+1)\n        y_points = f(x_points)\n        \n        # Simpson's rule: h/3 * (f(x₀) + 4f(x₁) + 2f(x₂) + 4f(x₃) + ... + 4f(xₙ₋₁) + f(xₙ))\n        approximation = h/3 * (y_points[0] + 4*np.sum(y_points[1:-1:2]) + \n                              2*np.sum(y_points[2:-1:2]) + y_points[-1])\n        approximations.append(approximation)\n        \n        error = abs(approximation - exact_value)\n        print(f\"n = {n:3d}: Approximation = {approximation:.6f}, Error = {error:.6f}\")\n    \n    return n_values, approximations, exact_value\n\nn_values, approximations, exact_value = simpsons_rule()\n\n# Compare different integration methods\ndef compare_integration_methods():\n    \"\"\"\n    Compare the accuracy of different numerical integration methods.\n    Shows how error decreases with increasing n for each method.\n    \"\"\"\n    def f(x):\n        return x**2\n    \n    a, b = 0, 2\n    exact_value = 8/3\n    n = 20\n    \n    # Rectangle rule\n    h = (b - a) / n\n    x_points = np.linspace(a, b, n+1)\n    y_points = f(x_points[:-1])\n    rectangle_approx = h * np.sum(y_points)\n    \n    # Trapezoidal rule\n    y_points_full = f(x_points)\n    trapezoidal_approx = h/2 * (y_points_full[0] + 2*np.sum(y_points_full[1:-1]) + y_points_full[-1])\n    \n    # Simpson's rule\n    simpson_approx = h/3 * (y_points_full[0] + 4*np.sum(y_points_full[1:-1:2]) + \n                           2*np.sum(y_points_full[2:-1:2]) + y_points_full[-1])\n    \n    print(\"Comparison of Integration Methods (n = 20):\")\n    print(f\"Exact value: {exact_value:.6f}\")\n    print(f\"Rectangle rule: {rectangle_approx:.6f}, Error: {abs(rectangle_approx - exact_value):.6f}\")\n    print(f\"Trapezoidal rule: {trapezoidal_approx:.6f}, Error: {abs(trapezoidal_approx - exact_value):.6f}\")\n    print(f\"Simpson's rule: {simpson_approx:.6f}, Error: {abs(simpson_approx - exact_value):.6f}\")\n    \n    return rectangle_approx, trapezoidal_approx, simpson_approx\n\nrectangle_approx, trapezoidal_approx, simpson_approx = compare_integration_methods()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:**\n- Simpson's rule uses quadratic interpolation for higher accuracy\n- The method requires an even number of subintervals\n- Error decreases as O(h⁴), making it much more accurate than other methods\n- The comparison shows the relative accuracy of different methods\n\n---\n\n## 10.2 Numerical Differentiation\n\n### Mathematical Foundations\n\nNumerical differentiation approximates derivatives when analytical differentiation is difficult. The goal is to compute:\n\\[\nf'(x) \\approx \\frac{f(x+h) - f(x)}{h}\n\\]\nfor some small step size \\( h \\).\n\n**Key Methods:**\n- **Forward Difference:** \\( f'(x) \\approx \\frac{f(x+h) - f(x)}{h} \\)\n- **Backward Difference:** \\( f'(x) \\approx \\frac{f(x) - f(x-h)}{h} \\)\n- **Central Difference:** \\( f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h} \\)\n\n**Error Analysis:**\n- Forward/Backward: Error \\( O(h) \\)\n- Central: Error \\( O(h²) \\) - more accurate\n- Trade-off between accuracy and step size\n\n**Relevance to AI/ML:**\n- Computing gradients for optimization algorithms\n- Sensitivity analysis of model parameters\n- Finite difference methods for complex functions\n\n### Python Implementation: Finite Difference Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def finite_differences():\n    \"\"\"\n    Implement finite difference methods for numerical differentiation.\n    \n    Mathematical foundation:\n    - Forward difference: f'(x) ≈ (f(x+h) - f(x))/h\n    - Backward difference: f'(x) ≈ (f(x) - f(x-h))/h  \n    - Central difference: f'(x) ≈ (f(x+h) - f(x-h))/(2h)\n    - Error analysis: Forward/Backward O(h), Central O(h²)\n    \"\"\"\n    \n    def f(x):\n        return x**3\n    \n    x0 = 1.0\n    exact_derivative = 3 * x0**2  # f'(x) = 3x²\n    \n    # Different step sizes\n    h_values = [0.1, 0.05, 0.01, 0.005, 0.001]\n    \n    print(\"Numerical Differentiation Methods:\")\n    print(f\"Function: f(x) = x³\")\n    print(f\"Point: x = {x0}\")\n    print(f\"Exact derivative: f'({x0}) = {exact_derivative}\")\n    print()\n    \n    # Forward difference: f'(x) ≈ (f(x+h) - f(x))/h\n    print(\"Forward Difference Method:\")\n    for h in h_values:\n        forward_diff = (f(x0 + h) - f(x0)) / h\n        error = abs(forward_diff - exact_derivative)\n        print(f\"h = {h:.3f}: Approximation = {forward_diff:.6f}, Error = {error:.6f}\")\n    \n    print()\n    \n    # Backward difference: f'(x) ≈ (f(x) - f(x-h))/h\n    print(\"Backward Difference Method:\")\n    for h in h_values:\n        backward_diff = (f(x0) - f(x0 - h)) / h\n        error = abs(backward_diff - exact_derivative)\n        print(f\"h = {h:.3f}: Approximation = {backward_diff:.6f}, Error = {error:.6f}\")\n    \n    print()\n    \n    # Central difference: f'(x) ≈ (f(x+h) - f(x-h))/(2h)\n    print(\"Central Difference Method:\")\n    for h in h_values:\n        central_diff = (f(x0 + h) - f(x0 - h)) / (2*h)\n        error = abs(central_diff - exact_derivative)\n        print(f\"h = {h:.3f}: Approximation = {central_diff:.6f}, Error = {error:.6f}\")\n    \n    return h_values, exact_derivative\n\nh_values, exact_derivative = finite_differences()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:**\n- Forward difference uses the function value at the current point and one step ahead\n- Backward difference uses the function value at the current point and one step back\n- Central difference uses points on both sides, providing better accuracy\n- The error analysis shows how accuracy depends on step size and method choice\n\n---\n\n## 10.3 Root Finding Methods\n\n### Bisection Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bisection_method():\n    \"\"\"Implement bisection method for finding roots\"\"\"\n    \n    def f(x):\n        return x**2 - 4  # Roots at x = ±2\n    \n    a, b = 0, 3  # Initial interval [0, 3]\n    tolerance = 1e-6\n    max_iterations = 100\n    \n    print(\"Bisection Method:\")\n    print(f\"Function: f(x) = x² - 4\")\n    print(f\"Initial interval: [{a}, {b}]\")\n    print(f\"Tolerance: {tolerance}\")\n    print()\n    \n    iterations = []\n    errors = []\n    \n    for i in range(max_iterations):\n        c = (a + b) / 2  # Midpoint\n        f_c = f(c)\n        \n        iterations.append(i + 1)\n        errors.append(abs(f_c))\n        \n        print(f\"Iteration {i+1:2d}: c = {c:.6f}, f(c) = {f_c:.6f}\")\n        \n        if abs(f_c) < tolerance:\n            print(f\"\\nRoot found: x = {c:.6f}\")\n            print(f\"Function value: f({c:.6f}) = {f_c:.6f}\")\n            print(f\"Number of iterations: {i+1}\")\n            break\n        \n        if f(a) * f_c < 0:\n            b = c  # Root is in left half\n        else:\n            a = c  # Root is in right half\n    else:\n        print(\"Maximum iterations reached\")\n    \n    return iterations, errors, c\n\niterations, errors, root = bisection_method()\n\n# Visualize bisection method\ndef visualize_bisection_method():\n    def f(x):\n        return x**2 - 4\n    \n    a, b = 0, 3\n    x_plot = np.linspace(-1, 4, 1000)\n    y_plot = f(x_plot)\n    \n    plt.figure(figsize=(12, 8))\n    \n    # Plot function\n    plt.plot(x_plot, y_plot, 'b-', linewidth=2, label='f(x) = x² - 4')\n    plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n    \n    # Plot initial interval\n    plt.plot([a, a], [0, f(a)], 'r--', alpha=0.7, label='Initial interval')\n    plt.plot([b, b], [0, f(b)], 'r--', alpha=0.7)\n    plt.scatter([a, b], [f(a), f(b)], c='red', s=100)\n    \n    # Plot convergence\n    plt.scatter(root, 0, c='green', s=200, label=f'Root: x = {root:.6f}')\n    \n    plt.xlabel('x')\n    plt.ylabel('f(x)')\n    plt.title('Bisection Method')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nvisualize_bisection_method()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Newton's Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def newton_method():\n    \"\"\"Implement Newton's method for finding roots\"\"\"\n    \n    def f(x):\n        return x**2 - 4\n    \n    def f_prime(x):\n        return 2 * x\n    \n    x0 = 3.0  # Initial guess\n    tolerance = 1e-6\n    max_iterations = 100\n    \n    print(\"Newton's Method:\")\n    print(f\"Function: f(x) = x² - 4\")\n    print(f\"Derivative: f'(x) = 2x\")\n    print(f\"Initial guess: x₀ = {x0}\")\n    print(f\"Tolerance: {tolerance}\")\n    print()\n    \n    iterations = []\n    errors = []\n    x_values = [x0]\n    \n    for i in range(max_iterations):\n        x_old = x_values[-1]\n        f_x = f(x_old)\n        f_prime_x = f_prime(x_old)\n        \n        if abs(f_prime_x) < 1e-10:\n            print(\"Derivative too close to zero\")\n            break\n        \n        x_new = x_old - f_x / f_prime_x\n        x_values.append(x_new)\n        \n        iterations.append(i + 1)\n        errors.append(abs(f_x))\n        \n        print(f\"Iteration {i+1:2d}: x = {x_new:.6f}, f(x) = {f_x:.6f}\")\n        \n        if abs(f_x) < tolerance:\n            print(f\"\\nRoot found: x = {x_new:.6f}\")\n            print(f\"Function value: f({x_new:.6f}) = {f_x:.6f}\")\n            print(f\"Number of iterations: {i+1}\")\n            break\n    else:\n        print(\"Maximum iterations reached\")\n    \n    return iterations, errors, x_values\n\niterations, errors, x_values = newton_method()\n\n# Visualize Newton's method\ndef visualize_newton_method():\n    def f(x):\n        return x**2 - 4\n    \n    def f_prime(x):\n        return 2 * x\n    \n    x0 = 3.0\n    x_plot = np.linspace(-1, 4, 1000)\n    y_plot = f(x_plot)\n    \n    plt.figure(figsize=(12, 8))\n    \n    # Plot function\n    plt.plot(x_plot, y_plot, 'b-', linewidth=2, label='f(x) = x² - 4')\n    plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n    \n    # Plot Newton iterations\n    for i in range(len(x_values) - 1):\n        x_old = x_values[i]\n        x_new = x_values[i + 1]\n        f_old = f(x_old)\n        f_prime_old = f_prime(x_old)\n        \n        # Plot tangent line\n        x_tangent = np.linspace(x_old - 0.5, x_old + 0.5, 100)\n        y_tangent = f_old + f_prime_old * (x_tangent - x_old)\n        \n        plt.plot(x_tangent, y_tangent, 'r--', alpha=0.5)\n        plt.scatter(x_old, f_old, c='red', s=100)\n        plt.plot([x_old, x_new], [f_old, 0], 'g-', alpha=0.7)\n    \n    plt.scatter(x_values[-1], 0, c='green', s=200, label=f'Root: x = {x_values[-1]:.6f}')\n    \n    plt.xlabel('x')\n    plt.ylabel('f(x)')\n    plt.title(\"Newton's Method\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nvisualize_newton_method()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.4 Applications in Machine Learning\n\n### Gradient Descent with Numerical Gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def numerical_gradient_descent():\n    \"\"\"Implement gradient descent using numerical gradients\"\"\"\n    \n    def objective_function(x, y):\n        return x**2 + y**2\n    \n    def numerical_gradient(f, x, y, h=1e-6):\n        \"\"\"Compute numerical gradient using finite differences\"\"\"\n        grad_x = (f(x + h, y) - f(x - h, y)) / (2 * h)\n        grad_y = (f(x, y + h) - f(x, y - h)) / (2 * h)\n        return np.array([grad_x, grad_y])\n    \n    # Gradient descent with numerical gradients\n    def gradient_descent_numerical(f, start_point, learning_rate=0.1, max_iter=100):\n        x, y = start_point\n        history = [(x, y)]\n        \n        for i in range(max_iter):\n            gradient = numerical_gradient(f, x, y)\n            x = x - learning_rate * gradient[0]\n            y = y - learning_rate * gradient[1]\n            history.append((x, y))\n            \n            # Check convergence\n            if np.linalg.norm(gradient) < 1e-6:\n                break\n        \n        return np.array(history)\n    \n    # Run gradient descent\n    start_point = (2.0, 2.0)\n    path = gradient_descent_numerical(objective_function, start_point)\n    \n    print(\"Numerical Gradient Descent:\")\n    print(f\"Starting point: {start_point}\")\n    print(f\"Final point: {path[-1]}\")\n    print(f\"Final value: {objective_function(path[-1, 0], path[-1, 1]):.6f}\")\n    print(f\"Number of iterations: {len(path)}\")\n    \n    return path, objective_function\n\npath, objective_function = numerical_gradient_descent()\n\n# Visualize numerical gradient descent\ndef visualize_numerical_gradient_descent():\n    x = np.linspace(-3, 3, 100)\n    y = np.linspace(-3, 3, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = X**2 + Y**2\n    \n    plt.figure(figsize=(10, 8))\n    \n    # Contour plot\n    contour = plt.contour(X, Y, Z, levels=20)\n    plt.clabel(contour, inline=True, fontsize=8)\n    \n    # Optimization path\n    plt.plot(path[:, 0], path[:, 1], 'r-', linewidth=2, label='Numerical gradient descent')\n    plt.scatter(path[0, 0], path[0, 1], c='red', s=100, label='Start')\n    plt.scatter(path[-1, 0], path[-1, 1], c='green', s=100, label='End')\n    \n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Numerical Gradient Descent on f(x,y) = x² + y²')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nvisualize_numerical_gradient_descent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\n- **Numerical Integration**: Rectangle, trapezoidal, and Simpson's rules for approximating definite integrals\n- **Numerical Differentiation**: Forward, backward, and central difference methods for approximating derivatives\n- **Root Finding**: Bisection and Newton's methods for finding zeros of functions\n- **Applications**: Gradient descent with numerical gradients when analytical derivatives are unavailable\n- **Error Analysis**: Understanding convergence and accuracy of numerical methods\n\n## Next Steps\n\nNumerical methods provide essential tools for solving calculus problems computationally. These techniques are fundamental to scientific computing, machine learning algorithms, and engineering applications where analytical solutions are not feasible."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}