{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimization Techniques\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-orange.svg)](https://matplotlib.org/)\n[![SymPy](https://img.shields.io/badge/SymPy-1.10+-purple.svg)](https://www.sympy.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-red.svg)](https://scipy.org/)\n\n## Introduction\n\nOptimization is the process of finding the best solution to a problem, typically involving finding the minimum or maximum of a function. This is fundamental to machine learning, where we optimize loss functions to train models. Calculus provides the mathematical foundation for most optimization algorithms used in machine learning and data science.\n\n### Why Optimization Matters in AI/ML\n\nOptimization is the core of machine learning and artificial intelligence:\n\n1. **Model Training**: Finding optimal parameters that minimize loss functions\n2. **Hyperparameter Tuning**: Optimizing learning rates, regularization parameters, etc.\n3. **Feature Selection**: Finding optimal subsets of features\n4. **Neural Network Architecture**: Optimizing network structure and connections\n5. **Reinforcement Learning**: Finding optimal policies and value functions\n\n### Mathematical Foundation\n\nOptimization problems can be formulated as:\n\n**Unconstrained Optimization**: min f(x) or max f(x)\n\n**Constrained Optimization**: min f(x) subject to g(x) = 0, h(x) ≤ 0\n\nwhere f(x) is the objective function, and g(x), h(x) are constraint functions.\n\n### Types of Optimization Problems\n\n1. **Linear Programming**: Linear objective and constraints\n2. **Nonlinear Programming**: Nonlinear objective and/or constraints\n3. **Convex Optimization**: Convex objective and constraints\n4. **Non-convex Optimization**: Non-convex objective or constraints\n5. **Discrete Optimization**: Integer or binary variables\n\n### Optimization Landscape\n\nUnderstanding the optimization landscape is crucial:\n- **Local Minima/Maxima**: Points where the function is lower/higher than nearby points\n- **Global Minima/Maxima**: Points where the function attains its lowest/highest value\n- **Saddle Points**: Points where the gradient is zero but not an extremum\n- **Plateaus**: Regions where the gradient is very small\n\n## 8.1 First and Second Derivative Tests\n\n### Critical Points and Extrema\n\nThe foundation of optimization lies in understanding where functions attain their extreme values. Critical points are where the first derivative is zero or undefined.\n\n### Mathematical Theory\n\n**First Derivative Test**: If f'(c) = 0 and f'(x) changes sign at x = c, then f has a local extremum at c.\n\n**Second Derivative Test**: If f'(c) = 0, then:\n- If f''(c) > 0: f has a local minimum at c\n- If f''(c) < 0: f has a local maximum at c\n- If f''(c) = 0: The test is inconclusive\n\n### Why These Tests Matter\n\n1. **Gradient Descent**: Relies on first derivatives to find descent directions\n2. **Newton's Method**: Uses second derivatives for faster convergence\n3. **Convergence Analysis**: Understanding local vs global optima\n4. **Model Interpretability**: Understanding where models are most sensitive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport sympy as sp\nfrom scipy.optimize import minimize, minimize_scalar\n\n# Comprehensive critical point analysis\ndef find_critical_points_comprehensive():\n    x = sp.Symbol('x')\n    \n    print(\"=== CRITICAL POINT ANALYSIS ===\\n\")\n    \n    # Example 1: Polynomial function\n    print(\"1. POLYNOMIAL FUNCTION\")\n    f1 = x**3 - 3*x**2 + 2\n    f1_prime = sp.diff(f1, x)\n    f1_double_prime = sp.diff(f1_prime, x)\n    \n    print(f\"   f(x) = {f1}\")\n    print(f\"   f'(x) = {f1_prime}\")\n    print(f\"   f''(x) = {f1_double_prime}\")\n    \n    # Find critical points\n    critical_points1 = sp.solve(f1_prime, x)\n    print(f\"   Critical points: {critical_points1}\")\n    \n    # Classify critical points\n    for point in critical_points1:\n        second_deriv = f1_double_prime.subs(x, point)\n        func_value = f1.subs(x, point)\n        \n        if second_deriv > 0:\n            print(f\"   x = {point}: Local minimum (f''({point}) = {second_deriv}, f({point}) = {func_value})\")\n        elif second_deriv < 0:\n            print(f\"   x = {point}: Local maximum (f''({point}) = {second_deriv}, f({point}) = {func_value})\")\n        else:\n            print(f\"   x = {point}: Saddle point or inflection point (f''({point}) = {second_deriv})\")\n    \n    # Example 2: Trigonometric function\n    print(\"\\n2. TRIGONOMETRIC FUNCTION\")\n    f2 = sp.sin(x) + 0.5*x**2\n    f2_prime = sp.diff(f2, x)\n    f2_double_prime = sp.diff(f2_prime, x)\n    \n    print(f\"   f(x) = {f2}\")\n    print(f\"   f'(x) = {f2_prime}\")\n    print(f\"   f''(x) = {f2_double_prime}\")\n    \n    # Find critical points numerically for complex functions\n    critical_points2 = sp.solve(f2_prime, x)\n    print(f\"   Critical points: {critical_points2}\")\n    \n    # Example 3: Exponential function\n    print(\"\\n3. EXPONENTIAL FUNCTION\")\n    f3 = sp.exp(-x**2/2) * sp.sin(x)\n    f3_prime = sp.diff(f3, x)\n    f3_double_prime = sp.diff(f3_prime, x)\n    \n    print(f\"   f(x) = {f3}\")\n    print(f\"   f'(x) = {f3_prime}\")\n    print(f\"   f''(x) = {f3_double_prime}\")\n    \n    return f1, f1_prime, f1_double_prime, critical_points1\n\nf1, f1_prime, f1_double_prime, critical_points1 = find_critical_points_comprehensive()\n\n# Advanced visualization of critical points\ndef visualize_critical_points_advanced():\n    x_vals = np.linspace(-1, 3, 1000)\n    \n    # Convert symbolic expressions to numerical functions\n    f_numeric = sp.lambdify(sp.Symbol('x'), f1)\n    f_prime_numeric = sp.lambdify(sp.Symbol('x'), f1_prime)\n    f_double_prime_numeric = sp.lambdify(sp.Symbol('x'), f1_double_prime)\n    \n    y_vals = f_numeric(x_vals)\n    dy_vals = f_prime_numeric(x_vals)\n    ddy_vals = f_double_prime_numeric(x_vals)\n    \n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Function and critical points\n    ax1.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = x³ - 3x² + 2')\n    for point in critical_points1:\n        y_point = f_numeric(point)\n        ax1.scatter(point, y_point, c='red', s=100, zorder=5, \n                   label=f'Critical point: ({point:.2f}, {y_point:.2f})')\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('f(x)')\n    ax1.set_title('Function and Critical Points')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # First derivative\n    ax2.plot(x_vals, dy_vals, 'r-', linewidth=2, label=\"f'(x)\")\n    ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n    for point in critical_points1:\n        ax2.scatter(point, 0, c='red', s=100, zorder=5)\n    ax2.set_xlabel('x')\n    ax2.set_ylabel(\"f'(x)\")\n    ax2.set_title('First Derivative (Critical Points at Zeros)')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Second derivative\n    ax3.plot(x_vals, ddy_vals, 'g-', linewidth=2, label=\"f''(x)\")\n    ax3.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n    for point in critical_points1:\n        second_deriv = f_double_prime_numeric(point)\n        ax3.scatter(point, second_deriv, c='red', s=100, zorder=5,\n                   label=f'f''({point:.2f}) = {second_deriv:.2f}')\n    ax3.set_xlabel('x')\n    ax3.set_ylabel(\"f''(x)\")\n    ax3.set_title('Second Derivative (Classifies Critical Points)')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    # Optimization landscape\n    ax4.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x)')\n    ax4.fill_between(x_vals, y_vals, alpha=0.3, color='blue')\n    \n    # Mark different regions\n    for i, point in enumerate(critical_points1):\n        y_point = f_numeric(point)\n        second_deriv = f_double_prime_numeric(point)\n        \n        if second_deriv > 0:\n            ax4.scatter(point, y_point, c='green', s=200, zorder=5, \n                       marker='^', label=f'Local min: ({point:.2f}, {y_point:.2f})')\n        elif second_deriv < 0:\n            ax4.scatter(point, y_point, c='red', s=200, zorder=5, \n                       marker='v', label=f'Local max: ({point:.2f}, {y_point:.2f})')\n        else:\n            ax4.scatter(point, y_point, c='orange', s=200, zorder=5, \n                       marker='s', label=f'Saddle point: ({point:.2f}, {y_point:.2f})')\n    \n    ax4.set_xlabel('x')\n    ax4.set_ylabel('f(x)')\n    ax4.set_title('Optimization Landscape')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_critical_points_advanced()\n\n# Numerical optimization comparison\ndef numerical_optimization_comparison():\n    print(\"\\n=== NUMERICAL OPTIMIZATION COMPARISON ===\\n\")\n    \n    def objective_function(x):\n        return x**3 - 3*x**2 + 2\n    \n    def gradient_function(x):\n        return 3*x**2 - 6*x\n    \n    def hessian_function(x):\n        return 6*x - 6\n    \n    # Test different optimization methods\n    methods = [\n        ('BFGS', 'BFGS (quasi-Newton)'),\n        ('CG', 'Conjugate Gradient'),\n        ('L-BFGS-B', 'L-BFGS-B (bounded)'),\n        ('TNC', 'Truncated Newton'),\n        ('SLSQP', 'Sequential Least Squares')\n    ]\n    \n    # Test from different starting points\n    starting_points = [-0.5, 0.5, 1.5, 2.5]\n    \n    print(\"Optimization Results from Different Starting Points:\")\n    print(\"Method\\t\\tStart\\t\\tOptimal x\\tOptimal f(x)\\tIterations\")\n    print(\"-\" * 80)\n    \n    for method_name, method_desc in methods:\n        for start_point in starting_points:\n            try:\n                result = minimize(objective_function, start_point, \n                                method=method_name, jac=gradient_function)\n                \n                print(f\"{method_name:12s}\\t{start_point:6.1f}\\t{result.x[0]:10.6f}\\t{result.fun:12.6f}\\t{result.nit:10d}\")\n            except:\n                print(f\"{method_name:12s}\\t{start_point:6.1f}\\t{'Failed':>10s}\\t{'N/A':>12s}\\t{'N/A':>10s}\")\n        print()\n\nnumerical_optimization_comparison()\n\n# Advanced critical point analysis with multiple functions\ndef advanced_critical_point_analysis():\n    print(\"\\n=== ADVANCED CRITICAL POINT ANALYSIS ===\\n\")\n    \n    x = sp.Symbol('x')\n    \n    # Test functions with different characteristics\n    test_functions = [\n        (x**4 - 4*x**2, \"Quartic function with multiple extrema\"),\n        (sp.sin(x) + 0.1*x**2, \"Sine with quadratic trend\"),\n        (sp.exp(-x**2/2) * sp.cos(x), \"Gaussian modulated cosine\"),\n        (x**3 - x, \"Cubic with inflection point\"),\n        (sp.log(1 + x**2), \"Logarithmic function\")\n    ]\n    \n    for i, (func, description) in enumerate(test_functions, 1):\n        print(f\"{i}. {description}\")\n        print(f\"   f(x) = {func}\")\n        \n        # Compute derivatives\n        f_prime = sp.diff(func, x)\n        f_double_prime = sp.diff(f_prime, x)\n        \n        print(f\"   f'(x) = {f_prime}\")\n        print(f\"   f''(x) = {f_double_prime}\")\n        \n        # Find critical points\n        try:\n            critical_points = sp.solve(f_prime, x)\n            print(f\"   Critical points: {critical_points}\")\n            \n            # Classify critical points\n            for point in critical_points:\n                if point.is_real:  # Only consider real critical points\n                    second_deriv = f_double_prime.subs(x, point)\n                    func_value = func.subs(x, point)\n                    \n                    if second_deriv > 0:\n                        print(f\"     x = {point}: Local minimum (f''({point}) = {second_deriv:.4f})\")\n                    elif second_deriv < 0:\n                        print(f\"     x = {point}: Local maximum (f''({point}) = {second_deriv:.4f})\")\n                    else:\n                        print(f\"     x = {point}: Saddle point or inflection point\")\n        except:\n            print(f\"   Critical points: Could not solve analytically\")\n        \n        print()\n\nadvanced_critical_point_analysis()\n\n# Convergence analysis of optimization methods\ndef convergence_analysis():\n    print(\"\\n=== CONVERGENCE ANALYSIS ===\\n\")\n    \n    def objective_function(x):\n        return x**3 - 3*x**2 + 2\n    \n    def gradient_function(x):\n        return 3*x**2 - 6*x\n    \n    # Track optimization progress\n    def track_optimization(start_point, method='BFGS'):\n        history = []\n        \n        def callback(xk):\n            history.append(xk[0])\n        \n        result = minimize(objective_function, start_point, \n                         method=method, jac=gradient_function,\n                         callback=callback, options={'maxiter': 100})\n        \n        return result, history\n    \n    # Test different starting points\n    starting_points = [0.5, 1.5, 2.5]\n    methods = ['BFGS', 'CG', 'L-BFGS-B']\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    for i, method in enumerate(methods):\n        ax = axes[i]\n        \n        for start_point in starting_points:\n            result, history = track_optimization(start_point, method)\n            \n            # Plot convergence\n            iterations = range(len(history))\n            ax.plot(iterations, history, 'o-', linewidth=2, \n                   label=f'Start: {start_point}')\n            \n            # Mark final point\n            ax.scatter(len(history)-1, history[-1], c='red', s=100, zorder=5)\n        \n        ax.set_xlabel('Iteration')\n        ax.set_ylabel('x value')\n        ax.set_title(f'{method} Convergence')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Analyze convergence rates\n    print(\"Convergence Analysis:\")\n    for method in methods:\n        print(f\"\\n{method} method:\")\n        for start_point in starting_points:\n            result, history = track_optimization(start_point, method)\n            print(f\"  Start {start_point}: {len(history)} iterations, final x = {result.x[0]:.6f}\")\n\nconvergence_analysis()\n\n### Applications in Machine Learning\n\nFirst and second derivative tests are fundamental to:\n\n1. **Gradient Descent**: Uses first derivatives to find descent directions\n2. **Newton's Method**: Uses both first and second derivatives for faster convergence\n3. **Loss Function Analysis**: Understanding where loss functions have minima\n4. **Model Convergence**: Analyzing whether optimization algorithms will converge\n5. **Hyperparameter Optimization**: Finding optimal learning rates and other parameters\n\n## 8.2 Constrained Optimization\n\n### Lagrange Multipliers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python\n# Lagrange multipliers for constrained optimization\ndef lagrange_multipliers_example():\n    \"\"\"\n    Example: Maximize f(x,y) = xy subject to x + y = 10\n    \"\"\"\n    x, y, lambda_var = sp.symbols('x y lambda')\n    \n    # Objective function: f(x,y) = xy\n    f = x * y\n    \n    # Constraint: g(x,y) = x + y - 10 = 0\n    g = x + y - 10\n    \n    # Lagrange function: L = f - λg\n    L = f - lambda_var * g\n    \n    # Partial derivatives\n    dL_dx = sp.diff(L, x)\n    dL_dy = sp.diff(L, y)\n    dL_dlambda = sp.diff(L, lambda_var)\n    \n    print(\"Lagrange equations:\")\n    print(f\"∂L/∂x = {dL_dx} = 0\")\n    print(f\"∂L/∂y = {dL_dy} = 0\")\n    print(f\"∂L/∂λ = {dL_dlambda} = 0\")\n    \n    # Solve the system of equations\n    solution = sp.solve([dL_dx, dL_dy, dL_dlambda], [x, y, lambda_var])\n    print(f\"\\nSolution: {solution}\")\n    \n    # Verify the solution\n    if solution:\n        x_opt, y_opt, lambda_opt = solution[0]\n        print(f\"Optimal x = {x_opt}\")\n        print(f\"Optimal y = {y_opt}\")\n        print(f\"Optimal value = {f.subs([(x, x_opt), (y, y_opt)])}\")\n        print(f\"Constraint satisfied: {g.subs([(x, x_opt), (y, y_opt)])}\")\n\nlagrange_multipliers_example()\n\n# Visualize constrained optimization\ndef visualize_constrained_optimization():\n    # Create grid\n    x = np.linspace(0, 10, 100)\n    y = np.linspace(0, 10, 100)\n    X, Y = np.meshgrid(x, y)\n    \n    # Objective function: f(x,y) = xy\n    Z = X * Y\n    \n    # Constraint: x + y = 10\n    constraint_x = np.linspace(0, 10, 100)\n    constraint_y = 10 - constraint_x\n    \n    plt.figure(figsize=(10, 8))\n    \n    # Contour plot of objective function\n    contour = plt.contour(X, Y, Z, levels=20)\n    plt.clabel(contour, inline=True, fontsize=8)\n    \n    # Constraint line\n    plt.plot(constraint_x, constraint_y, 'r-', linewidth=3, label='Constraint: x + y = 10')\n    \n    # Optimal point\n    plt.scatter(5, 5, c='red', s=200, zorder=5, label='Optimal point (5, 5)')\n    \n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Constrained Optimization: Maximize xy subject to x + y = 10')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nvisualize_constrained_optimization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 8.3 Global vs Local Optimization\n\n### Global Optimization Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python\n# Global optimization using different methods\ndef global_optimization_comparison():\n    # Test function with multiple local minima\n    def test_function(x):\n        return np.sin(x) * np.exp(-x/10) + 0.1 * x**2\n    \n    def test_function_gradient(x):\n        return np.cos(x) * np.exp(-x/10) - np.sin(x) * np.exp(-x/10) / 10 + 0.2 * x\n    \n    # Define search range\n    x_range = (0, 20)\n    \n    # Method 1: Local optimization from multiple starting points\n    def multi_start_optimization():\n        n_starts = 10\n        start_points = np.random.uniform(x_range[0], x_range[1], n_starts)\n        results = []\n        \n        for start_point in start_points:\n            result = minimize_scalar(test_function, bounds=x_range, \n                                   method='bounded', x0=start_point)\n            results.append((result.x, result.fun))\n        \n        # Find the best result\n        best_result = min(results, key=lambda x: x[1])\n        return best_result\n    \n    # Method 2: Global optimization using differential evolution\n    from scipy.optimize import differential_evolution\n    \n    def differential_evolution_optimization():\n        result = differential_evolution(test_function, bounds=[x_range])\n        return result.x, result.fun\n    \n    # Method 3: Grid search\n    def grid_search_optimization():\n        x_grid = np.linspace(x_range[0], x_range[1], 1000)\n        y_grid = test_function(x_grid)\n        min_idx = np.argmin(y_grid)\n        return x_grid[min_idx], y_grid[min_idx]\n    \n    # Run all methods\n    print(\"Global Optimization Comparison:\")\n    print(\"-\" * 50)\n    \n    # Multi-start\n    x_opt1, f_opt1 = multi_start_optimization()\n    print(f\"Multi-start optimization: x = {x_opt1:.6f}, f(x) = {f_opt1:.6f}\")\n    \n    # Differential evolution\n    x_opt2, f_opt2 = differential_evolution_optimization()\n    print(f\"Differential evolution: x = {x_opt2:.6f}, f(x) = {f_opt2:.6f}\")\n    \n    # Grid search\n    x_opt3, f_opt3 = grid_search_optimization()\n    print(f\"Grid search: x = {x_opt3:.6f}, f(x) = {f_opt3:.6f}\")\n    \n    # Visualize\n    x_vals = np.linspace(x_range[0], x_range[1], 1000)\n    y_vals = test_function(x_vals)\n    \n    plt.figure(figsize=(12, 8))\n    plt.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = sin(x)*exp(-x/10) + 0.1x²')\n    \n    # Mark optimal points\n    plt.scatter(x_opt1, f_opt1, c='red', s=100, label=f'Multi-start: ({x_opt1:.3f}, {f_opt1:.3f})')\n    plt.scatter(x_opt2, f_opt2, c='green', s=100, label=f'Differential evolution: ({x_opt2:.3f}, {f_opt2:.3f})')\n    plt.scatter(x_opt3, f_opt3, c='orange', s=100, label=f'Grid search: ({x_opt3:.3f}, {f_opt3:.3f})')\n    \n    plt.xlabel('x')\n    plt.ylabel('f(x)')\n    plt.title('Global Optimization Methods Comparison')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nglobal_optimization_comparison()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 8.4 Convex Optimization\n\n### Convex Functions and Properties\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python\n# Convex optimization examples\ndef convex_optimization_examples():\n    # Example 1: Quadratic function (convex)\n    def quadratic_function(x):\n        return x**2 + 2*x + 1\n    \n    def quadratic_gradient(x):\n        return 2*x + 2\n    \n    # Example 2: Non-convex function\n    def non_convex_function(x):\n        return x**3 - 3*x**2 + 2\n    \n    def non_convex_gradient(x):\n        return 3*x**2 - 6*x\n    \n    # Test convexity using second derivative\n    def test_convexity():\n        x = sp.Symbol('x')\n        \n        # Quadratic function\n        f1 = x**2 + 2*x + 1\n        f1_double_prime = sp.diff(sp.diff(f1, x), x)\n        print(f\"f(x) = x² + 2x + 1\")\n        print(f\"f''(x) = {f1_double_prime}\")\n        print(f\"Convex: {f1_double_prime >= 0}\")\n        \n        # Non-convex function\n        f2 = x**3 - 3*x**2 + 2\n        f2_double_prime = sp.diff(sp.diff(f2, x), x)\n        print(f\"\\nf(x) = x³ - 3x² + 2\")\n        print(f\"f''(x) = {f2_double_prime}\")\n        print(f\"Convex: {f2_double_prime >= 0}\")\n    \n    test_convexity()\n    \n    # Visualize convex vs non-convex functions\n    x_vals = np.linspace(-2, 4, 1000)\n    y1_vals = quadratic_function(x_vals)\n    y2_vals = non_convex_function(x_vals)\n    \n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(x_vals, y1_vals, 'b-', linewidth=2, label='f(x) = x² + 2x + 1 (Convex)')\n    plt.xlabel('x')\n    plt.ylabel('f(x)')\n    plt.title('Convex Function')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(x_vals, y2_vals, 'r-', linewidth=2, label='f(x) = x³ - 3x² + 2 (Non-convex)')\n    plt.xlabel('x')\n    plt.ylabel('f(x)')\n    plt.title('Non-convex Function')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\nconvex_optimization_examples()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 8.5 Multi-objective Optimization\n\n### Pareto Optimality\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python\n# Multi-objective optimization example\ndef multi_objective_optimization():\n    \"\"\"\n    Example: Minimize both f1(x) = x² and f2(x) = (x-2)²\n    \"\"\"\n    def objective_functions(x):\n        f1 = x**2\n        f2 = (x - 2)**2\n        return f1, f2\n    \n    # Generate Pareto front\n    x_vals = np.linspace(0, 2, 100)\n    f1_vals, f2_vals = objective_functions(x_vals)\n    \n    # Find Pareto optimal solutions\n    pareto_indices = []\n    for i in range(len(x_vals)):\n        is_pareto = True\n        for j in range(len(x_vals)):\n            if i != j:\n                # Check if point j dominates point i\n                if f1_vals[j] <= f1_vals[i] and f2_vals[j] <= f2_vals[i] and \\\n                   (f1_vals[j] < f1_vals[i] or f2_vals[j] < f2_vals[i]):\n                    is_pareto = False\n                    break\n        if is_pareto:\n            pareto_indices.append(i)\n    \n    plt.figure(figsize=(10, 8))\n    \n    # Plot all solutions\n    plt.scatter(f1_vals, f2_vals, c='blue', alpha=0.6, label='All solutions')\n    \n    # Plot Pareto optimal solutions\n    pareto_f1 = f1_vals[pareto_indices]\n    pareto_f2 = f2_vals[pareto_indices]\n    plt.scatter(pareto_f1, pareto_f2, c='red', s=100, label='Pareto optimal solutions')\n    \n    # Connect Pareto optimal solutions\n    plt.plot(pareto_f1, pareto_f2, 'r-', linewidth=2, label='Pareto front')\n    \n    plt.xlabel('f₁(x) = x²')\n    plt.ylabel('f₂(x) = (x-2)²')\n    plt.title('Multi-objective Optimization: Pareto Front')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    \n    print(\"Pareto optimal solutions:\")\n    for i in pareto_indices:\n        print(f\"x = {x_vals[i]:.3f}, f₁ = {f1_vals[i]:.3f}, f₂ = {f2_vals[i]:.3f}\")\n\nmulti_objective_optimization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 8.6 Optimization in Machine Learning\n\n### Hyperparameter Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python\n# Hyperparameter optimization example\ndef hyperparameter_optimization():\n    \"\"\"\n    Example: Optimize learning rate and regularization for a simple model\n    \"\"\"\n    from sklearn.model_selection import cross_val_score\n    from sklearn.linear_model import Ridge\n    from sklearn.datasets import make_regression\n    from sklearn.model_selection import train_test_split\n    \n    # Generate synthetic data\n    X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Define objective function for optimization\n    def objective_function(params):\n        learning_rate, alpha = params\n        \n        # Simple model with Ridge regression\n        model = Ridge(alpha=alpha)\n        \n        # Cross-validation score (negative because we want to maximize)\n        scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n        return -np.mean(scores)  # Return positive MSE\n    \n    # Grid search\n    learning_rates = np.logspace(-3, 0, 10)\n    alphas = np.logspace(-3, 2, 10)\n    \n    best_score = float('inf')\n    best_params = None\n    results = []\n    \n    for lr in learning_rates:\n        for alpha in alphas:\n            score = objective_function([lr, alpha])\n            results.append((lr, alpha, score))\n            \n            if score < best_score:\n                best_score = score\n                best_params = (lr, alpha)\n    \n    print(f\"Best parameters: learning_rate = {best_params[0]:.6f}, alpha = {best_params[1]:.6f}\")\n    print(f\"Best score (MSE): {best_score:.6f}\")\n    \n    # Visualize results\n    results = np.array(results)\n    lr_vals = results[:, 0]\n    alpha_vals = results[:, 1]\n    scores = results[:, 2]\n    \n    plt.figure(figsize=(12, 5))\n    \n    # 3D scatter plot\n    ax1 = plt.subplot(121, projection='3d')\n    scatter = ax1.scatter(lr_vals, alpha_vals, scores, c=scores, cmap='viridis')\n    ax1.set_xlabel('Learning Rate')\n    ax1.set_ylabel('Alpha')\n    ax1.set_zlabel('MSE')\n    ax1.set_title('Hyperparameter Optimization Results')\n    plt.colorbar(scatter)\n    \n    # 2D contour plot\n    ax2 = plt.subplot(122)\n    # Create grid for contour plot\n    lr_grid = np.logspace(-3, 0, 20)\n    alpha_grid = np.logspace(-3, 2, 20)\n    LR, ALPHA = np.meshgrid(lr_grid, alpha_grid)\n    \n    # Interpolate scores for contour plot\n    from scipy.interpolate import griddata\n    points = np.column_stack((lr_vals, alpha_vals))\n    grid_scores = griddata(points, scores, (LR, ALPHA), method='linear')\n    \n    contour = ax2.contour(LR, ALPHA, grid_scores, levels=20)\n    ax2.clabel(contour, inline=True, fontsize=8)\n    ax2.scatter(best_params[0], best_params[1], c='red', s=200, marker='*', \n                label=f'Best: ({best_params[0]:.3f}, {best_params[1]:.3f})')\n    ax2.set_xscale('log')\n    ax2.set_yscale('log')\n    ax2.set_xlabel('Learning Rate')\n    ax2.set_ylabel('Alpha')\n    ax2.set_title('Contour Plot of MSE')\n    ax2.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\nhyperparameter_optimization()\n```\n\n## Summary\n\n- **First and second derivative tests** help identify local extrema\n- **Constrained optimization** uses Lagrange multipliers for equality constraints\n- **Global optimization** methods find the best solution across the entire domain\n- **Convex optimization** guarantees global optimality for convex problems\n- **Multi-objective optimization** finds Pareto optimal solutions\n- **Hyperparameter optimization** is crucial for machine learning model tuning\n\n## Next Steps\n\nUnderstanding optimization techniques enables you to design efficient algorithms, tune machine learning models, and solve complex real-world problems. The next section covers numerical methods for when analytical solutions are not available."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}