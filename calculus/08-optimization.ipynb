{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Optimization Techniques\n",
       "\n",
       "[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)\n",
       "[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n",
       "[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-orange.svg)](https://matplotlib.org/)\n",
       "[![SymPy](https://img.shields.io/badge/SymPy-1.10+-purple.svg)](https://www.sympy.org/)\n",
       "\n",
       "## Introduction\n",
       "\n",
       "Optimization is the process of finding the best solution to a problem, often involving finding minima or maxima of functions."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "from scipy.optimize import minimize\n",
       "\n",
       "plt.style.use('seaborn-v0_8')\n",
       "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 8.1 Gradient Descent\n",
       "\n",
       "Gradient descent is the most fundamental optimization algorithm in machine learning."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Gradient descent implementation\n",
       "def objective_function(x):\n",
       "    return x**2 + 2*x + 1\n",
       "\n",
       "def gradient_function(x):\n",
       "    return 2*x + 2\n",
       "\n",
       "def gradient_descent(start_x, learning_rate, num_iterations):\n",
       "    x = start_x\n",
       "    history = []\n",
       "    \n",
       "    for i in range(num_iterations):\n",
       "        grad = gradient_function(x)\n",
       "        x_new = x - learning_rate * grad\n",
       "        history.append((x, objective_function(x)))\n",
       "        x = x_new\n",
       "    \n",
       "    return x, history\n",
       "\n",
       "# Run optimization\n",
       "optimal_x, history = gradient_descent(start_x=5.0, learning_rate=0.1, num_iterations=20)\n",
       "print(f\"Optimal x: {optimal_x:.6f}\")\n",
       "print(f\"Optimal value: {objective_function(optimal_x):.6f}\")\n",
       "\n",
       "# Visualize\n",
       "x_vals = np.linspace(-2, 6, 1000)\n",
       "y_vals = objective_function(x_vals)\n",
       "\n",
       "plt.plot(x_vals, y_vals, 'b-', linewidth=3, label='f(x)')\n",
       "x_history = [h[0] for h in history]\n",
       "y_history = [h[1] for h in history]\n",
       "plt.plot(x_history, y_history, 'ro-', linewidth=2, markersize=8, label='Gradient descent')\n",
       "plt.xlabel('x')\n",
       "plt.ylabel('f(x)')\n",
       "plt.title('Gradient Descent Optimization')\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "plt.show()"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }