{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multivariable Calculus\n\n[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)\n[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-orange.svg)](https://matplotlib.org/)\n[![SymPy](https://img.shields.io/badge/SymPy-1.10+-purple.svg)](https://www.sympy.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-1.7+-red.svg)](https://scipy.org/)\n\n## Introduction\n\nMultivariable calculus generalizes the concepts of single-variable calculus to functions of several variables. This is essential for understanding high-dimensional spaces, which are ubiquitous in AI/ML and data science. Many models, such as neural networks, operate in spaces with thousands or millions of dimensions, making multivariable calculus foundational for:\n- Optimization of loss functions with many parameters\n- Sensitivity analysis and feature importance\n- Modeling complex systems with multiple inputs\n\n## 6.1 Functions of Multiple Variables\n\n### Mathematical Foundations and Visualization\n\nA function of two variables, \\( f(x, y) \\), assigns a real number to each point \\( (x, y) \\) in its domain. The graph of such a function is a surface in three-dimensional space. Key concepts include:\n- **Level curves (contours):** Curves where \\( f(x, y) = c \\) for constant \\( c \\). These help visualize the function's behavior in the plane.\n- **Surfaces:** The set of points \\( (x, y, f(x, y)) \\) forms a surface, which can be visualized in 3D.\n\n**Relevance to AI/ML:**\n- Loss landscapes in neural networks are high-dimensional surfaces.\n- Contour plots help visualize optimization paths and convergence.\n- Understanding the geometry of multivariable functions aids in interpreting model behavior and feature interactions.\n\n### Python Implementation: Multivariable Functions\n\nThe following code demonstrates how to define and visualize several multivariable functions, with commentary on their geometric and practical significance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport sympy as sp\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.optimize import minimize\n\ndef multivariable_functions():\n    \"\"\"\n    Explore functions of multiple variables.\n    Examples:\n    1. Paraboloid: f(x, y) = x^2 + y^2 (convex surface, unique minimum)\n    2. Oscillatory: f(x, y) = sin(x) * cos(y) (multiple local extrema)\n    3. Saddle: f(x, y) = x*y (saddle point at origin)\n    \"\"\"\n    \n    # Example 1: f(x,y) = x^2 + y^2 (paraboloid)\n    x, y = sp.symbols('x y')\n    f1 = x**2 + y**2\n    \n    # Example 2: f(x,y) = sin(x) * cos(y)\n    f2 = sp.sin(x) * sp.cos(y)\n    \n    # Example 3: f(x,y) = x*y (saddle surface)\n    f3 = x * y\n    \n    print(\"Multivariable Functions:\")\n    print(f\"f1(x,y) = {f1}\")\n    print(f\"f2(x,y) = {f2}\")\n    print(f\"f3(x,y) = {f3}\")\n    \n    return f1, f2, f3\n\nf1, f2, f3 = multivariable_functions()\n\n# Visualize multivariable functions\ndef visualize_multivariable_functions():\n    x = np.linspace(-3, 3, 100)\n    y = np.linspace(-3, 3, 100)\n    X, Y = np.meshgrid(x, y)\n    \n    # Function 1: f(x,y) = x^2 + y^2\n    Z1 = X**2 + Y**2\n    \n    # Function 2: f(x,y) = sin(x) * cos(y)\n    Z2 = np.sin(X) * np.cos(Y)\n    \n    # Function 3: f(x,y) = x*y\n    Z3 = X * Y\n    \n    fig = plt.figure(figsize=(15, 5))\n    \n    # 3D surface plots\n    ax1 = fig.add_subplot(131, projection='3d')\n    surf1 = ax1.plot_surface(X, Y, Z1, cmap='viridis', alpha=0.8)\n    ax1.set_title('f(x,y) = x^2 + y^2')\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_zlabel('f(x,y)')\n    \n    ax2 = fig.add_subplot(132, projection='3d')\n    surf2 = ax2.plot_surface(X, Y, Z2, cmap='plasma', alpha=0.8)\n    ax2.set_title('f(x,y) = sin(x) * cos(y)')\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.set_zlabel('f(x,y)')\n    \n    ax3 = fig.add_subplot(133, projection='3d')\n    surf3 = ax3.plot_surface(X, Y, Z3, cmap='coolwarm', alpha=0.8)\n    ax3.set_title('f(x,y) = x*y')\n    ax3.set_xlabel('x')\n    ax3.set_ylabel('y')\n    ax3.set_zlabel('f(x,y)')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Contour plots\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n    \n    contour1 = ax1.contour(X, Y, Z1, levels=10)\n    ax1.clabel(contour1, inline=True, fontsize=8)\n    ax1.set_title('Contours: f(x,y) = x^2 + y^2')\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.grid(True)\n    \n    contour2 = ax2.contour(X, Y, Z2, levels=10)\n    ax2.clabel(contour2, inline=True, fontsize=8)\n    ax2.set_title('Contours: f(x,y) = sin(x) * cos(y)')\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.grid(True)\n    \n    contour3 = ax3.contour(X, Y, Z3, levels=10)\n    ax3.clabel(contour3, inline=True, fontsize=8)\n    ax3.set_title('Contours: f(x,y) = x*y')\n    ax3.set_xlabel('x')\n    ax3.set_ylabel('y')\n    ax3.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_multivariable_functions()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:**\n- The code defines and visualizes three types of surfaces: convex (paraboloid), oscillatory, and saddle.\n- 3D plots show the geometry of each function, while contour plots reveal level curves and critical points.\n- These visualizations are directly relevant to understanding optimization landscapes and feature interactions in AI/ML.\n\n## 6.2 Partial Derivatives\n\n### Mathematical Foundations and Computation\n\nA partial derivative measures how a multivariable function changes as one variable varies, holding the others constant. For \\( f(x, y) \\):\n\\[\n\\frac{\\partial f}{\\partial x} = \\lim_{h \\to 0} \\frac{f(x + h, y) - f(x, y)}{h}\n\\]\n\\[\n\\frac{\\partial f}{\\partial y} = \\lim_{h \\to 0} \\frac{f(x, y + h) - f(x, y)}{h}\n\\]\nPartial derivatives are the building blocks for gradients, Jacobians, and optimization in high-dimensional spaces.\n\n**Relevance to AI/ML:**\n- Gradients (vectors of partial derivatives) are used in gradient descent and backpropagation.\n- Sensitivity analysis: Partial derivatives quantify how sensitive a model's output is to each input feature.\n\n### Python Implementation: Partial Derivatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def partial_derivatives():\n    \"\"\"\n    Compute partial derivatives of multivariable functions.\n    Steps:\n    1. Define the function symbolically.\n    2. Compute partial derivatives with respect to each variable.\n    3. Interpret the results.\n    \"\"\"\n    x, y = sp.symbols('x y')\n    \n    # Example 1: f(x,y) = x^2 + y^2\n    f1 = x**2 + y**2\n    df1_dx = sp.diff(f1, x)\n    df1_dy = sp.diff(f1, y)\n    \n    print(\"Partial Derivatives:\")\n    print(f\"f(x,y) = {f1}\")\n    print(f\"∂f/∂x = {df1_dx}\")\n    print(f\"∂f/∂y = {df1_dy}\")\n    \n    # Example 2: f(x,y) = x*y + sin(x)\n    f2 = x*y + sp.sin(x)\n    df2_dx = sp.diff(f2, x)\n    df2_dy = sp.diff(f2, y)\n    \n    print(f\"\\nf(x,y) = {f2}\")\n    print(f\"∂f/∂x = {df2_dx}\")\n    print(f\"∂f/∂y = {df2_dy}\")\n    \n    # Example 3: f(x,y) = e^(x*y)\n    f3 = sp.exp(x*y)\n    df3_dx = sp.diff(f3, x)\n    df3_dy = sp.diff(f3, y)\n    \n    print(f\"\\nf(x,y) = {f3}\")\n    print(f\"∂f/∂x = {df3_dx}\")\n    print(f\"∂f/∂y = {df3_dy}\")\n    \n    return f1, f2, f3, df1_dx, df1_dy, df2_dx, df2_dy, df3_dx, df3_dy\n\nf1, f2, f3, df1_dx, df1_dy, df2_dx, df2_dy, df3_dx, df3_dy = partial_derivatives()\n\n# Visualize partial derivatives\ndef visualize_partial_derivatives():\n    x = np.linspace(-2, 2, 50)\n    y = np.linspace(-2, 2, 50)\n    X, Y = np.meshgrid(x, y)\n    \n    # Function and its partial derivatives\n    Z = X**2 + Y**2\n    dZ_dx = 2 * X\n    dZ_dy = 2 * Y\n    \n    fig = plt.figure(figsize=(15, 5))\n    \n    # Original function\n    ax1 = fig.add_subplot(131, projection='3d')\n    surf1 = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n    ax1.set_title('f(x,y) = x^2 + y^2')\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_zlabel('f(x,y)')\n    \n    # ∂f/∂x\n    ax2 = fig.add_subplot(132, projection='3d')\n    surf2 = ax2.plot_surface(X, Y, dZ_dx, cmap='plasma', alpha=0.8)\n    ax2.set_title('∂f/∂x = 2x')\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.set_zlabel('∂f/∂x')\n    \n    # ∂f/∂y\n    ax3 = fig.add_subplot(133, projection='3d')\n    surf3 = ax3.plot_surface(X, Y, dZ_dy, cmap='coolwarm', alpha=0.8)\n    ax3.set_title('∂f/∂y = 2y')\n    ax3.set_xlabel('x')\n    ax3.set_ylabel('y')\n    ax3.set_zlabel('∂f/∂y')\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_partial_derivatives()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:**\n- The code computes partial derivatives for several functions, illustrating how each variable affects the output.\n- 3D plots show the original function and the effect of changing each variable independently.\n- These concepts are foundational for gradient-based optimization and feature sensitivity in AI/ML.\n\n## 6.3 Gradient and Directional Derivatives\n\n### Gradient Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_calculations():\n    \"\"\"Calculate gradients of multivariable functions\"\"\"\n    \n    x, y = sp.symbols('x y')\n    \n    # Example: f(x,y) = x² + y²\n    f = x**2 + y**2\n    grad_f = [sp.diff(f, x), sp.diff(f, y)]\n    \n    print(\"Gradient:\")\n    print(f\"f(x,y) = {f}\")\n    print(f\"∇f = [{grad_f[0]}, {grad_f[1]}]\")\n    \n    # Evaluate gradient at specific points\n    points = [(0, 0), (1, 1), (-1, 0), (2, -1)]\n    print(f\"\\nGradient at specific points:\")\n    for point in points:\n        grad_at_point = [grad_f[0].subs([(x, point[0]), (y, point[1])]),\n                        grad_f[1].subs([(x, point[0]), (y, point[1])])]\n        print(f\"∇f({point}) = {grad_at_point}\")\n    \n    return f, grad_f\n\nf, grad_f = gradient_calculations()\n\n# Visualize gradient field\ndef visualize_gradient_field():\n    x = np.linspace(-2, 2, 10)\n    y = np.linspace(-2, 2, 10)\n    X, Y = np.meshgrid(x, y)\n    \n    # Gradient components\n    dF_dx = 2 * X\n    dF_dy = 2 * Y\n    \n    plt.figure(figsize=(10, 8))\n    \n    # Gradient field\n    plt.quiver(X, Y, dF_dx, dF_dy, angles='xy', scale_units='xy', scale=1, alpha=0.7)\n    \n    # Contour plot\n    contour = plt.contour(X, Y, X**2 + Y**2, levels=10, alpha=0.5)\n    plt.clabel(contour, inline=True, fontsize=8)\n    \n    plt.title('Gradient Field of f(x,y) = x² + y²')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.show()\n\nvisualize_gradient_field()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Directional Derivatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def directional_derivative():\n    \"\"\"Calculate directional derivatives\"\"\"\n    \n    x, y = sp.symbols('x y')\n    \n    # Function: f(x,y) = x² + y²\n    f = x**2 + y**2\n    \n    # Direction vector: u = [cos(θ), sin(θ)]\n    theta = sp.Symbol('theta')\n    u_x = sp.cos(theta)\n    u_y = sp.sin(theta)\n    \n    # Gradient\n    grad_f = [sp.diff(f, x), sp.diff(f, y)]\n    \n    # Directional derivative: D_u f = ∇f · u\n    directional_deriv = grad_f[0] * u_x + grad_f[1] * u_y\n    directional_deriv = sp.simplify(directional_deriv)\n    \n    print(\"Directional Derivative:\")\n    print(f\"f(x,y) = {f}\")\n    print(f\"Direction vector: u = [cos(θ), sin(θ)]\")\n    print(f\"D_u f = ∇f · u = {directional_deriv}\")\n    \n    # Evaluate at specific point and direction\n    point = (1, 1)\n    direction_angle = np.pi/4  # 45 degrees\n    \n    grad_at_point = [grad_f[0].subs([(x, point[0]), (y, point[1])]),\n                    grad_f[1].subs([(x, point[0]), (y, point[1])])]\n    \n    u_at_angle = [np.cos(direction_angle), np.sin(direction_angle)]\n    \n    directional_deriv_at_point = grad_at_point[0] * u_at_angle[0] + grad_at_point[1] * u_at_angle[1]\n    \n    print(f\"\\nAt point {point} in direction θ = {direction_angle:.2f}:\")\n    print(f\"∇f({point}) = {grad_at_point}\")\n    print(f\"u = {u_at_angle}\")\n    print(f\"D_u f({point}) = {directional_deriv_at_point:.4f}\")\n    \n    return f, directional_deriv, point, direction_angle, directional_deriv_at_point\n\nf, directional_deriv, point, direction_angle, directional_deriv_at_point = directional_derivative()\n\n# Visualize directional derivative\ndef visualize_directional_derivative():\n    x = np.linspace(-2, 2, 50)\n    y = np.linspace(-2, 2, 50)\n    X, Y = np.meshgrid(x, y)\n    Z = X**2 + Y**2\n    \n    plt.figure(figsize=(10, 8))\n    \n    # Contour plot\n    contour = plt.contour(X, Y, Z, levels=10)\n    plt.clabel(contour, inline=True, fontsize=8)\n    \n    # Point and direction\n    plt.scatter(point[0], point[1], c='red', s=100, zorder=5, label=f'Point {point}')\n    \n    # Direction vector\n    u_x = np.cos(direction_angle)\n    u_y = np.sin(direction_angle)\n    plt.arrow(point[0], point[1], u_x, u_y, head_width=0.1, head_length=0.1, \n              fc='red', ec='red', label=f'Direction θ = {direction_angle:.2f}')\n    \n    # Gradient at point\n    grad_x = 2 * point[0]\n    grad_y = 2 * point[1]\n    plt.arrow(point[0], point[1], grad_x, grad_y, head_width=0.1, head_length=0.1,\n              fc='blue', ec='blue', label=f'Gradient ∇f({point})')\n    \n    plt.title('Directional Derivative')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nvisualize_directional_derivative()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.4 Optimization in Multiple Dimensions\n\n### Critical Points and Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multivariable_optimization():\n    \"\"\"Find and classify critical points of multivariable functions\"\"\"\n    \n    x, y = sp.symbols('x y')\n    \n    # Example: f(x,y) = x³ + y³ - 3xy\n    f = x**3 + y**3 - 3*x*y\n    \n    # Partial derivatives\n    df_dx = sp.diff(f, x)\n    df_dy = sp.diff(f, y)\n    \n    # Second partial derivatives\n    d2f_dx2 = sp.diff(df_dx, x)\n    d2f_dy2 = sp.diff(df_dy, y)\n    d2f_dxdy = sp.diff(df_dx, y)\n    \n    print(\"Multivariable Optimization:\")\n    print(f\"f(x,y) = {f}\")\n    print(f\"∂f/∂x = {df_dx}\")\n    print(f\"∂f/∂y = {df_dy}\")\n    print(f\"∂²f/∂x² = {d2f_dx2}\")\n    print(f\"∂²f/∂y² = {d2f_dy2}\")\n    print(f\"∂²f/∂x∂y = {d2f_dxdy}\")\n    \n    # Find critical points\n    critical_points = sp.solve([df_dx, df_dy], [x, y])\n    print(f\"\\nCritical points: {critical_points}\")\n    \n    # Classify critical points using second derivative test\n    # D = f_xx * f_yy - (f_xy)²\n    for point in critical_points:\n        x_val, y_val = point\n        \n        # Evaluate second derivatives at critical point\n        f_xx = d2f_dx2.subs([(x, x_val), (y, y_val)])\n        f_yy = d2f_dy2.subs([(x, x_val), (y, y_val)])\n        f_xy = d2f_dxdy.subs([(x, x_val), (y, y_val)])\n        \n        D = f_xx * f_yy - f_xy**2\n        \n        print(f\"\\nAt point ({x_val}, {y_val}):\")\n        print(f\"f_xx = {f_xx}, f_yy = {f_yy}, f_xy = {f_xy}\")\n        print(f\"D = {D}\")\n        \n        if D > 0:\n            if f_xx > 0:\n                print(\"Local minimum\")\n            else:\n                print(\"Local maximum\")\n        elif D < 0:\n            print(\"Saddle point\")\n        else:\n            print(\"Inconclusive (need higher order derivatives)\")\n    \n    return f, critical_points\n\nf, critical_points = multivariable_optimization()\n\n# Visualize optimization\ndef visualize_multivariable_optimization():\n    x = np.linspace(-2, 2, 100)\n    y = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = X**3 + Y**3 - 3*X*Y\n    \n    fig = plt.figure(figsize=(15, 5))\n    \n    # 3D surface\n    ax1 = fig.add_subplot(131, projection='3d')\n    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n    \n    # Mark critical points\n    for point in critical_points:\n        x_val, y_val = point\n        z_val = x_val**3 + y_val**3 - 3*x_val*y_val\n        ax1.scatter(x_val, y_val, z_val, c='red', s=100, zorder=5)\n    \n    ax1.set_title('f(x,y) = x³ + y³ - 3xy')\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_zlabel('f(x,y)')\n    \n    # Contour plot\n    ax2 = fig.add_subplot(132)\n    contour = ax2.contour(X, Y, Z, levels=20)\n    ax2.clabel(contour, inline=True, fontsize=8)\n    \n    # Mark critical points\n    for point in critical_points:\n        x_val, y_val = point\n        ax2.scatter(x_val, y_val, c='red', s=100, zorder=5)\n    \n    ax2.set_title('Contour Plot with Critical Points')\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.grid(True)\n    \n    # Gradient field\n    ax3 = fig.add_subplot(133)\n    dF_dx = 3*X**2 - 3*Y\n    dF_dy = 3*Y**2 - 3*X\n    \n    ax3.quiver(X[::5, ::5], Y[::5, ::5], dF_dx[::5, ::5], dF_dy[::5, ::5], \n               angles='xy', scale_units='xy', scale=1, alpha=0.7)\n    \n    # Mark critical points\n    for point in critical_points:\n        x_val, y_val = point\n        ax3.scatter(x_val, y_val, c='red', s=100, zorder=5)\n    \n    ax3.set_title('Gradient Field')\n    ax3.set_xlabel('x')\n    ax3.set_ylabel('y')\n    ax3.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_multivariable_optimization()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.5 Lagrange Multipliers\n\n### Constrained Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lagrange_multipliers():\n    \"\"\"Solve constrained optimization problems using Lagrange multipliers\"\"\"\n    \n    x, y, lambda_var = sp.symbols('x y lambda')\n    \n    # Example: Maximize f(x,y) = xy subject to x + y = 10\n    f = x * y\n    g = x + y - 10  # constraint: x + y = 10\n    \n    # Lagrange function: L = f - λg\n    L = f - lambda_var * g\n    \n    # Partial derivatives\n    dL_dx = sp.diff(L, x)\n    dL_dy = sp.diff(L, y)\n    dL_dlambda = sp.diff(L, lambda_var)\n    \n    print(\"Lagrange Multipliers:\")\n    print(f\"Objective: maximize f(x,y) = {f}\")\n    print(f\"Constraint: {g} = 0\")\n    print(f\"Lagrange function: L = {L}\")\n    print(f\"∂L/∂x = {dL_dx}\")\n    print(f\"∂L/∂y = {dL_dy}\")\n    print(f\"∂L/∂λ = {dL_dlambda}\")\n    \n    # Solve system of equations\n    solution = sp.solve([dL_dx, dL_dy, dL_dlambda], [x, y, lambda_var])\n    print(f\"\\nSolution: {solution}\")\n    \n    # Verify solution\n    if solution:\n        x_opt, y_opt, lambda_opt = solution[0]\n        print(f\"Optimal x = {x_opt}\")\n        print(f\"Optimal y = {y_opt}\")\n        print(f\"Optimal value = {f.subs([(x, x_opt), (y, y_opt)])}\")\n        print(f\"Constraint satisfied: {g.subs([(x, x_opt), (y, y_opt)])}\")\n    \n    return f, g, solution\n\nf, g, solution = lagrange_multipliers()\n\n# Visualize constrained optimization\ndef visualize_constrained_optimization():\n    x = np.linspace(0, 10, 100)\n    y = np.linspace(0, 10, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = X * Y\n    \n    # Constraint line\n    constraint_x = np.linspace(0, 10, 100)\n    constraint_y = 10 - constraint_x\n    \n    plt.figure(figsize=(10, 8))\n    \n    # Contour plot of objective function\n    contour = plt.contour(X, Y, Z, levels=20)\n    plt.clabel(contour, inline=True, fontsize=8)\n    \n    # Constraint line\n    plt.plot(constraint_x, constraint_y, 'r-', linewidth=3, label='Constraint: x + y = 10')\n    \n    # Optimal point\n    if solution:\n        x_opt, y_opt, _ = solution[0]\n        plt.scatter(x_opt, y_opt, c='red', s=200, zorder=5, \n                   label=f'Optimal point: ({x_opt}, {y_opt})')\n    \n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Constrained Optimization: Maximize xy subject to x + y = 10')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nvisualize_constrained_optimization()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.6 Applications in Machine Learning\n\n### Gradient Descent in Multiple Dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent_2d():\n    \"\"\"Implement gradient descent for multivariable functions\"\"\"\n    \n    # Objective function: f(x,y) = x² + y²\n    def objective_function(x, y):\n        return x**2 + y**2\n    \n    def gradient_function(x, y):\n        return np.array([2*x, 2*y])\n    \n    # Gradient descent implementation\n    def gradient_descent_2d(f, grad_f, start_point, learning_rate=0.1, max_iter=100):\n        x, y = start_point\n        history = [(x, y)]\n        \n        for i in range(max_iter):\n            gradient = grad_f(x, y)\n            x = x - learning_rate * gradient[0]\n            y = y - learning_rate * gradient[1]\n            history.append((x, y))\n            \n            # Check convergence\n            if np.linalg.norm(gradient) < 1e-6:\n                break\n        \n        return np.array(history)\n    \n    # Run gradient descent\n    start_point = (2.0, 2.0)\n    path = gradient_descent_2d(objective_function, gradient_function, start_point)\n    \n    print(\"2D Gradient Descent:\")\n    print(f\"Starting point: {start_point}\")\n    print(f\"Final point: {path[-1]}\")\n    print(f\"Final value: {objective_function(path[-1, 0], path[-1, 1]):.6f}\")\n    print(f\"Number of iterations: {len(path)}\")\n    \n    return path, objective_function\n\npath, objective_function = gradient_descent_2d()\n\n# Visualize gradient descent path\ndef visualize_gradient_descent_2d():\n    x = np.linspace(-3, 3, 100)\n    y = np.linspace(-3, 3, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = X**2 + Y**2\n    \n    plt.figure(figsize=(10, 8))\n    \n    # Contour plot\n    contour = plt.contour(X, Y, Z, levels=20)\n    plt.clabel(contour, inline=True, fontsize=8)\n    \n    # Optimization path\n    plt.plot(path[:, 0], path[:, 1], 'r-', linewidth=2, label='Gradient descent path')\n    plt.scatter(path[0, 0], path[0, 1], c='red', s=100, label='Start')\n    plt.scatter(path[-1, 0], path[-1, 1], c='green', s=100, label='End')\n    \n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('2D Gradient Descent on f(x,y) = x² + y²')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nvisualize_gradient_descent_2d()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\n- **Multivariable Functions**: Functions of multiple variables and their visualization\n- **Partial Derivatives**: Derivatives with respect to individual variables\n- **Gradient**: Vector of partial derivatives indicating direction of steepest ascent\n- **Directional Derivatives**: Rate of change in specific directions\n- **Optimization**: Finding critical points and classifying them using second derivative test\n- **Lagrange Multipliers**: Constrained optimization technique\n- **Applications**: Gradient descent in machine learning and optimization algorithms\n\n## Next Steps\n\nUnderstanding multivariable calculus enables you to work with high-dimensional optimization problems, understand gradient-based learning algorithms, and model complex systems. The next section explores vector calculus for understanding vector fields and their properties."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}