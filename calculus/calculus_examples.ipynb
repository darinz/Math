{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Calculus Examples for AI/ML and Data Science\n",
    "\n",
    "This notebook provides practical examples of calculus concepts applied to machine learning and data science problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "from scipy import optimize, integrate\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Enable LaTeX rendering\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Derivatives and Gradient Descent\n",
    "\n",
    "Let's start with a simple example of gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 1) * 2\n",
    "y = 3 * X + 2 + np.random.randn(100, 1) * 0.5\n",
    "\n",
    "# Define the loss function (MSE)\n",
    "def mse_loss(w, b, X, y):\n",
    "    predictions = X * w + b\n",
    "    return np.mean((predictions - y)**2)\n",
    "\n",
    "# Define gradients\n",
    "def mse_gradients(w, b, X, y):\n",
    "    predictions = X * w + b\n",
    "    dw = 2 * np.mean(X * (predictions - y))\n",
    "    db = 2 * np.mean(predictions - y)\n",
    "    return dw, db\n",
    "\n",
    "# Gradient descent\n",
    "def gradient_descent(X, y, learning_rate=0.01, epochs=1000):\n",
    "    w, b = 0.0, 0.0\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        dw, db = mse_gradients(w, b, X, y)\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            loss = mse_loss(w, b, X, y)\n",
    "            history.append((epoch, w, b, loss))\n",
    "    \n",
    "    return w, b, history\n",
    "\n",
    "# Run gradient descent\n",
    "w_opt, b_opt, history = gradient_descent(X, y)\n",
    "\n",
    "print(f\"Optimal parameters: w = {w_opt:.3f}, b = {b_opt:.3f}\")\n",
    "print(f\"Final loss: {mse_loss(w_opt, b_opt, X, y):.6f}\")\n",
    "\n",
    "# Visualize the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Data and fitted line\n",
    "ax1.scatter(X, y, alpha=0.6, label='Data')\n",
    "X_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "y_line = w_opt * X_line + b_opt\n",
    "ax1.plot(X_line, y_line, 'r-', linewidth=2, label=f'Fitted: y = {w_opt:.3f}x + {b_opt:.3f}')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Linear Regression with Gradient Descent')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Loss convergence\n",
    "epochs, ws, bs, losses = zip(*history)\n",
    "ax2.plot(epochs, losses, 'b-', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MSE Loss')\n",
    "ax2.set_title('Loss Convergence')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Integration and Probability\n",
    "\n",
    "Demonstrate integration for probability calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal distribution integration\n",
    "def normal_pdf(x, mu=0, sigma=1):\n",
    "    return (1/(sigma * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x - mu)/sigma)**2)\n",
    "\n",
    "# Calculate probabilities using integration\n",
    "prob_1sigma, _ = integrate.quad(normal_pdf, -1, 1)\n",
    "prob_2sigma, _ = integrate.quad(normal_pdf, -2, 2)\n",
    "prob_3sigma, _ = integrate.quad(normal_pdf, -3, 3)\n",
    "\n",
    "print(f\"P(-1 ≤ X ≤ 1): {prob_1sigma:.4f} (68.27%)\")\n",
    "print(f\"P(-2 ≤ X ≤ 2): {prob_2sigma:.4f} (95.45%)\")\n",
    "print(f\"P(-3 ≤ X ≤ 3): {prob_3sigma:.4f} (99.73%)\")\n",
    "\n",
    "# Visualize\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "y = normal_pdf(x)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Full distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x, y, 'b-', linewidth=2)\n",
    "plt.fill_between(x, y, alpha=0.3, color='blue')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Standard Normal Distribution')\n",
    "plt.grid(True)\n",
    "\n",
    "# ±1σ region\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x, y, 'b-', linewidth=2)\n",
    "mask = (x >= -1) & (x <= 1)\n",
    "plt.fill_between(x[mask], y[mask], alpha=0.5, color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(f'P(-1 ≤ X ≤ 1) = {prob_1sigma:.4f}')\n",
    "plt.grid(True)\n",
    "\n",
    "# ±2σ region\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(x, y, 'b-', linewidth=2)\n",
    "mask = (x >= -2) & (x <= 2)\n",
    "plt.fill_between(x[mask], y[mask], alpha=0.5, color='green')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(f'P(-2 ≤ X ≤ 2) = {prob_2sigma:.4f}')\n",
    "plt.grid(True)\n",
    "\n",
    "# ±3σ region\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(x, y, 'b-', linewidth=2)\n",
    "mask = (x >= -3) & (x <= 3)\n",
    "plt.fill_between(x[mask], y[mask], alpha=0.5, color='orange')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(f'P(-3 ≤ X ≤ 3) = {prob_3sigma:.4f}')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization with Constraints\n",
    "\n",
    "Solve a constrained optimization problem using Lagrange multipliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained optimization: Maximize f(x,y) = xy subject to x + y = 10\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective_function(params):\n",
    "    x, y = params\n",
    "    return -x * y  # Negative because we want to maximize\n",
    "\n",
    "def constraint_function(params):\n",
    "    x, y = params\n",
    "    return x + y - 10\n",
    "\n",
    "# Solve using scipy\n",
    "constraints = {'type': 'eq', 'fun': constraint_function}\n",
    "bounds = [(0, 10), (0, 10)]\n",
    "\n",
    "result = minimize(objective_function, [5, 5], constraints=constraints, bounds=bounds)\n",
    "\n",
    "print(f\"Optimal x: {result.x[0]:.6f}\")\n",
    "print(f\"Optimal y: {result.x[1]:.6f}\")\n",
    "print(f\"Optimal value: {result.x[0] * result.x[1]:.6f}\")\n",
    "print(f\"Constraint satisfied: {constraint_function(result.x):.2e}\")\n",
    "\n",
    "# Visualize\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.linspace(0, 10, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X * Y\n",
    "\n",
    "# Constraint line\n",
    "constraint_x = np.linspace(0, 10, 100)\n",
    "constraint_y = 10 - constraint_x\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Contour plot\n",
    "contour = plt.contour(X, Y, Z, levels=20)\n",
    "plt.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Constraint\n",
    "plt.plot(constraint_x, constraint_y, 'r-', linewidth=3, label='Constraint: x + y = 10')\n",
    "\n",
    "# Optimal point\n",
    "plt.scatter(result.x[0], result.x[1], c='red', s=200, marker='*', \n",
    "            label=f'Optimal: ({result.x[0]:.2f}, {result.x[1]:.2f})')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Constrained Optimization: Maximize xy subject to x + y = 10')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network with Backpropagation\n",
    "\n",
    "Implement a simple neural network with manual backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, learning_rate=0.1):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Backward pass (backpropagation)\n",
    "        dz2 = self.a2 - y\n",
    "        dW2 = (1/m) * self.a1.T @ dz2\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        dz1 = dz2 @ self.W2.T * self.sigmoid_derivative(self.a1)\n",
    "        dW1 = (1/m) * X.T @ dz1\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.1):\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = np.mean((y_pred - y)**2)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y, learning_rate)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "\n",
    "# XOR problem\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Create and train network\n",
    "nn = SimpleNeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "losses = nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
    "\n",
    "# Test predictions\n",
    "predictions = nn.forward(X)\n",
    "print(\"\\nPredictions:\")\n",
    "for i, (x, pred, true) in enumerate(zip(X, predictions, y)):\n",
    "    print(f\"Input: {x}, Prediction: {pred[0]:.3f}, True: {true[0]}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Neural Network Training Loss')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Optimization: Adam Optimizer\n",
    "\n",
    "Implement the Adam optimizer and compare with standard gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None  # First moment\n",
    "        self.v = None  # Second moment\n",
    "        self.t = 0     # Time step\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(params)\n",
    "            self.v = np.zeros_like(params)\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        # Update biased first moment estimate\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients\n",
    "        \n",
    "        # Update biased second moment estimate\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (gradients**2)\n",
    "        \n",
    "        # Compute bias-corrected first moment estimate\n",
    "        m_hat = self.m / (1 - self.beta1**self.t)\n",
    "        \n",
    "        # Compute bias-corrected second moment estimate\n",
    "        v_hat = self.v / (1 - self.beta2**self.t)\n",
    "        \n",
    "        # Update parameters\n",
    "        params -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        \n",
    "        return params\n",
    "\n",
    "# Test function: Rosenbrock function\n",
    "def rosenbrock(x):\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "def rosenbrock_gradient(x):\n",
    "    dx = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)\n",
    "    dy = 200 * (x[1] - x[0]**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Compare optimization methods\n",
    "def compare_optimizers():\n",
    "    # Initial point\n",
    "    x0 = np.array([-1.0, -1.0])\n",
    "    \n",
    "    # Standard gradient descent\n",
    "    x_gd = x0.copy()\n",
    "    gd_history = [x_gd.copy()]\n",
    "    \n",
    "    for i in range(1000):\n",
    "        gradient = rosenbrock_gradient(x_gd)\n",
    "        x_gd -= 0.001 * gradient\n",
    "        gd_history.append(x_gd.copy())\n",
    "    \n",
    "    # Adam optimizer\n",
    "    x_adam = x0.copy()\n",
    "    adam = AdamOptimizer(learning_rate=0.01)\n",
    "    adam_history = [x_adam.copy()]\n",
    "    \n",
    "    for i in range(1000):\n",
    "        gradient = rosenbrock_gradient(x_adam)\n",
    "        x_adam = adam.update(x_adam, gradient)\n",
    "        adam_history.append(x_adam.copy())\n",
    "    \n",
    "    return np.array(gd_history), np.array(adam_history)\n",
    "\n",
    "gd_path, adam_path = compare_optimizers()\n",
    "\n",
    "print(f\"Gradient Descent final point: {gd_path[-1]}\")\n",
    "print(f\"Gradient Descent final value: {rosenbrock(gd_path[-1]):.6f}\")\n",
    "print(f\"Adam final point: {adam_path[-1]}\")\n",
    "print(f\"Adam final value: {rosenbrock(adam_path[-1]):.6f}\")\n",
    "\n",
    "# Visualize optimization paths\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = rosenbrock([X, Y])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "contour = plt.contour(X, Y, Z, levels=20)\n",
    "plt.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "plt.plot(gd_path[:, 0], gd_path[:, 1], 'r-', linewidth=2, label='Gradient Descent')\n",
    "plt.plot(adam_path[:, 0], adam_path[:, 1], 'g-', linewidth=2, label='Adam')\n",
    "\n",
    "plt.scatter(gd_path[0, 0], gd_path[0, 1], c='red', s=100, label='Start')\n",
    "plt.scatter(gd_path[-1, 0], gd_path[-1, 1], c='red', s=100, marker='*', label='GD End')\n",
    "plt.scatter(adam_path[-1, 0], adam_path[-1, 1], c='green', s=100, marker='*', label='Adam End')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Optimization Paths on Rosenbrock Function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration for Expected Values\n",
    "\n",
    "Calculate expected values and variances using integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate expected values for different distributions\n",
    "\n",
    "# Exponential distribution\n",
    "def exponential_pdf(x, lambda_param=1):\n",
    "    return lambda_param * np.exp(-lambda_param * x)\n",
    "\n",
    "def expected_value_exponential(lambda_param=1):\n",
    "    integrand = lambda x: x * exponential_pdf(x, lambda_param)\n",
    "    result, _ = integrate.quad(integrand, 0, np.inf)\n",
    "    return result\n",
    "\n",
    "def variance_exponential(lambda_param=1):\n",
    "    # E[X²] - (E[X])²\n",
    "    integrand = lambda x: x**2 * exponential_pdf(x, lambda_param)\n",
    "    e_x_squared, _ = integrate.quad(integrand, 0, np.inf)\n",
    "    e_x = expected_value_exponential(lambda_param)\n",
    "    return e_x_squared - e_x**2\n",
    "\n",
    "# Calculate for different lambda values\n",
    "lambda_values = [0.5, 1.0, 2.0]\n",
    "\n",
    "print(\"Exponential Distribution Properties:\")\n",
    "print(\"-\" * 50)\n",
    "for lambda_val in lambda_values:\n",
    "    e_x = expected_value_exponential(lambda_val)\n",
    "    var_x = variance_exponential(lambda_val)\n",
    "    theoretical_e_x = 1 / lambda_val\n",
    "    theoretical_var = 1 / lambda_val**2\n",
    "    \n",
    "    print(f\"λ = {lambda_val}:\")\n",
    "    print(f\"  E[X] = {e_x:.6f} (theoretical: {theoretical_e_x:.6f})\")\n",
    "    print(f\"  Var(X) = {var_x:.6f} (theoretical: {theoretical_var:.6f})\")\n",
    "    print()\n",
    "\n",
    "# Visualize exponential distributions\n",
    "x = np.linspace(0, 10, 1000)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for lambda_val in lambda_values:\n",
    "    y = exponential_pdf(x, lambda_val)\n",
    "    plt.plot(x, y, linewidth=2, label=f'λ = {lambda_val}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Exponential Distribution PDF')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "e_x_values = [expected_value_exponential(lambda_val) for lambda_val in lambda_values]\n",
    "var_values = [variance_exponential(lambda_val) for lambda_val in lambda_values]\n",
    "\n",
    "plt.bar([f'λ={λ}' for λ in lambda_values], e_x_values, alpha=0.7, label='E[X]')\n",
    "plt.bar([f'λ={λ}' for λ in lambda_values], var_values, alpha=0.7, label='Var(X)', bottom=e_x_values)\n",
    "plt.ylabel('Value')\n",
    "plt.title('Expected Values and Variances')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculus in Machine Learning: Loss Functions\n",
    "\n",
    "Compare different loss functions and their derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different loss functions\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "def mae_loss(y_pred, y_true):\n",
    "    return np.mean(np.abs(y_pred - y_true))\n",
    "\n",
    "def huber_loss(y_pred, y_true, delta=1.0):\n",
    "    error = y_pred - y_true\n",
    "    abs_error = np.abs(error)\n",
    "    quadratic = np.minimum(abs_error, delta)\n",
    "    linear = abs_error - quadratic\n",
    "    return np.mean(0.5 * quadratic**2 + delta * linear)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Derivatives\n",
    "def mse_derivative(y_pred, y_true):\n",
    "    return 2 * (y_pred - y_true) / len(y_pred)\n",
    "\n",
    "def mae_derivative(y_pred, y_true):\n",
    "    return np.sign(y_pred - y_true) / len(y_pred)\n",
    "\n",
    "def huber_derivative(y_pred, y_true, delta=1.0):\n",
    "    error = y_pred - y_true\n",
    "    abs_error = np.abs(error)\n",
    "    return np.where(abs_error <= delta, error, delta * np.sign(error)) / len(y_pred)\n",
    "\n",
    "def cross_entropy_derivative(y_pred, y_true):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred)) / len(y_pred)\n",
    "\n",
    "# Test with sample data\n",
    "y_true = np.array([0, 1, 0, 1, 0.5])\n",
    "y_pred = np.array([0.1, 0.8, 0.3, 0.9, 0.6])\n",
    "\n",
    "print(\"Loss Function Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"MSE Loss: {mse_loss(y_pred, y_true):.6f}\")\n",
    "print(f\"MAE Loss: {mae_loss(y_pred, y_true):.6f}\")\n",
    "print(f\"Huber Loss: {huber_loss(y_pred, y_true):.6f}\")\n",
    "print(f\"Cross-Entropy Loss: {cross_entropy_loss(y_pred, y_true):.6f}\")\n",
    "\n",
    "print(\"\\nDerivatives:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"MSE Derivative: {mse_derivative(y_pred, y_true)}\")\n",
    "print(f\"MAE Derivative: {mae_derivative(y_pred, y_true)}\")\n",
    "print(f\"Huber Derivative: {huber_derivative(y_pred, y_true)}\")\n",
    "print(f\"Cross-Entropy Derivative: {cross_entropy_derivative(y_pred, y_true)}\")\n",
    "\n",
    "# Visualize loss functions\n",
    "x_vals = np.linspace(0, 1, 100)\n",
    "y_true_fixed = 0.5\n",
    "\n",
    "mse_vals = [(x - y_true_fixed)**2 for x in x_vals]\n",
    "mae_vals = [abs(x - y_true_fixed) for x in x_vals]\n",
    "huber_vals = [huber_loss(x, y_true_fixed) for x in x_vals]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x_vals, mse_vals, 'b-', linewidth=2, label='MSE')\n",
    "plt.plot(x_vals, mae_vals, 'r-', linewidth=2, label='MAE')\n",
    "plt.plot(x_vals, huber_vals, 'g-', linewidth=2, label='Huber')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Regression Loss Functions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "y_true_binary = 1.0\n",
    "ce_vals = [-y_true_binary * np.log(x) - (1 - y_true_binary) * np.log(1 - x) for x in x_vals]\n",
    "plt.plot(x_vals, ce_vals, 'purple', linewidth=2, label='Cross-Entropy')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Classification Loss Function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "# Derivatives\n",
    "mse_deriv_vals = [2 * (x - y_true_fixed) for x in x_vals]\n",
    "mae_deriv_vals = [np.sign(x - y_true_fixed) for x in x_vals]\n",
    "huber_deriv_vals = [huber_derivative(x, y_true_fixed) * len(x_vals) for x in x_vals]\n",
    "\n",
    "plt.plot(x_vals, mse_deriv_vals, 'b-', linewidth=2, label='MSE Derivative')\n",
    "plt.plot(x_vals, mae_deriv_vals, 'r-', linewidth=2, label='MAE Derivative')\n",
    "plt.plot(x_vals, huber_deriv_vals, 'g-', linewidth=2, label='Huber Derivative')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Derivative')\n",
    "plt.title('Loss Function Derivatives')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated key calculus concepts applied to machine learning and data science:\n",
    "\n",
    "1. **Derivatives and Gradient Descent**: Used for optimization in linear regression\n",
    "2. **Integration and Probability**: Calculated probabilities and expected values\n",
    "3. **Constrained Optimization**: Solved problems with Lagrange multipliers\n",
    "4. **Neural Networks**: Implemented backpropagation using the chain rule\n",
    "5. **Advanced Optimization**: Compared different optimization algorithms\n",
    "6. **Expected Values**: Used integration for probability calculations\n",
    "7. **Loss Functions**: Analyzed different loss functions and their derivatives\n",
    "\n",
    "These concepts form the mathematical foundation of modern machine learning algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
