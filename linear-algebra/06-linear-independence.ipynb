{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Independence and Basis\n\n[![Chapter](https://img.shields.io/badge/Chapter-6-blue.svg)]()\n[![Topic](https://img.shields.io/badge/Topic-Linear_Independence-green.svg)]()\n[![Difficulty](https://img.shields.io/badge/Difficulty-Intermediate-orange.svg)]()\n\n## Introduction\n\nLinear independence and basis are fundamental concepts that determine the structure and dimension of vector spaces. Understanding these concepts is crucial for solving systems of equations, performing coordinate transformations, and analyzing data in machine learning. In ML, the concepts of independence and basis underlie feature selection, dimensionality reduction, and the expressiveness of models.\n\n### Why Linear Independence and Basis Matter in AI/ML\n\n1. **Feature Selection**: Redundant features are linearly dependent; removing them improves model efficiency\n2. **Dimensionality Reduction**: PCA finds a new basis of independent directions (principal components)\n3. **Model Expressiveness**: The basis determines the space of possible solutions\n4. **Coordinate Systems**: Changing basis is essential for understanding embeddings and transformations\n\n## Linear Independence\n\nA set of vectors $\\{v_1, v_2, \\ldots, v_n\\}$ is linearly independent if the only solution to:\n$$c_1 v_1 + c_2 v_2 + \\cdots + c_n v_n = 0$$\nis $c_1 = c_2 = \\cdots = c_n = 0$ (the trivial solution).\n\n### Mathematical Definition\nVectors $v_1, v_2, \\ldots, v_n$ are linearly independent if:\n- The equation $c_1 v_1 + c_2 v_2 + \\cdots + c_n v_n = 0$ has only the trivial solution\n- No vector in the set can be written as a linear combination of the others\n- The rank of the matrix $[v_1\\ v_2\\ \\ldots\\ v_n]$ equals $n$\n\n**Geometric Interpretation:**\n- In $\\mathbb{R}^2$, two vectors are independent if they are not collinear\n- In $\\mathbb{R}^3$, three vectors are independent if they do not all lie in the same plane\n\n**Why It Matters:**\n- The maximum number of linearly independent vectors in a space is its dimension\n- Basis vectors must be linearly independent\n- In ML, linearly dependent features do not add new information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n\ndef is_linearly_independent(vectors, tol=1e-10):\n    \"\"\"Check if vectors are linearly independent\"\"\"\n    if not vectors:\n        return True\n    \n    # Convert to matrix\n    matrix = np.column_stack(vectors)\n    \n    # Check rank\n    rank = np.linalg.matrix_rank(matrix)\n    \n    return rank == len(vectors)\n\n# Example 1: Linearly independent vectors (standard basis in ℝ³)\nv1 = np.array([1, 0, 0])\nv2 = np.array([0, 1, 0])\nv3 = np.array([0, 0, 1])\n\nindependent_vectors = [v1, v2, v3]\nprint(\"Example 1: Standard basis vectors\")\nprint(f\"Are linearly independent: {is_linearly_independent(independent_vectors)}\")\n\n# Example 2: Linearly dependent vectors (v4 = v1 + v2)\nv4 = np.array([1, 1, 0])\ndependent_vectors = [v1, v2, v4]\nprint(\"\\nExample 2: Including dependent vector\")\nprint(f\"Are linearly independent: {is_linearly_independent(dependent_vectors)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Linear Independence (Detailed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_linear_independence_detailed(vectors):\n    \"\"\"Detailed test of linear independence\"\"\"\n    if not vectors:\n        return True, \"Empty set is linearly independent\"\n    \n    matrix = np.column_stack(vectors)\n    rank = np.linalg.matrix_rank(matrix)\n    n_vectors = len(vectors)\n    \n    print(f\"Number of vectors: {n_vectors}\")\n    print(f\"Matrix rank: {rank}\")\n    print(f\"Matrix shape: {matrix.shape}\")\n    \n    if rank == n_vectors:\n        print(\"✓ Vectors are linearly independent\")\n        return True, \"Full rank\"\n    else:\n        print(\"✗ Vectors are linearly dependent\")\n        \n        # Find dependent vectors using SVD\n        U, S, Vt = np.linalg.svd(matrix)\n        tol = 1e-10\n        independent_cols = np.where(S > tol)[0]\n        dependent_cols = np.where(S <= tol)[0]\n        \n        print(f\"Independent columns: {independent_cols}\")\n        print(f\"Dependent columns: {dependent_cols}\")\n        \n        return False, f\"Rank {rank} < {n_vectors}\"\n\n# Test with different sets\nprint(\"=== Testing Linear Independence ===\")\ntest_vectors1 = [\n    np.array([1, 2, 3]),\n    np.array([4, 5, 6]),\n    np.array([7, 8, 9])\n]\nis_indep1, msg1 = test_linear_independence_detailed(test_vectors1)\n\nprint(\"\\n\" + \"=\"*50)\ntest_vectors2 = [\n    np.array([1, 0, 0]),\n    np.array([0, 1, 0]),\n    np.array([1, 1, 0])\n]\nis_indep2, msg2 = test_linear_independence_detailed(test_vectors2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basis\n\nA **basis** for a vector space $V$ is a linearly independent set that spans $V$.\n\n### Properties of a Basis\n1. **Linear Independence**: All vectors in the basis are linearly independent\n2. **Spanning**: Every vector in $V$ can be written as a linear combination of basis vectors\n3. **Minimal**: No proper subset spans $V$\n4. **Unique Representation**: Each vector has a unique representation in terms of the basis\n\n**Geometric Interpretation:**\n- In $\\mathbb{R}^2$, any two non-collinear vectors form a basis\n- In $\\mathbb{R}^3$, any three non-coplanar vectors form a basis\n- The standard basis for $\\mathbb{R}^n$ is $\\{e_1, \\ldots, e_n\\}$\n\n**Why It Matters:**\n- The basis provides a coordinate system for the space\n- Dimensionality reduction (PCA) finds a new basis for the data\n- The number of features in ML is the dimension of the feature space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_basis(vectors):\n    \"\"\"Find a basis for the span of vectors\"\"\"\n    if not vectors:\n        return []\n    \n    matrix = np.column_stack(vectors)\n    \n    # Use QR decomposition with pivoting\n    Q, R, P = np.linalg.qr(matrix, mode='full', pivoting=True)\n    \n    # Find rank\n    rank = np.linalg.matrix_rank(matrix)\n    \n    # Return first 'rank' vectors as basis\n    basis = [vectors[P[i]] for i in range(rank)]\n    \n    return basis\n\n# Example: Find basis for a set of vectors\nvectors = [\n    np.array([1, 2, 3]),\n    np.array([4, 5, 6]),\n    np.array([7, 8, 9]),\n    np.array([2, 4, 6])  # This is 2 * first vector\n]\n\nprint(\"Original vectors:\")\nfor i, v in enumerate(vectors):\n    print(f\"v{i+1} = {v}\")\n\nbasis = find_basis(vectors)\nprint(f\"\\nBasis vectors (dimension: {len(basis)}):\")\nfor i, v in enumerate(basis):\n    print(f\"b{i+1} = {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standard Basis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def standard_basis(n):\n    \"\"\"Generate standard basis for ℝⁿ\"\"\"\n    basis = []\n    for i in range(n):\n        e_i = np.zeros(n)\n        e_i[i] = 1\n        basis.append(e_i)\n    return basis\n\n# Generate standard basis for ℝ³\nstd_basis_3d = standard_basis(3)\nprint(\"Standard basis for ℝ³:\")\nfor i, e in enumerate(std_basis_3d):\n    print(f\"e{i+1} = {e}\")\n\n# Verify it's a basis\nprint(f\"\\nIs standard basis linearly independent: {is_linearly_independent(std_basis_3d)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Coordinate Systems\n\n### Vector Representation in Different Bases\n\nThe concept of representing vectors in different bases is fundamental to linear algebra and has profound implications in machine learning. When we represent a vector in different bases, we're essentially changing our \"coordinate system\" or \"perspective\" for describing the same mathematical object.\n\n**Mathematical Foundation:**\nGiven a vector space V with basis B = {b₁, b₂, ..., bₙ}, any vector v ∈ V can be uniquely written as:\nv = c₁b₁ + c₂b₂ + ... + cₙbₙ\n\nThe coefficients c₁, c₂, ..., cₙ are called the coordinates of v with respect to basis B, denoted [v]B.\n\n**Key Properties:**\n1. **Uniqueness**: Each vector has exactly one representation in a given basis\n2. **Completeness**: Every vector can be represented in any basis\n3. **Linearity**: The coordinate mapping preserves vector operations\n\n**Geometric Interpretation:**\nThink of changing bases as changing the \"ruler\" or \"measuring stick\" we use to describe vectors. Just as we can describe a location using different coordinate systems (Cartesian, polar, etc.), we can describe vectors using different bases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vector_in_basis(vector, basis):\n    \"\"\"\n    Find coordinates of vector in given basis\n    \n    Mathematical approach:\n    We solve the system: vector = c₁b₁ + c₂b₂ + ... + cₙbₙ\n    This is equivalent to solving: B @ coordinates = vector\n    where B is the matrix whose columns are the basis vectors\n    \n    Parameters:\n    vector: numpy array - the vector to represent\n    basis: list of numpy arrays - the basis vectors\n    \n    Returns:\n    numpy array - coordinates of vector in the basis\n    \"\"\"\n    # Construct basis matrix B = [b₁ | b₂ | ... | bₙ]\n    basis_matrix = np.column_stack(basis)\n    \n    # Solve B @ coordinates = vector\n    # This gives us the unique coordinates\n    coordinates = np.linalg.solve(basis_matrix, vector)\n    \n    return coordinates\n\ndef reconstruct_vector(coordinates, basis):\n    \"\"\"\n    Reconstruct vector from coordinates in basis\n    \n    This is the inverse operation: v = c₁b₁ + c₂b₂ + ... + cₙbₙ\n    \n    Parameters:\n    coordinates: numpy array - coordinates in the basis\n    basis: list of numpy arrays - the basis vectors\n    \n    Returns:\n    numpy array - the reconstructed vector\n    \"\"\"\n    basis_matrix = np.column_stack(basis)\n    vector = basis_matrix @ coordinates\n    return vector\n\ndef verify_basis_representation(vector, basis, coordinates):\n    \"\"\"\n    Verify that coordinates correctly represent vector in basis\n    \n    Parameters:\n    vector: numpy array - original vector\n    basis: list of numpy arrays - basis vectors\n    coordinates: numpy array - coordinates to verify\n    \n    Returns:\n    bool - True if representation is correct\n    \"\"\"\n    reconstructed = reconstruct_vector(coordinates, basis)\n    error = np.linalg.norm(vector - reconstructed)\n    return error < 1e-10, error\n\n# Example: Vector in different bases\nprint(\"=== Vector Representation in Different Bases ===\")\n\nv = np.array([3, 4])\nprint(f\"Original vector: {v}\")\n\n# Standard basis (canonical basis)\nstd_basis = [np.array([1, 0]), np.array([0, 1])]\ncoords_std = vector_in_basis(v, std_basis)\nprint(f\"\\nStandard basis: {[b.tolist() for b in std_basis]}\")\nprint(f\"Coordinates in standard basis: {coords_std}\")\n\n# Verify reconstruction\nis_correct, error = verify_basis_representation(v, std_basis, coords_std)\nprint(f\"Reconstruction correct: {is_correct} (error: {error:.2e})\")\n\n# Different basis (non-orthogonal)\nnew_basis = [np.array([1, 1]), np.array([1, -1])]\ncoords_new = vector_in_basis(v, new_basis)\nprint(f\"\\nNew basis: {[b.tolist() for b in new_basis]}\")\nprint(f\"Coordinates in new basis: {coords_new}\")\n\n# Verify reconstruction\nis_correct, error = verify_basis_representation(v, new_basis, coords_new)\nprint(f\"Reconstruction correct: {is_correct} (error: {error:.2e})\")\n\n# Geometric interpretation\nprint(f\"\\nGeometric interpretation:\")\nprint(f\"In standard basis: {coords_std[0]:.2f} * (1,0) + {coords_std[1]:.2f} * (0,1)\")\nprint(f\"In new basis: {coords_new[0]:.2f} * (1,1) + {coords_new[1]:.2f} * (1,-1)\")\n\n# Test with multiple vectors\ntest_vectors = [np.array([1, 0]), np.array([0, 1]), np.array([2, 3])]\nprint(f\"\\n=== Testing Multiple Vectors ===\")\n\nfor i, test_v in enumerate(test_vectors):\n    coords_std = vector_in_basis(test_v, std_basis)\n    coords_new = vector_in_basis(test_v, new_basis)\n    \n    print(f\"Vector {test_v}:\")\n    print(f\"  Standard coords: {coords_std}\")\n    print(f\"  New basis coords: {coords_new}\")\n    \n    # Verify both representations\n    is_correct_std, _ = verify_basis_representation(test_v, std_basis, coords_std)\n    is_correct_new, _ = verify_basis_representation(test_v, new_basis, coords_new)\n    print(f\"  Both correct: {is_correct_std and is_correct_new}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change of Basis\n\n### Change of Basis Matrix\n\nThe change of basis transformation is a fundamental operation that allows us to convert between different coordinate representations of the same vector space. This is crucial in machine learning for feature transformations, dimensionality reduction, and understanding data from different perspectives.\n\n**Mathematical Foundation:**\nGiven two bases B₁ and B₂ for vector space V, the change of basis matrix P satisfies:\n[v]B₂ = P⁻¹[v]B₁\n\nwhere [v]B₁ and [v]B₂ are the coordinate representations of vector v in bases B₁ and B₂ respectively.\n\n**Key Properties:**\n1. **Invertibility**: P is always invertible\n2. **Composition**: P₁→₂ @ P₂→₃ = P₁→₃\n3. **Identity**: P₁→₁ = I\n\n**Geometric Interpretation:**\nThe change of basis matrix P tells us how to \"rotate\" or \"transform\" our coordinate system. Each column of P represents the coordinates of the new basis vectors in the old basis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def change_of_basis_matrix(old_basis, new_basis):\n    \"\"\"\n    Find change of basis matrix from old_basis to new_basis\n    \n    Mathematical approach:\n    P = [new_basis]_old_basis\n    This matrix satisfies: [v]_new = P^(-1) [v]_old\n    \n    The matrix P has the property that:\n    P @ [v]_old = [v]_new\n    \n    Parameters:\n    old_basis: list of numpy arrays - original basis\n    new_basis: list of numpy arrays - new basis\n    \n    Returns:\n    numpy array - change of basis matrix P\n    \"\"\"\n    # Construct matrices for both bases\n    old_matrix = np.column_stack(old_basis)\n    new_matrix = np.column_stack(new_basis)\n    \n    # P = new_matrix @ old_matrix^(-1)\n    # This gives us the transformation matrix\n    P = new_matrix @ np.linalg.inv(old_matrix)\n    \n    return P\n\ndef change_coordinates(vector, old_basis, new_basis):\n    \"\"\"\n    Change vector coordinates from old_basis to new_basis\n    \n    This is the direct application of the change of basis transformation\n    \n    Parameters:\n    vector: numpy array - vector in old basis coordinates\n    old_basis: list of numpy arrays - original basis\n    new_basis: list of numpy arrays - new basis\n    \n    Returns:\n    numpy array - coordinates in new basis\n    \"\"\"\n    P = change_of_basis_matrix(old_basis, new_basis)\n    \n    # Get coordinates in old basis\n    coords_old = vector_in_basis(vector, old_basis)\n    \n    # Apply change of basis transformation\n    coords_new = np.linalg.inv(P) @ coords_old\n    \n    return coords_new\n\ndef verify_change_of_basis(old_basis, new_basis, vector):\n    \"\"\"\n    Verify change of basis transformation\n    \n    Parameters:\n    old_basis: list of numpy arrays - original basis\n    new_basis: list of numpy arrays - new basis\n    vector: numpy array - test vector\n    \n    Returns:\n    bool - True if transformation is correct\n    \"\"\"\n    # Method 1: Direct computation\n    coords_old = vector_in_basis(vector, old_basis)\n    coords_new_direct = vector_in_basis(vector, new_basis)\n    \n    # Method 2: Using change of basis matrix\n    P = change_of_basis_matrix(old_basis, new_basis)\n    coords_new_via_P = np.linalg.inv(P) @ coords_old\n    \n    # Compare results\n    error = np.linalg.norm(coords_new_direct - coords_new_via_P)\n    return error < 1e-10, error\n\n# Example: Change of basis\nprint(\"\\n=== Change of Basis Transformation ===\")\n\nold_basis = [np.array([1, 0]), np.array([0, 1])]  # Standard basis\nnew_basis = [np.array([1, 1]), np.array([1, -1])]  # New basis\n\nvector = np.array([3, 4])\n\nprint(f\"Vector: {vector}\")\nprint(f\"Old basis: {[b.tolist() for b in old_basis]}\")\nprint(f\"New basis: {[b.tolist() for b in new_basis]}\")\n\n# Method 1: Direct computation\ncoords_old = vector_in_basis(vector, old_basis)\ncoords_new = vector_in_basis(vector, new_basis)\n\nprint(f\"\\nMethod 1 - Direct computation:\")\nprint(f\"Coordinates in old basis: {coords_old}\")\nprint(f\"Coordinates in new basis: {coords_new}\")\n\n# Method 2: Using change of basis matrix\nP = change_of_basis_matrix(old_basis, new_basis)\ncoords_new_via_P = np.linalg.inv(P) @ coords_old\n\nprint(f\"\\nMethod 2 - Using change of basis matrix:\")\nprint(f\"Change of basis matrix P:\")\nprint(P)\nprint(f\"Coordinates via change of basis: {coords_new_via_P}\")\n\n# Verify the transformation\nis_correct, error = verify_change_of_basis(old_basis, new_basis, vector)\nprint(f\"\\nTransformation verification:\")\nprint(f\"Methods agree: {is_correct} (error: {error:.2e})\")\n\n# Properties of change of basis matrix\nprint(f\"\\nProperties of change of basis matrix:\")\nprint(f\"P shape: {P.shape}\")\nprint(f\"P is invertible: {np.linalg.det(P) != 0}\")\nprint(f\"P^(-1) @ P = I (error: {np.linalg.norm(np.eye(2) - P @ np.linalg.inv(P)):.2e})\")\n\n# Test with multiple vectors\ntest_vectors = [np.array([1, 0]), np.array([0, 1]), np.array([2, 3])]\nprint(f\"\\n=== Testing Change of Basis with Multiple Vectors ===\")\n\nfor i, test_v in enumerate(test_vectors):\n    coords_old = vector_in_basis(test_v, old_basis)\n    coords_new_direct = vector_in_basis(test_v, new_basis)\n    coords_new_via_P = np.linalg.inv(P) @ coords_old\n    \n    print(f\"Vector {test_v}:\")\n    print(f\"  Old coords: {coords_old}\")\n    print(f\"  New coords (direct): {coords_new_direct}\")\n    print(f\"  New coords (via P): {coords_new_via_P}\")\n    \n    error = np.linalg.norm(coords_new_direct - coords_new_via_P)\n    print(f\"  Agreement: {error < 1e-10} (error: {error:.2e})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gram-Schmidt Orthogonalization\n\n### Orthogonal Basis\n\nThe Gram-Schmidt process is a method for converting a set of linearly independent vectors into an orthogonal (or orthonormal) set that spans the same subspace. This is fundamental in machine learning for creating orthogonal features, implementing QR decomposition, and building orthogonal bases for numerical stability.\n\n**Mathematical Foundation:**\nGiven linearly independent vectors v₁, v₂, ..., vₙ, the Gram-Schmidt process constructs orthogonal vectors u₁, u₂, ..., uₙ as follows:\n\nu₁ = v₁\nu₂ = v₂ - proj_u₁(v₂)\nu₃ = v₃ - proj_u₁(v₃) - proj_u₂(v₃)\n...\n\nwhere proj_u(v) = (v·u)/(u·u) * u is the projection of v onto u.\n\n**Key Properties:**\n1. **Preserves span**: span{u₁, u₂, ..., uₙ} = span{v₁, v₂, ..., vₙ}\n2. **Orthogonality**: uᵢ·uⱼ = 0 for i ≠ j\n3. **Uniqueness**: The orthogonal set is unique up to scaling\n\n**Geometric Interpretation:**\nGram-Schmidt is like \"straightening\" a set of vectors. We start with the first vector, then \"subtract out\" the component of each subsequent vector that lies in the direction of the previous orthogonal vectors, leaving only the \"perpendicular\" component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gram_schmidt(vectors, normalize=True):\n    \"\"\"\n    Apply Gram-Schmidt orthogonalization to vectors\n    \n    Mathematical approach:\n    For each vector v_i, we subtract its projection onto all previous\n    orthogonal vectors u_1, u_2, ..., u_{i-1}\n    \n    u_i = v_i - Σ_{j=1}^{i-1} proj_{u_j}(v_i)\n    where proj_u(v) = (v·u)/(u·u) * u\n    \n    Parameters:\n    vectors: list of numpy arrays - linearly independent vectors\n    normalize: bool - whether to normalize the orthogonal vectors\n    \n    Returns:\n    list of numpy arrays - orthogonal (or orthonormal) basis\n    \"\"\"\n    if not vectors:\n        return []\n    \n    orthogonal_basis = []\n    \n    for i, v in enumerate(vectors):\n        # Start with original vector\n        u = v.copy().astype(float)\n        \n        # Subtract projections onto previous orthogonal vectors\n        for j in range(i):\n            # Compute projection: proj_u_j(v_i) = (v_i · u_j) / (u_j · u_j) * u_j\n            numerator = np.dot(v, orthogonal_basis[j])\n            denominator = np.dot(orthogonal_basis[j], orthogonal_basis[j])\n            \n            if abs(denominator) > 1e-10:  # Avoid division by zero\n                proj_coeff = numerator / denominator\n                u = u - proj_coeff * orthogonal_basis[j]\n        \n        # Check if resulting vector is non-zero\n        norm = np.linalg.norm(u)\n        if norm > 1e-10:  # Avoid zero vectors\n            if normalize:\n                u = u / norm  # Normalize to unit length\n            orthogonal_basis.append(u)\n        else:\n            print(f\"Warning: Vector {i+1} became zero after orthogonalization\")\n    \n    return orthogonal_basis\n\ndef verify_gram_schmidt(original_vectors, orthogonal_vectors):\n    \"\"\"\n    Verify Gram-Schmidt orthogonalization\n    \n    Parameters:\n    original_vectors: list of numpy arrays - original vectors\n    orthogonal_vectors: list of numpy arrays - orthogonal vectors\n    \n    Returns:\n    dict - verification results\n    \"\"\"\n    results = {}\n    \n    # Check orthogonality\n    max_dot_product = 0\n    for i in range(len(orthogonal_vectors)):\n        for j in range(i+1, len(orthogonal_vectors)):\n            dot_product = abs(np.dot(orthogonal_vectors[i], orthogonal_vectors[j]))\n            max_dot_product = max(max_dot_product, dot_product)\n    \n    results['orthogonal'] = max_dot_product < 1e-10\n    \n    # Check normalization (if applicable)\n    norms = [np.linalg.norm(u) for u in orthogonal_vectors]\n    results['normalized'] = all(abs(norm - 1.0) < 1e-10 for norm in norms)\n    \n    # Check span preservation (simplified)\n    if len(original_vectors) == len(orthogonal_vectors):\n        results['span_preserved'] = True\n    else:\n        results['span_preserved'] = False\n    \n    return results\n\ndef gram_schmidt_with_projection_details(vectors):\n    \"\"\"\n    Gram-Schmidt with detailed projection information\n    \n    Parameters:\n    vectors: list of numpy arrays - linearly independent vectors\n    \n    Returns:\n    tuple - (orthogonal_basis, projection_details)\n    \"\"\"\n    if not vectors:\n        return [], []\n    \n    orthogonal_basis = []\n    projection_details = []\n    \n    for i, v in enumerate(vectors):\n        u = v.copy().astype(float)\n        projections = []\n        \n        print(f\"\\nProcessing vector v{i+1} = {v}\")\n        \n        for j in range(i):\n            numerator = np.dot(v, orthogonal_basis[j])\n            denominator = np.dot(orthogonal_basis[j], orthogonal_basis[j])\n            \n            if abs(denominator) > 1e-10:\n                proj_coeff = numerator / denominator\n                proj_vector = proj_coeff * orthogonal_basis[j]\n                u = u - proj_vector\n                \n                projections.append({\n                    'onto': j,\n                    'coefficient': proj_coeff,\n                    'projection': proj_vector\n                })\n                \n                print(f\"  Subtract projection onto u{j+1}: {proj_coeff:.3f} * u{j+1}\")\n        \n        norm = np.linalg.norm(u)\n        if norm > 1e-10:\n            u_normalized = u / norm\n            orthogonal_basis.append(u_normalized)\n            projection_details.append(projections)\n            \n            print(f\"  Result: u{i+1} = {u_normalized}\")\n        else:\n            print(f\"  Warning: Vector became zero!\")\n    \n    return orthogonal_basis, projection_details\n\n# Example: Gram-Schmidt orthogonalization\nprint(\"\\n=== Gram-Schmidt Orthogonalization ===\")\n\nvectors = [\n    np.array([1, 1, 0]),\n    np.array([1, 0, 1]),\n    np.array([0, 1, 1])\n]\n\nprint(\"Original vectors:\")\nfor i, v in enumerate(vectors):\n    print(f\"v{i+1} = {v}\")\n\n# Apply Gram-Schmidt with detailed output\northogonal_basis, details = gram_schmidt_with_projection_details(vectors)\n\nprint(f\"\\nOrthogonal basis:\")\nfor i, u in enumerate(orthogonal_basis):\n    print(f\"u{i+1} = {u}\")\n\n# Verify orthogonality\nprint(f\"\\nOrthogonality check:\")\nmax_dot_product = 0\nfor i in range(len(orthogonal_basis)):\n    for j in range(i+1, len(orthogonal_basis)):\n        dot_product = np.dot(orthogonal_basis[i], orthogonal_basis[j])\n        max_dot_product = max(max_dot_product, abs(dot_product))\n        print(f\"u{i+1} · u{j+1} = {dot_product:.2e}\")\n\nprint(f\"Maximum off-diagonal dot product: {max_dot_product:.2e}\")\n\n# Verify normalization\nprint(f\"\\nNormalization check:\")\nfor i, u in enumerate(orthogonal_basis):\n    norm = np.linalg.norm(u)\n    print(f\"||u{i+1}|| = {norm:.6f}\")\n\n# Comprehensive verification\nverification = verify_gram_schmidt(vectors, orthogonal_basis)\nprint(f\"\\nVerification results:\")\nfor key, value in verification.items():\n    print(f\"  {key}: {value}\")\n\n# Test with different vector sets\nprint(f\"\\n=== Testing Different Vector Sets ===\")\n\n# Set 1: Already orthogonal vectors\northogonal_vectors = [\n    np.array([1, 0, 0]),\n    np.array([0, 1, 0]),\n    np.array([0, 0, 1])\n]\n\north_basis1 = gram_schmidt(orthogonal_vectors)\nprint(f\"Already orthogonal vectors:\")\nfor i, u in enumerate(orth_basis1):\n    print(f\"  u{i+1} = {u}\")\n\n# Set 2: Linearly dependent vectors\ndependent_vectors = [\n    np.array([1, 0, 0]),\n    np.array([2, 0, 0]),  # Multiple of first vector\n    np.array([0, 1, 0])\n]\n\north_basis2 = gram_schmidt(dependent_vectors)\nprint(f\"\\nLinearly dependent vectors:\")\nprint(f\"Original count: {len(dependent_vectors)}\")\nprint(f\"Orthogonal count: {len(orth_basis2)}\")\nfor i, u in enumerate(orth_basis2):\n    print(f\"  u{i+1} = {u}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applications in Machine Learning\n\nLinear independence and basis concepts are fundamental to many machine learning algorithms and techniques. Understanding these concepts helps us design better features, reduce dimensionality, and improve model performance.\n\n### Feature Selection and Dimensionality Reduction\n\n**Mathematical Foundation:**\nIn machine learning, we often work with feature matrices X ∈ ℝ^(n×d) where n is the number of samples and d is the number of features. Linear dependencies among features can cause:\n\n1. **Multicollinearity**: Features that are linearly dependent can cause numerical instability in regression models\n2. **Redundant Information**: Dependent features don't add new information\n3. **Overfitting**: Models may fit to noise in dependent features\n\n**Key Concepts:**\n- **Feature Rank**: The rank of the feature matrix determines the maximum number of linearly independent features\n- **Feature Selection**: Choosing a subset of linearly independent features that preserve the most information\n- **Dimensionality Reduction**: Reducing the number of features while maintaining model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_linearly_independent_features(feature_matrix, tol=1e-10, method='qr'):\n    \"\"\"\n    Select linearly independent features from matrix\n    \n    Mathematical approaches:\n    1. QR decomposition with pivoting: A = Q @ R @ P^T\n       where P is a permutation matrix that identifies independent columns\n    2. SVD decomposition: A = U @ Σ @ V^T\n       where the rank is determined by non-zero singular values\n    3. Gaussian elimination: Row reduction to identify pivot columns\n    \n    Parameters:\n    feature_matrix: numpy array - feature matrix (samples × features)\n    tol: float - tolerance for numerical rank determination\n    method: str - 'qr', 'svd', or 'gaussian'\n    \n    Returns:\n    tuple - (independent_features, independent_indices, rank)\n    \"\"\"\n    n_samples, n_features = feature_matrix.shape\n    \n    if method == 'qr':\n        # QR decomposition with pivoting\n        Q, R, P = np.linalg.qr(feature_matrix, mode='full', pivoting=True)\n        \n        # Find rank by counting non-zero diagonal elements of R\n        rank = np.sum(np.abs(np.diag(R)) > tol)\n        \n        # Select independent features\n        independent_indices = P[:rank]\n        independent_features = feature_matrix[:, independent_indices]\n        \n    elif method == 'svd':\n        # SVD decomposition\n        U, S, Vt = np.linalg.svd(feature_matrix, full_matrices=False)\n        \n        # Find rank by counting non-zero singular values\n        rank = np.sum(S > tol)\n        \n        # Select independent features (first rank columns of U)\n        independent_features = U[:, :rank] @ np.diag(S[:rank])\n        independent_indices = np.arange(rank)\n        \n    elif method == 'gaussian':\n        # Gaussian elimination approach\n        A = feature_matrix.copy()\n        n_rows, n_cols = A.shape\n        rank = 0\n        independent_indices = []\n        \n        for col in range(n_cols):\n            # Find pivot\n            pivot_row = rank\n            for row in range(rank, n_rows):\n                if abs(A[row, col]) > abs(A[pivot_row, col]):\n                    pivot_row = row\n            \n            if abs(A[pivot_row, col]) > tol:\n                # Swap rows if necessary\n                if pivot_row != rank:\n                    A[rank, :], A[pivot_row, :] = A[pivot_row, :].copy(), A[rank, :].copy()\n                \n                # Eliminate column\n                for row in range(rank + 1, n_rows):\n                    factor = A[row, col] / A[rank, col]\n                    A[row, :] -= factor * A[rank, :]\n                \n                independent_indices.append(col)\n                rank += 1\n        \n        independent_features = feature_matrix[:, independent_indices]\n        independent_indices = np.array(independent_indices)\n    \n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n    \n    return independent_features, independent_indices, rank\n\ndef analyze_feature_dependencies(feature_matrix, feature_names=None):\n    \"\"\"\n    Analyze linear dependencies in feature matrix\n    \n    Parameters:\n    feature_matrix: numpy array - feature matrix\n    feature_names: list - names of features (optional)\n    \n    Returns:\n    dict - analysis results\n    \"\"\"\n    n_samples, n_features = feature_matrix.shape\n    \n    if feature_names is None:\n        feature_names = [f\"Feature_{i}\" for i in range(n_features)]\n    \n    # Compute correlation matrix\n    corr_matrix = np.corrcoef(feature_matrix.T)\n    \n    # Find highly correlated feature pairs\n    high_corr_pairs = []\n    for i in range(n_features):\n        for j in range(i+1, n_features):\n            if abs(corr_matrix[i, j]) > 0.95:  # High correlation threshold\n                high_corr_pairs.append({\n                    'feature1': feature_names[i],\n                    'feature2': feature_names[j],\n                    'correlation': corr_matrix[i, j]\n                })\n    \n    # Compute rank and condition number\n    rank = np.linalg.matrix_rank(feature_matrix)\n    condition_number = np.linalg.cond(feature_matrix)\n    \n    # Find independent features\n    independent_features, independent_indices, computed_rank = select_linearly_independent_features(feature_matrix)\n    \n    return {\n        'original_shape': feature_matrix.shape,\n        'rank': rank,\n        'computed_rank': computed_rank,\n        'condition_number': condition_number,\n        'independent_features': independent_features,\n        'independent_indices': independent_indices,\n        'independent_feature_names': [feature_names[i] for i in independent_indices],\n        'high_correlation_pairs': high_corr_pairs,\n        'correlation_matrix': corr_matrix\n    }\n\ndef verify_feature_selection(original_matrix, independent_matrix, independent_indices):\n    \"\"\"\n    Verify that selected features preserve the original space\n    \n    Parameters:\n    original_matrix: numpy array - original feature matrix\n    independent_matrix: numpy array - selected independent features\n    independent_indices: numpy array - indices of selected features\n    \n    Returns:\n    dict - verification results\n    \"\"\"\n    # Check if independent features can reconstruct original space\n    # This is a simplified check - in practice, we'd need more sophisticated methods\n    \n    # Compute projection matrix\n    P = independent_matrix @ np.linalg.pinv(independent_matrix)\n    \n    # Project original matrix onto independent subspace\n    projected_matrix = P @ original_matrix\n    \n    # Compute reconstruction error\n    reconstruction_error = np.linalg.norm(original_matrix - projected_matrix, 'fro')\n    relative_error = reconstruction_error / np.linalg.norm(original_matrix, 'fro')\n    \n    # Check rank preservation\n    original_rank = np.linalg.matrix_rank(original_matrix)\n    independent_rank = np.linalg.matrix_rank(independent_matrix)\n    \n    return {\n        'reconstruction_error': reconstruction_error,\n        'relative_error': relative_error,\n        'original_rank': original_rank,\n        'independent_rank': independent_rank,\n        'rank_preserved': original_rank == independent_rank,\n        'good_reconstruction': relative_error < 1e-10\n    }\n\n# Example: Feature selection\nprint(\"=== Feature Selection and Dimensionality Reduction ===\")\n\nnp.random.seed(42)\n# Create feature matrix with some linear dependencies\nn_samples, n_features = 100, 5\nfeatures = np.random.randn(n_samples, n_features)\nfeature_names = [f\"X{i+1}\" for i in range(n_features)]\n\n# Add linear dependencies\nfeatures[:, 2] = 2 * features[:, 0] + 3 * features[:, 1]  # Feature 2 is dependent\nfeatures[:, 4] = features[:, 0] - features[:, 1]          # Feature 4 is dependent\n\nprint(f\"Original feature matrix shape: {features.shape}\")\nprint(f\"Original rank: {np.linalg.matrix_rank(features)}\")\n\n# Analyze feature dependencies\nanalysis = analyze_feature_dependencies(features, feature_names)\nprint(f\"\\nFeature dependency analysis:\")\nprint(f\"Rank: {analysis['rank']}\")\nprint(f\"Condition number: {analysis['condition_number']:.2e}\")\nprint(f\"High correlation pairs: {len(analysis['high_correlation_pairs'])}\")\n\nfor pair in analysis['high_correlation_pairs']:\n    print(f\"  {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n\n# Select independent features using different methods\nmethods = ['qr', 'svd', 'gaussian']\nfor method in methods:\n    print(f\"\\n--- Method: {method.upper()} ---\")\n    independent_features, indices, rank = select_linearly_independent_features(features, method=method)\n    \n    print(f\"Selected features: {[feature_names[i] for i in indices]}\")\n    print(f\"Independent features shape: {independent_features.shape}\")\n    print(f\"Rank: {rank}\")\n    \n    # Verify selection\n    verification = verify_feature_selection(features, independent_features, indices)\n    print(f\"Reconstruction error: {verification['relative_error']:.2e}\")\n    print(f\"Rank preserved: {verification['rank_preserved']}\")\n\n# Test with real-world scenario\nprint(f\"\\n=== Real-world Feature Selection Example ===\")\n\n# Simulate a dataset with known dependencies\nnp.random.seed(123)\nn_samples = 200\n\n# Generate base features\nbase_features = np.random.randn(n_samples, 3)\n\n# Create derived features\nderived_features = np.column_stack([\n    base_features[:, 0] + 0.1 * np.random.randn(n_samples),  # Slightly noisy copy\n    2 * base_features[:, 1] - base_features[:, 2],            # Linear combination\n    base_features[:, 0] * base_features[:, 1],                # Non-linear feature\n    base_features[:, 2] ** 2                                  # Quadratic feature\n])\n\n# Combine all features\nall_features = np.column_stack([base_features, derived_features])\nfeature_names = ['Base1', 'Base2', 'Base3', 'Derived1', 'Derived2', 'Derived3', 'Derived4']\n\nprint(f\"Combined feature matrix shape: {all_features.shape}\")\nprint(f\"Combined rank: {np.linalg.matrix_rank(all_features)}\")\n\n# Analyze and select features\nanalysis = analyze_feature_dependencies(all_features, feature_names)\nindependent_features, indices, rank = select_linearly_independent_features(all_features)\n\nprint(f\"\\nSelected independent features:\")\nfor i, idx in enumerate(indices):\n    print(f\"  {i+1}. {feature_names[idx]}\")\n\nprint(f\"Reduced from {all_features.shape[1]} to {len(indices)} features\")\nprint(f\"Rank: {rank}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Principal Component Analysis (PCA)\n\n**Mathematical Foundation:**\nPCA finds an orthogonal basis for the data that maximizes variance along each direction. The principal components are the eigenvectors of the covariance matrix, ordered by their corresponding eigenvalues.\n\n**Key Properties:**\n1. **Orthogonality**: Principal components are orthogonal to each other\n2. **Variance Maximization**: Each component captures maximum variance in the remaining directions\n3. **Dimensionality Reduction**: We can truncate to k components while preserving most variance\n\n**Geometric Interpretation:**\nPCA finds the \"natural\" coordinate system for the data. The first principal component points in the direction of maximum variance, the second in the direction of maximum variance perpendicular to the first, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pca_basis(data, n_components=None, center=True, scale=False):\n    \"\"\"\n    Find PCA basis for data\n    \n    Mathematical approach:\n    1. Center the data: X_centered = X - mean(X)\n    2. Compute covariance matrix: C = (1/n) X_centered^T @ X_centered\n    3. Find eigenvalues and eigenvectors: C @ v = λ @ v\n    4. Sort by eigenvalues (descending order)\n    5. Principal components are the eigenvectors\n    \n    Parameters:\n    data: numpy array - data matrix (samples × features)\n    n_components: int - number of components to return\n    center: bool - whether to center the data\n    scale: bool - whether to scale the data\n    \n    Returns:\n    tuple - (basis_vectors, eigenvalues, explained_variance_ratio, mean_vector)\n    \"\"\"\n    n_samples, n_features = data.shape\n    \n    # Center the data\n    if center:\n        mean_vector = np.mean(data, axis=0)\n        data_centered = data - mean_vector\n    else:\n        mean_vector = np.zeros(n_features)\n        data_centered = data\n    \n    # Scale the data (optional)\n    if scale:\n        std_vector = np.std(data_centered, axis=0)\n        data_centered = data_centered / std_vector\n    \n    # Compute covariance matrix\n    cov_matrix = np.cov(data_centered.T)\n    \n    # Find eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n    \n    # Sort by eigenvalues (descending)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues_sorted = eigenvalues[sorted_indices]\n    eigenvectors_sorted = eigenvectors[:, sorted_indices]\n    \n    # Determine number of components\n    if n_components is None:\n        n_components = len(eigenvalues)\n    elif n_components > len(eigenvalues):\n        n_components = len(eigenvalues)\n    \n    # Extract basis vectors and eigenvalues\n    basis_vectors = [eigenvectors_sorted[:, i] for i in range(n_components)]\n    eigenvalues_selected = eigenvalues_sorted[:n_components]\n    \n    # Compute explained variance ratio\n    total_variance = np.sum(eigenvalues_sorted)\n    explained_variance_ratio = eigenvalues_selected / total_variance\n    \n    return basis_vectors, eigenvalues_selected, explained_variance_ratio, mean_vector\n\ndef project_data_pca(data, basis_vectors, mean_vector=None):\n    \"\"\"\n    Project data onto PCA basis\n    \n    Parameters:\n    data: numpy array - data to project\n    basis_vectors: list - PCA basis vectors\n    mean_vector: numpy array - mean vector (if data was centered)\n    \n    Returns:\n    numpy array - projected data\n    \"\"\"\n    if mean_vector is not None:\n        data_centered = data - mean_vector\n    else:\n        data_centered = data\n    \n    # Project onto basis\n    basis_matrix = np.column_stack(basis_vectors)\n    projected_data = data_centered @ basis_matrix\n    \n    return projected_data\n\ndef reconstruct_data_pca(projected_data, basis_vectors, mean_vector=None):\n    \"\"\"\n    Reconstruct data from PCA projection\n    \n    Parameters:\n    projected_data: numpy array - projected data\n    basis_vectors: list - PCA basis vectors\n    mean_vector: numpy array - mean vector (if data was centered)\n    \n    Returns:\n    numpy array - reconstructed data\n    \"\"\"\n    # Reconstruct from basis\n    basis_matrix = np.column_stack(basis_vectors)\n    reconstructed_centered = projected_data @ basis_matrix.T\n    \n    if mean_vector is not None:\n        reconstructed_data = reconstructed_centered + mean_vector\n    else:\n        reconstructed_data = reconstructed_centered\n    \n    return reconstructed_data\n\ndef analyze_pca_quality(original_data, reconstructed_data):\n    \"\"\"\n    Analyze quality of PCA reconstruction\n    \n    Parameters:\n    original_data: numpy array - original data\n    reconstructed_data: numpy array - reconstructed data\n    \n    Returns:\n    dict - quality metrics\n    \"\"\"\n    # Compute reconstruction error\n    mse = np.mean((original_data - reconstructed_data) ** 2)\n    rmse = np.sqrt(mse)\n    \n    # Compute relative error\n    relative_error = np.linalg.norm(original_data - reconstructed_data, 'fro') / np.linalg.norm(original_data, 'fro')\n    \n    # Compute explained variance\n    total_variance = np.var(original_data, axis=0).sum()\n    explained_variance = total_variance - np.var(original_data - reconstructed_data, axis=0).sum()\n    explained_variance_ratio = explained_variance / total_variance\n    \n    return {\n        'mse': mse,\n        'rmse': rmse,\n        'relative_error': relative_error,\n        'explained_variance_ratio': explained_variance_ratio\n    }\n\n# Example: PCA basis\nprint(\"\\n=== Principal Component Analysis ===\")\n\nnp.random.seed(42)\n# Create correlated data\nn_samples = 100\ndata = np.random.randn(n_samples, 3)\ndata[:, 2] = 0.8 * data[:, 0] + 0.2 * np.random.randn(n_samples)  # Add correlation\n\nprint(f\"Data shape: {data.shape}\")\nprint(f\"Data correlation matrix:\")\nprint(np.corrcoef(data.T))\n\n# Find PCA basis\npca_basis_vectors, eigenvalues, explained_variance_ratio, mean_vector = pca_basis(data, n_components=2)\n\nprint(f\"\\nPCA basis vectors:\")\nfor i, v in enumerate(pca_basis_vectors):\n    print(f\"PC{i+1} = {v}\")\n\nprint(f\"\\nEigenvalues:\")\nfor i, eigenval in enumerate(eigenvalues):\n    print(f\"λ{i+1} = {eigenval:.3f}\")\n\nprint(f\"\\nExplained variance ratios:\")\nfor i, ratio in enumerate(explained_variance_ratio):\n    print(f\"PC{i+1}: {ratio:.3f} ({ratio*100:.1f}%)\")\n\nprint(f\"Cumulative explained variance: {np.sum(explained_variance_ratio):.3f}\")\n\n# Project data onto PCA basis\nprojected_data = project_data_pca(data, pca_basis_vectors, mean_vector)\nprint(f\"\\nProjected data shape: {projected_data.shape}\")\n\n# Reconstruct data\nreconstructed_data = reconstruct_data_pca(projected_data, pca_basis_vectors, mean_vector)\n\n# Analyze reconstruction quality\nquality = analyze_pca_quality(data, reconstructed_data)\nprint(f\"\\nReconstruction quality:\")\nfor key, value in quality.items():\n    print(f\"  {key}: {value:.6f}\")\n\n# Test with different numbers of components\nprint(f\"\\n=== Testing Different Numbers of Components ===\")\n\nfor n_comp in [1, 2, 3]:\n    basis_vectors, eigenvalues, explained_variance, mean_vector = pca_basis(data, n_components=n_comp)\n    \n    projected = project_data_pca(data, basis_vectors, mean_vector)\n    reconstructed = reconstruct_data_pca(projected, basis_vectors, mean_vector)\n    \n    quality = analyze_pca_quality(data, reconstructed)\n    \n    print(f\"Components: {n_comp}\")\n    print(f\"  Explained variance: {np.sum(explained_variance):.3f}\")\n    print(f\"  Relative error: {quality['relative_error']:.6f}\")\n\n# Visualize PCA transformation\nprint(f\"\\n=== PCA Transformation Analysis ===\")\n\n# Original data statistics\nprint(f\"Original data:\")\nprint(f\"  Mean: {np.mean(data, axis=0)}\")\nprint(f\"  Variance: {np.var(data, axis=0)}\")\nprint(f\"  Total variance: {np.sum(np.var(data, axis=0)):.3f}\")\n\n# Projected data statistics\nprint(f\"\\nProjected data (2 components):\")\nprint(f\"  Mean: {np.mean(projected_data, axis=0)}\")\nprint(f\"  Variance: {np.var(projected_data, axis=0)}\")\nprint(f\"  Total variance: {np.sum(np.var(projected_data, axis=0)):.3f}\")\n\n# Verify orthogonality of principal components\nprint(f\"\\nOrthogonality of principal components:\")\nfor i in range(len(pca_basis_vectors)):\n    for j in range(i+1, len(pca_basis_vectors)):\n        dot_product = np.dot(pca_basis_vectors[i], pca_basis_vectors[j])\n        print(f\"  PC{i+1} · PC{j+1} = {dot_product:.2e}\")\n\n# Verify that principal components are unit vectors\nprint(f\"\\nNorm of principal components:\")\nfor i, pc in enumerate(pca_basis_vectors):\n    norm = np.linalg.norm(pc)\n    print(f\"  ||PC{i+1}|| = {norm:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Linear Independence in Neural Networks\n\n**Mathematical Foundation:**\nIn neural networks, linear independence is crucial for:\n1. **Weight Matrix Rank**: Ensures the network can learn diverse representations\n2. **Feature Learning**: Independent neurons learn different features\n3. **Gradient Flow**: Prevents vanishing/exploding gradients\n\n**Key Concepts:**\n- **Weight Matrix Conditioning**: Well-conditioned weight matrices have independent rows/columns\n- **Feature Diversity**: Independent neurons capture different aspects of the data\n- **Regularization**: Techniques like dropout promote independence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_neural_network_weights(weight_matrices, layer_names=None):\n    \"\"\"\n    Analyze linear independence in neural network weights\n    \n    Parameters:\n    weight_matrices: list of numpy arrays - weight matrices for each layer\n    layer_names: list - names of layers (optional)\n    \n    Returns:\n    dict - analysis results\n    \"\"\"\n    if layer_names is None:\n        layer_names = [f\"Layer_{i}\" for i in range(len(weight_matrices))]\n    \n    results = {}\n    \n    for i, (weights, name) in enumerate(zip(weight_matrices, layer_names)):\n        # Analyze rows (output neurons)\n        row_rank = np.linalg.matrix_rank(weights)\n        row_condition = np.linalg.cond(weights)\n        \n        # Analyze columns (input features)\n        col_rank = np.linalg.matrix_rank(weights.T)\n        col_condition = np.linalg.cond(weights.T)\n        \n        # Find independent rows and columns\n        independent_rows, row_indices, _ = select_linearly_independent_features(weights)\n        independent_cols, col_indices, _ = select_linearly_independent_features(weights.T)\n        \n        results[name] = {\n            'shape': weights.shape,\n            'row_rank': row_rank,\n            'col_rank': col_rank,\n            'row_condition': row_condition,\n            'col_condition': col_condition,\n            'independent_rows': independent_rows,\n            'independent_cols': independent_cols,\n            'row_indices': row_indices,\n            'col_indices': col_indices,\n            'row_independence_ratio': row_rank / weights.shape[0],\n            'col_independence_ratio': col_rank / weights.shape[1]\n        }\n    \n    return results\n\ndef promote_weight_independence(weight_matrix, method='orthogonal_init', strength=0.1):\n    \"\"\"\n    Promote linear independence in weight matrix\n    \n    Parameters:\n    weight_matrix: numpy array - weight matrix\n    method: str - method to promote independence\n    strength: float - strength of regularization\n    \n    Returns:\n    numpy array - modified weight matrix\n    \"\"\"\n    if method == 'orthogonal_init':\n        # Initialize with orthogonal matrix\n        U, _, Vt = np.linalg.svd(weight_matrix)\n        orthogonal_matrix = U @ Vt\n        return (1 - strength) * weight_matrix + strength * orthogonal_matrix\n    \n    elif method == 'rank_regularization':\n        # Add regularization to promote full rank\n        n_rows, n_cols = weight_matrix.shape\n        min_dim = min(n_rows, n_cols)\n        \n        # Compute singular values\n        U, S, Vt = np.linalg.svd(weight_matrix)\n        \n        # Promote larger singular values\n        S_modified = S + strength * np.ones_like(S)\n        \n        return U @ np.diag(S_modified) @ Vt\n    \n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\n# Example: Neural network weight analysis\nprint(\"\\n=== Neural Network Weight Analysis ===\")\n\n# Simulate neural network weights\nnp.random.seed(42)\nlayer_sizes = [10, 8, 6, 4]\nweight_matrices = []\n\nfor i in range(len(layer_sizes) - 1):\n    # Create weight matrix with some dependencies\n    weights = np.random.randn(layer_sizes[i+1], layer_sizes[i])\n    \n    # Add some linear dependencies\n    if i > 0:\n        weights[0, :] = 0.5 * weights[1, :] + 0.3 * weights[2, :]\n    \n    weight_matrices.append(weights)\n\nlayer_names = [f\"Hidden_{i+1}\" for i in range(len(weight_matrices)-1)] + [\"Output\"]\n\n# Analyze weights\nanalysis = analyze_neural_network_weights(weight_matrices, layer_names)\n\nprint(\"Weight matrix analysis:\")\nfor layer_name, layer_analysis in analysis.items():\n    print(f\"\\n{layer_name}:\")\n    print(f\"  Shape: {layer_analysis['shape']}\")\n    print(f\"  Row rank: {layer_analysis['row_rank']}/{layer_analysis['shape'][0]} ({layer_analysis['row_independence_ratio']:.2f})\")\n    print(f\"  Column rank: {layer_analysis['col_rank']}/{layer_analysis['shape'][1]} ({layer_analysis['col_independence_ratio']:.2f})\")\n    print(f\"  Row condition number: {layer_analysis['row_condition']:.2e}\")\n    print(f\"  Column condition number: {layer_analysis['col_condition']:.2e}\")\n\n# Promote independence in problematic layers\nprint(f\"\\n=== Promoting Weight Independence ===\")\n\nfor i, (weights, layer_name) in enumerate(zip(weight_matrices, layer_names)):\n    if analysis[layer_name]['row_independence_ratio'] < 0.9:\n        print(f\"Promoting independence in {layer_name}...\")\n        \n        original_rank = analysis[layer_name]['row_rank']\n        modified_weights = promote_weight_independence(weights, method='orthogonal_init', strength=0.1)\n        \n        # Analyze modified weights\n        modified_analysis = analyze_neural_network_weights([modified_weights], [layer_name])\n        modified_rank = modified_analysis[layer_name]['row_rank']\n        \n        print(f\"  Original rank: {original_rank}\")\n        print(f\"  Modified rank: {modified_rank}\")\n        print(f\"  Improvement: {modified_rank - original_rank}\")\n        \n        weight_matrices[i] = modified_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\nThe following exercises will help you master the concepts of linear independence, basis, and their applications in machine learning. Each exercise builds upon the previous ones and includes both theoretical understanding and practical implementation.\n\n### Exercise 1: Linear Independence Testing and Analysis\n\n**Objective**: Develop a comprehensive understanding of linear independence testing and its geometric interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test linear independence of different vector sets and analyze their properties\nvectors_set1 = [\n    np.array([1, 2, 3]),\n    np.array([4, 5, 6]),\n    np.array([7, 8, 9])\n]\n\nvectors_set2 = [\n    np.array([1, 0, 0]),\n    np.array([0, 1, 0]),\n    np.array([1, 1, 0])\n]\n\nvectors_set3 = [\n    np.array([1, 1, 1]),\n    np.array([1, -1, 0]),\n    np.array([0, 1, -1])\n]\n\n# Your tasks:\n# 1. Test linear independence of all three sets using multiple methods\n# 2. Find bases for each set and determine their dimensions\n# 3. Analyze the geometric properties of each set\n# 4. Compute the condition number of each set's matrix\n# 5. Visualize the vectors in 3D space (if possible)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Advanced Change of Basis Transformations\n\n**Objective**: Master change of basis transformations and understand their geometric interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform comprehensive change of basis analysis\nold_basis = [np.array([1, 0]), np.array([0, 1])]  # Standard basis\nnew_basis = [np.array([2, 1]), np.array([1, 2])]  # New basis\nvector = np.array([5, 3])\n\n# Your tasks:\n# 1. Find coordinates of vector in both bases\n# 2. Compute the change of basis matrix P and verify its properties\n# 3. Test the transformation with multiple vectors\n# 4. Analyze the geometric interpretation of the transformation\n# 5. Compute the determinant and condition number of P\n# 6. Verify that P^(-1) @ P = I and P @ P^(-1) = I\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Gram-Schmidt Process with Error Analysis\n\n**Objective**: Implement and analyze the Gram-Schmidt process with comprehensive error checking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Gram-Schmidt to various vector sets with detailed analysis\nvectors = [\n    np.array([1, 1, 1]),\n    np.array([1, 0, 1]),\n    np.array([0, 1, 1])\n]\n\n# Your tasks:\n# 1. Apply Gram-Schmidt orthogonalization with detailed step-by-step output\n# 2. Verify orthogonality of the resulting vectors\n# 3. Check if the result forms a basis for the original space\n# 4. Analyze numerical stability and error accumulation\n# 5. Compare with different normalization strategies\n# 6. Test with nearly dependent vectors to check robustness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4: Feature Selection and Dimensionality Reduction\n\n**Objective**: Apply linear independence concepts to real-world feature selection problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a realistic dataset with known dependencies\nnp.random.seed(42)\nn_samples = 500\nn_features = 10\n\n# Generate base features\nbase_features = np.random.randn(n_samples, 4)\n\n# Create derived features with various dependencies\nderived_features = np.column_stack([\n    base_features[:, 0] + 0.1 * np.random.randn(n_samples),  # Nearly dependent\n    2 * base_features[:, 1] - base_features[:, 2],            # Linear combination\n    base_features[:, 0] * base_features[:, 1],                # Non-linear\n    base_features[:, 2] ** 2,                                 # Quadratic\n    base_features[:, 0] + base_features[:, 1] + base_features[:, 2],  # Sum\n    base_features[:, 3] * 0.5                                 # Scaled copy\n])\n\n# Combine all features\nall_features = np.column_stack([base_features, derived_features])\nfeature_names = ['Base1', 'Base2', 'Base3', 'Base4', 'Derived1', 'Derived2', \n                'Derived3', 'Derived4', 'Derived5', 'Derived6']\n\n# Your tasks:\n# 1. Analyze the rank and condition number of the feature matrix\n# 2. Identify linear dependencies using correlation analysis\n# 3. Apply different feature selection methods (QR, SVD, Gaussian)\n# 4. Compare the results and explain differences\n# 5. Verify that selected features preserve the original space\n# 6. Analyze the impact on a simple regression model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5: PCA Implementation and Analysis\n\n**Objective**: Implement PCA from scratch and analyze its properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dataset with known structure for PCA analysis\nnp.random.seed(123)\nn_samples = 300\n\n# Generate data with known principal components\ntrue_pc1 = np.array([0.8, 0.6, 0.0])\ntrue_pc2 = np.array([0.0, 0.0, 1.0])\ntrue_pc3 = np.array([-0.6, 0.8, 0.0])\n\n# Create data along these directions with different variances\ndata = np.column_stack([\n    3.0 * np.random.randn(n_samples) * true_pc1,\n    1.5 * np.random.randn(n_samples) * true_pc2,\n    0.5 * np.random.randn(n_samples) * true_pc3\n])\n\n# Add some noise\ndata += 0.1 * np.random.randn(n_samples, 3)\n\n# Your tasks:\n# 1. Implement PCA from scratch (without using sklearn)\n# 2. Compare your implementation with the built-in functions\n# 3. Analyze the eigenvalues and explained variance ratios\n# 4. Project data onto different numbers of components\n# 5. Reconstruct data and analyze reconstruction quality\n# 6. Visualize the principal components and data projections\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 6: Neural Network Weight Analysis\n\n**Objective**: Analyze and improve linear independence in neural network weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate a neural network with known weight dependencies\nnp.random.seed(456)\nlayer_sizes = [20, 15, 10, 5]\nweight_matrices = []\n\nfor i in range(len(layer_sizes) - 1):\n    weights = np.random.randn(layer_sizes[i+1], layer_sizes[i])\n    \n    # Add some linear dependencies\n    if i > 0:\n        # Make some neurons nearly dependent\n        weights[0, :] = 0.7 * weights[1, :] + 0.3 * weights[2, :] + 0.1 * np.random.randn(layer_sizes[i])\n    \n    weight_matrices.append(weights)\n\nlayer_names = [f\"Hidden_{i+1}\" for i in range(len(weight_matrices)-1)] + [\"Output\"]\n\n# Your tasks:\n# 1. Analyze the rank and condition number of each weight matrix\n# 2. Identify dependent neurons in each layer\n# 3. Apply techniques to promote weight independence\n# 4. Compare different regularization methods\n# 5. Analyze the impact on network capacity\n# 6. Test the modified network on a simple classification task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solutions\n\n### Solution 1: Linear Independence Testing and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comprehensive_linear_independence_analysis(vector_sets, set_names):\n    \"\"\"\n    Comprehensive analysis of linear independence for multiple vector sets\n    \"\"\"\n    results = {}\n    \n    for vectors, name in zip(vector_sets, set_names):\n        print(f\"\\n=== Analysis of {name} ===\")\n        \n        # Convert to matrix\n        matrix = np.column_stack(vectors)\n        \n        # Basic properties\n        shape = matrix.shape\n        rank = np.linalg.matrix_rank(matrix)\n        condition_number = np.linalg.cond(matrix)\n        determinant = np.linalg.det(matrix) if shape[0] == shape[1] else None\n        \n        print(f\"Matrix shape: {shape}\")\n        print(f\"Rank: {rank}\")\n        print(f\"Condition number: {condition_number:.2e}\")\n        if determinant is not None:\n            print(f\"Determinant: {determinant:.6f}\")\n        \n        # Test linear independence\n        is_independent = rank == len(vectors)\n        print(f\"Linearly independent: {is_independent}\")\n        \n        # Find basis\n        if is_independent:\n            basis = vectors\n            print(\"Basis: All vectors are independent\")\n        else:\n            basis, indices, computed_rank = select_linearly_independent_features(matrix)\n            print(f\"Basis: {len(basis)} vectors (indices: {indices})\")\n        \n        # Geometric analysis\n        if shape[1] <= 3:  # Can visualize in 3D or less\n            print(\"Geometric properties:\")\n            for i, v in enumerate(vectors):\n                norm = np.linalg.norm(v)\n                print(f\"  Vector {i+1}: norm = {norm:.3f}\")\n            \n            # Check angles between vectors\n            for i in range(len(vectors)):\n                for j in range(i+1, len(vectors)):\n                    cos_angle = np.dot(vectors[i], vectors[j]) / (np.linalg.norm(vectors[i]) * np.linalg.norm(vectors[j]))\n                    angle = np.arccos(np.clip(cos_angle, -1, 1))\n                    print(f\"  Angle between v{i+1} and v{j+1}: {np.degrees(angle):.1f}°\")\n        \n        results[name] = {\n            'shape': shape,\n            'rank': rank,\n            'condition_number': condition_number,\n            'determinant': determinant,\n            'is_independent': is_independent,\n            'basis': basis if is_independent else len(basis)\n        }\n    \n    return results\n\n# Apply comprehensive analysis\nvector_sets = [vectors_set1, vectors_set2, vectors_set3]\nset_names = ['Set 1', 'Set 2', 'Set 3']\n\nanalysis_results = comprehensive_linear_independence_analysis(vector_sets, set_names)\n\n# Summary comparison\nprint(f\"\\n=== Summary Comparison ===\")\nfor name, results in analysis_results.items():\n    print(f\"{name}:\")\n    print(f\"  Rank: {results['rank']}/{results['shape'][1]}\")\n    print(f\"  Independent: {results['is_independent']}\")\n    print(f\"  Condition number: {results['condition_number']:.2e}\")\n\n# Additional geometric analysis for Set 1 (nearly dependent)\nprint(f\"\\n=== Detailed Analysis of Set 1 ===\")\nmatrix1 = np.column_stack(vectors_set1)\nprint(\"Set 1 has nearly dependent vectors (notice the pattern in coordinates)\")\n\n# Check if vectors lie on a plane\n# If three 3D vectors are linearly dependent, they lie on a plane\nif len(vectors_set1) == 3:\n    # Compute the volume of the parallelepiped\n    volume = abs(np.linalg.det(matrix1))\n    print(f\"Volume of parallelepiped: {volume:.6f}\")\n    if volume < 1e-10:\n        print(\"Vectors lie on a plane (volume ≈ 0)\")\n    else:\n        print(\"Vectors span 3D space\")\n\n# For Set 2 (standard basis with one dependent vector)\nprint(f\"\\n=== Detailed Analysis of Set 2 ===\")\nprint(\"Set 2 has the standard basis vectors plus a dependent vector\")\nprint(\"This creates a 2D subspace in 3D space\")\n\n# For Set 3 (independent vectors)\nprint(f\"\\n=== Detailed Analysis of Set 3 ===\")\nprint(\"Set 3 has three independent vectors\")\nprint(\"These vectors span the entire 3D space\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 2: Advanced Change of Basis Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comprehensive_change_of_basis_analysis(old_basis, new_basis, test_vectors):\n    \"\"\"\n    Comprehensive analysis of change of basis transformation\n    \"\"\"\n    print(\"=== Comprehensive Change of Basis Analysis ===\")\n    \n    # 1. Find coordinates in both bases\n    print(f\"\\n1. Coordinates in different bases:\")\n    for i, vector in enumerate(test_vectors):\n        coords_old = vector_in_basis(vector, old_basis)\n        coords_new = vector_in_basis(vector, new_basis)\n        \n        print(f\"Vector {vector}:\")\n        print(f\"  Old basis coords: {coords_old}\")\n        print(f\"  New basis coords: {coords_new}\")\n    \n    # 2. Compute change of basis matrix\n    P = change_of_basis_matrix(old_basis, new_basis)\n    print(f\"\\n2. Change of basis matrix P:\")\n    print(P)\n    \n    # 3. Verify properties of P\n    print(f\"\\n3. Properties of change of basis matrix:\")\n    print(f\"Shape: {P.shape}\")\n    print(f\"Determinant: {np.linalg.det(P):.6f}\")\n    print(f\"Condition number: {np.linalg.cond(P):.2e}\")\n    print(f\"P is invertible: {np.linalg.det(P) != 0}\")\n    \n    # 4. Verify P^(-1) @ P = I and P @ P^(-1) = I\n    P_inv = np.linalg.inv(P)\n    identity_error1 = np.linalg.norm(np.eye(P.shape[0]) - P_inv @ P)\n    identity_error2 = np.linalg.norm(np.eye(P.shape[0]) - P @ P_inv)\n    \n    print(f\"P^(-1) @ P = I (error: {identity_error1:.2e})\")\n    print(f\"P @ P^(-1) = I (error: {identity_error2:.2e})\")\n    \n    # 5. Test transformation with multiple vectors\n    print(f\"\\n4. Testing transformation with multiple vectors:\")\n    for i, vector in enumerate(test_vectors):\n        coords_old = vector_in_basis(vector, old_basis)\n        coords_new_direct = vector_in_basis(vector, new_basis)\n        coords_new_via_P = np.linalg.inv(P) @ coords_old\n        \n        error = np.linalg.norm(coords_new_direct - coords_new_via_P)\n        print(f\"Vector {vector}: transformation error = {error:.2e}\")\n    \n    # 6. Geometric interpretation\n    print(f\"\\n5. Geometric interpretation:\")\n    print(f\"Old basis vectors:\")\n    for i, v in enumerate(old_basis):\n        print(f\"  b{i+1} = {v}\")\n    \n    print(f\"New basis vectors:\")\n    for i, v in enumerate(new_basis):\n        print(f\"  b'{i+1} = {v}\")\n    \n    # Show how new basis vectors are expressed in old basis\n    print(f\"New basis vectors in old basis coordinates:\")\n    for i, v in enumerate(new_basis):\n        coords = vector_in_basis(v, old_basis)\n        print(f\"  [b'{i+1}]_old = {coords}\")\n    \n    return P, P_inv\n\n# Apply comprehensive analysis\ntest_vectors = [\n    np.array([5, 3]),\n    np.array([1, 0]),\n    np.array([0, 1]),\n    np.array([2, 2])\n]\n\nP, P_inv = comprehensive_change_of_basis_analysis(old_basis, new_basis, test_vectors)\n\n# Additional analysis: eigenvalues and eigenvectors of P\nprint(f\"\\n6. Eigenvalue analysis of change of basis matrix:\")\neigenvalues, eigenvectors = np.linalg.eig(P)\nprint(f\"Eigenvalues: {eigenvalues}\")\nprint(f\"Eigenvectors:\")\nfor i, eigenvector in enumerate(eigenvectors.T):\n    print(f\"  λ{i+1} = {eigenvalues[i]:.3f}: {eigenvector}\")\n\n# Check if P is orthogonal\nis_orthogonal = np.allclose(P @ P.T, np.eye(P.shape[0]))\nprint(f\"P is orthogonal: {is_orthogonal}\")\n\nif not is_orthogonal:\n    print(\"P is not orthogonal - this means the new basis is not orthonormal\")\n    print(\"The transformation involves both rotation and scaling\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 3: Gram-Schmidt Process with Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gram_schmidt_with_error_analysis(vectors, normalize=True, verbose=True):\n    \"\"\"\n    Gram-Schmidt process with comprehensive error analysis\n    \"\"\"\n    if verbose:\n        print(\"=== Gram-Schmidt Process with Error Analysis ===\")\n        print(\"Original vectors:\")\n        for i, v in enumerate(vectors):\n            print(f\"v{i+1} = {v}\")\n    \n    n_vectors = len(vectors)\n    orthogonal_basis = []\n    error_metrics = []\n    \n    for i, v in enumerate(vectors):\n        if verbose:\n            print(f\"\\nProcessing vector v{i+1} = {v}\")\n        \n        # Start with original vector\n        u = v.copy().astype(float)\n        step_errors = []\n        \n        # Subtract projections onto previous orthogonal vectors\n        for j in range(i):\n            # Compute projection coefficient\n            numerator = np.dot(v, orthogonal_basis[j])\n            denominator = np.dot(orthogonal_basis[j], orthogonal_basis[j])\n            \n            if abs(denominator) > 1e-10:\n                proj_coeff = numerator / denominator\n                proj_vector = proj_coeff * orthogonal_basis[j]\n                u = u - proj_vector\n                \n                # Compute error in this step\n                step_error = np.linalg.norm(u_old - u - proj_vector)\n                step_errors.append(step_error)\n                \n                if verbose:\n                    print(f\"  Subtract projection onto u{j+1}: {proj_coeff:.6f} * u{j+1}\")\n                    print(f\"  Step error: {step_error:.2e}\")\n            else:\n                if verbose:\n                    print(f\"  Warning: Denominator too small for projection onto u{j+1}\")\n        \n        # Check if resulting vector is non-zero\n        norm = np.linalg.norm(u)\n        if norm > 1e-10:\n            if normalize:\n                u = u / norm  # Normalize to unit length\n            orthogonal_basis.append(u)\n            if verbose:\n                print(f\"  Result: u{i+1} = {u_normalized}\")\n            \n            error_metrics.append({\n                'vector_index': i,\n                'step_errors': step_errors,\n                'final_norm': norm,\n                'normalized': normalize\n            })\n        else:\n            if verbose:\n                print(f\"  Warning: Vector {i+1} became zero after orthogonalization\")\n    \n    # Comprehensive verification\n    verification_results = verify_gram_schmidt_comprehensive(vectors, orthogonal_basis)\n    \n    if verbose:\n        print(f\"\\nVerification results:\")\n        for key, value in verification_results.items():\n            print(f\"  {key}: {value}\")\n    \n    return orthogonal_basis, error_metrics, verification_results\n\ndef verify_gram_schmidt_comprehensive(original_vectors, orthogonal_vectors):\n    \"\"\"\n    Comprehensive verification of Gram-Schmidt process\n    \"\"\"\n    results = {}\n    \n    # Check orthogonality\n    max_dot_product = 0\n    orthogonality_errors = []\n    for i in range(len(orthogonal_vectors)):\n        for j in range(i+1, len(orthogonal_vectors)):\n            dot_product = abs(np.dot(orthogonal_vectors[i], orthogonal_vectors[j]))\n            max_dot_product = max(max_dot_product, dot_product)\n            orthogonality_errors.append(dot_product)\n    \n    results['orthogonal'] = max_dot_product < 1e-10\n    \n    # Check normalization\n    norms = [np.linalg.norm(u) for u in orthogonal_vectors]\n    results['normalized'] = all(abs(norm - 1.0) < 1e-10 for norm in norms)\n    results['norm_errors'] = [abs(norm - 1.0) for norm in norms]\n    \n    # Check span preservation (simplified)\n    if len(original_vectors) == len(orthogonal_vectors):\n        results['span_preserved'] = True\n    else:\n        results['span_preserved'] = False\n    \n    # Check condition number of orthogonal basis\n    if len(orthogonal_vectors) > 0:\n        basis_matrix = np.column_stack(orthogonal_vectors)\n        results['condition_number'] = np.linalg.cond(basis_matrix)\n    else:\n        results['condition_number'] = float('inf')\n    \n    return results\n\n# Apply Gram-Schmidt with error analysis\northogonal_basis, error_metrics, verification = gram_schmidt_with_error_analysis(vectors, verbose=True)\n\n# Test with nearly dependent vectors\nprint(f\"\\n=== Testing with Nearly Dependent Vectors ===\")\nnearly_dependent_vectors = [\n    np.array([1, 0, 0]),\n    np.array([1, 1e-8, 0]),  # Nearly parallel to first vector\n    np.array([0, 1, 0])\n]\n\north_basis_nearly, error_metrics_nearly, verification_nearly = gram_schmidt_with_error_analysis(\n    nearly_dependent_vectors, verbose=True\n)\n\n# Compare different normalization strategies\nprint(f\"\\n=== Comparing Normalization Strategies ===\")\north_basis_normalized, _, verification_normalized = gram_schmidt_with_error_analysis(\n    vectors, normalize=True, verbose=False\n)\north_basis_not_normalized, _, verification_not_normalized = gram_schmidt_with_error_analysis(\n    vectors, normalize=False, verbose=False\n)\n\nprint(\"Normalized basis:\")\nfor i, v in enumerate(orth_basis_normalized):\n    norm = np.linalg.norm(v)\n    print(f\"  u{i+1}: norm = {norm:.6f}\")\n\nprint(\"Non-normalized basis:\")\nfor i, v in enumerate(orth_basis_not_normalized):\n    norm = np.linalg.norm(v)\n    print(f\"  u{i+1}: norm = {norm:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 4: Feature Selection and Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comprehensive_feature_analysis(feature_matrix, feature_names):\n    \"\"\"\n    Comprehensive feature analysis with multiple selection methods\n    \"\"\"\n    print(\"=== Comprehensive Feature Analysis ===\")\n    \n    # Basic properties\n    n_samples, n_features = feature_matrix.shape\n    print(f\"Feature matrix shape: {feature_matrix.shape}\")\n    print(f\"Number of samples: {n_samples}\")\n    print(f\"Number of features: {n_features}\")\n    \n    # Rank analysis\n    rank = np.linalg.matrix_rank(feature_matrix)\n    condition_number = np.linalg.cond(feature_matrix)\n    print(f\"Matrix rank: {rank}\")\n    print(f\"Condition number: {condition_number:.2e}\")\n    \n    # Correlation analysis\n    corr_matrix = np.corrcoef(feature_matrix.T)\n    print(f\"\\nCorrelation analysis:\")\n    \n    high_corr_pairs = []\n    for i in range(n_features):\n        for j in range(i+1, n_features):\n            corr = corr_matrix[i, j]\n            if abs(corr) > 0.9:\n                high_corr_pairs.append((i, j, corr))\n                print(f\"  High correlation: {feature_names[i]} ↔ {feature_names[j]}: {corr:.3f}\")\n    \n    # Feature selection with different methods\n    methods = ['qr', 'svd', 'gaussian']\n    selection_results = {}\n    \n    print(f\"\\nFeature selection results:\")\n    for method in methods:\n        print(f\"\\n--- Method: {method.upper()} ---\")\n        \n        independent_features, indices, computed_rank = select_linearly_independent_features(\n            feature_matrix, method=method\n        )\n        \n        print(f\"Selected features: {[feature_names[i] for i in indices]}\")\n        print(f\"Number of features: {len(indices)}\")\n        print(f\"Rank: {computed_rank}\")\n        \n        # Verify selection\n        verification = verify_feature_selection(feature_matrix, independent_features, indices)\n        print(f\"Reconstruction error: {verification['relative_error']:.2e}\")\n        print(f\"Rank preserved: {verification['rank_preserved']}\")\n        \n        selection_results[method] = {\n            'independent_features': independent_features,\n            'indices': indices,\n            'rank': computed_rank,\n            'verification': verification\n        }\n    \n    # Impact on simple regression model\n    print(f\"\\n=== Impact on Regression Model ===\")\n    \n    # Generate synthetic target variable\n    np.random.seed(42)\n    true_coefficients = np.random.randn(n_features)\n    target = feature_matrix @ true_coefficients + 0.1 * np.random.randn(n_samples)\n    \n    # Fit model with original features\n    from sklearn.linear_model import LinearRegression\n    from sklearn.metrics import r2_score\n    \n    model_original = LinearRegression()\n    model_original.fit(feature_matrix, target)\n    score_original = r2_score(target, model_original.predict(feature_matrix))\n    \n    print(f\"Original features R² score: {score_original:.4f}\")\n    \n    # Fit models with selected features\n    for method, results in selection_results.items():\n        selected_features = results['independent_features']\n        model_selected = LinearRegression()\n        model_selected.fit(selected_features, target)\n        score_selected = r2_score(target, model_selected.predict(selected_features))\n        \n        print(f\"{method.upper()} selected features R² score: {score_selected:.4f}\")\n    \n    return selection_results\n\n# Apply comprehensive analysis\nselection_results = comprehensive_feature_analysis(all_features, feature_names)\n\n# Additional analysis: feature importance\nprint(f\"\\n=== Feature Importance Analysis ===\")\n\n# Compute feature importance using correlation with target\ntarget = all_features @ np.random.randn(all_features.shape[1]) + 0.1 * np.random.randn(all_features.shape[0])\nfeature_importance = np.abs(np.corrcoef(all_features.T, target)[:-1, -1])\n\nprint(\"Feature importance (correlation with target):\")\nfor i, importance in enumerate(feature_importance):\n    print(f\"  {feature_names[i]}: {importance:.3f}\")\n\n# Compare with selected features\nfor method, results in selection_results.items():\n    selected_indices = results['indices']\n    avg_importance = np.mean([feature_importance[i] for i in selected_indices])\n    print(f\"{method.upper()} selected features average importance: {avg_importance:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 5: PCA Implementation and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pca_from_scratch(data, n_components=None, center=True):\n    \"\"\"\n    PCA implementation from scratch\n    \"\"\"\n    print(\"=== PCA Implementation from Scratch ===\")\n    \n    n_samples, n_features = data.shape\n    \n    # Step 1: Center the data\n    if center:\n        mean_vector = np.mean(data, axis=0)\n        data_centered = data - mean_vector\n        print(f\"Data centered: mean = {mean_vector}\")\n    else:\n        mean_vector = np.zeros(n_features)\n        data_centered = data\n    \n    # Step 2: Compute covariance matrix\n    cov_matrix = np.cov(data_centered.T)\n    print(f\"Covariance matrix shape: {cov_matrix.shape}\")\n    \n    # Step 3: Find eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n    \n    # Step 4: Sort by eigenvalues (descending)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues_sorted = eigenvalues[sorted_indices]\n    eigenvectors_sorted = eigenvectors[:, sorted_indices]\n    \n    # Step 5: Determine number of components\n    if n_components is None:\n        n_components = len(eigenvalues)\n    elif n_components > len(eigenvalues):\n        n_components = len(eigenvalues)\n    \n    # Extract components\n    principal_components = eigenvectors_sorted[:, :n_components]\n    eigenvalues_selected = eigenvalues_sorted[:n_components]\n    \n    # Compute explained variance\n    total_variance = np.sum(eigenvalues_selected)\n    explained_variance_ratio = eigenvalues_selected / total_variance\n    \n    print(f\"Eigenvalues: {eigenvalues_selected}\")\n    print(f\"Explained variance ratios: {explained_variance_ratio}\")\n    print(f\"Cumulative explained variance: {np.sum(explained_variance_ratio):.3f}\")\n    \n    return principal_components, eigenvalues_selected, explained_variance_ratio, mean_vector\n\ndef compare_pca_implementations(data, n_components=2):\n    \"\"\"\n    Compare our PCA implementation with built-in functions\n    \"\"\"\n    print(\"=== Comparing PCA Implementations ===\")\n    \n    # Our implementation\n    pc_ours, eigenvals_ours, var_ratio_ours, mean_ours = pca_from_scratch(data, n_components)\n    \n    # Built-in implementation\n    from sklearn.decomposition import PCA\n    pca_sklearn = PCA(n_components=n_components)\n    pca_sklearn.fit(data)\n    \n    pc_sklearn = pca_sklearn.components_.T\n    eigenvals_sklearn = pca_sklearn.explained_variance_\n    var_ratio_sklearn = pca_sklearn.explained_variance_ratio_\n    \n    print(f\"\\nComparison:\")\n    print(f\"Our eigenvalues: {eigenvals_ours}\")\n    print(f\"Sklearn eigenvalues: {eigenvals_sklearn}\")\n    \n    print(f\"Our explained variance: {var_ratio_ours}\")\n    print(f\"Sklearn explained variance: {var_ratio_sklearn}\")\n    \n    # Compare principal components (up to sign)\n    for i in range(n_components):\n        pc_diff = np.linalg.norm(pc_ours[:, i] - pc_sklearn[:, i])\n        pc_diff_alt = np.linalg.norm(pc_ours[:, i] + pc_sklearn[:, i])  # Check opposite sign\n        min_diff = min(pc_diff, pc_diff_alt)\n        print(f\"PC{i+1} difference: {min_diff:.2e}\")\n    \n    return pc_ours, eigenvals_ours, var_ratio_ours\n\n# Apply PCA analysis\npc_components, eigenvalues, explained_variance = compare_pca_implementations(data, n_components=2)\n\n# Project and reconstruct data\nprint(f\"\\n=== Data Projection and Reconstruction ===\")\n\n# Project data\nprojected_data = project_data_pca(data, pc_components, mean_vector=None)\nprint(f\"Projected data shape: {projected_data.shape}\")\n\n# Reconstruct data\nreconstructed_data = reconstruct_data_pca(projected_data, pc_components, mean_vector=None)\n\n# Analyze reconstruction quality\nquality = analyze_pca_quality(data, reconstructed_data)\nprint(f\"Reconstruction quality:\")\nfor key, value in quality.items():\n    print(f\"  {key}: {value:.6f}\")\n\n# Test with different numbers of components\nprint(f\"\\n=== Testing Different Numbers of Components ===\")\ncomponent_numbers = [1, 2, 3]\n\nfor n_comp in component_numbers:\n    pc_comp, eigenvals_comp, var_ratio_comp = compare_pca_implementations(data, n_components=n_comp)\n    \n    projected_comp = project_data_pca(data, pc_comp, mean_vector=None)\n    reconstructed_comp = reconstruct_data_pca(projected_comp, pc_comp, mean_vector=None)\n    \n    quality_comp = analyze_pca_quality(data, reconstructed_comp)\n    \n    print(f\"\\nComponents: {n_comp}\")\n    print(f\"  Explained variance: {np.sum(var_ratio_comp):.3f}\")\n    print(f\"  Relative error: {quality_comp['relative_error']:.6f}\")\n\n# Verify orthogonality and normalization\nprint(f\"\\n=== Verifying PCA Properties ===\")\n\n# Check orthogonality\nprint(\"Orthogonality of principal components:\")\nfor i in range(len(pc_components)):\n    for j in range(i+1, len(pc_components)):\n        dot_product = np.dot(pc_components[:, i], pc_components[:, j])\n        print(f\"  PC{i+1} · PC{j+1} = {dot_product:.2e}\")\n\n# Check normalization\nprint(\"Norm of principal components:\")\nfor i, pc in enumerate(pc_components.T):\n    norm = np.linalg.norm(pc)\n    print(f\"  ||PC{i+1}|| = {norm:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 6: Neural Network Weight Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comprehensive_neural_network_analysis(weight_matrices, layer_names):\n    \"\"\"\n    Comprehensive analysis of neural network weights\n    \"\"\"\n    print(\"=== Comprehensive Neural Network Weight Analysis ===\")\n    \n    analysis_results = analyze_neural_network_weights(weight_matrices, layer_names)\n    \n    # Display analysis results\n    for layer_name, layer_analysis in analysis_results.items():\n        print(f\"\\n{layer_name}:\")\n        print(f\"  Shape: {layer_analysis['shape']}\")\n        print(f\"  Row rank: {layer_analysis['row_rank']}/{layer_analysis['shape'][0]} ({layer_analysis['row_independence_ratio']:.2f})\")\n        print(f\"  Column rank: {layer_analysis['col_rank']}/{layer_analysis['shape'][1]} ({layer_analysis['col_independence_ratio']:.2f})\")\n        print(f\"  Row condition number: {layer_analysis['row_condition']:.2e}\")\n        print(f\"  Column condition number: {layer_analysis['col_condition']:.2e}\")\n\n    return analysis_results\n\ndef improve_network_weights(weight_matrices, layer_names, analysis_results):\n    \"\"\"\n    Improve neural network weights by promoting independence\n    \"\"\"\n    print(f\"\\n=== Improving Network Weights ===\")\n    \n    improved_matrices = []\n    improvements = []\n    \n    for i, (weights, layer_name) in enumerate(zip(weight_matrices, layer_names)):\n        original_analysis = analysis_results[layer_name]\n        \n        print(f\"\\nImproving {layer_name}...\")\n        print(f\"  Original row independence: {original_analysis['row_independence_ratio']:.3f}\")\n        print(f\"  Original column independence: {original_analysis['col_independence_ratio']:.3f}\")\n        \n        # Apply different improvement methods\n        methods = ['orthogonal_init', 'rank_regularization']\n        best_improvement = 0\n        best_weights = weights\n        \n        for method in methods:\n            for strength in [0.1, 0.2, 0.3]:\n                improved_weights = promote_weight_independence(weights, method=method, strength=strength)\n                \n                # Analyze improved weights\n                improved_analysis = analyze_neural_network_weights([improved_weights], [layer_name])\n                improved_metrics = improved_analysis[layer_name]\n                \n                row_improvement = improved_metrics['row_independence_ratio'] - original_analysis['row_independence_ratio']\n                col_improvement = improved_metrics['col_independence_ratio'] - original_analysis['col_independence_ratio']\n                \n                total_improvement = row_improvement + col_improvement\n                \n                if total_improvement > best_improvement:\n                    best_improvement = total_improvement\n                    best_weights = improved_weights\n                    best_method = method\n                    best_strength = strength\n        \n        if best_improvement > 0:\n            print(f\"  Best improvement: {best_improvement:.3f} (method: {best_method}, strength: {best_strength})\")\n            \n            # Analyze best improved weights\n            best_analysis = analyze_neural_network_weights([best_weights], [layer_name])\n            best_metrics = best_analysis[layer_name]\n            \n            print(f\"  Improved row independence: {best_metrics['row_independence_ratio']:.3f}\")\n            print(f\"  Improved column independence: {best_metrics['col_independence_ratio']:.3f}\")\n        else:\n            print(f\"  No improvement found\")\n            best_weights = weights\n        \n        improved_matrices.append(best_weights)\n        improvements.append(best_improvement)\n    \n    return improved_matrices, improvements\n\ndef test_network_capacity(original_matrices, improved_matrices, layer_names):\n    \"\"\"\n    Test the impact of weight improvements on network capacity\n    \"\"\"\n    print(f\"\\n=== Testing Network Capacity ===\")\n    \n    # Generate synthetic data\n    np.random.seed(42)\n    n_samples = 100\n    input_size = original_matrices[0].shape[1]\n    output_size = original_matrices[-1].shape[0]\n    \n    X = np.random.randn(n_samples, input_size)\n    y = np.random.randn(n_samples, output_size)\n    \n    def forward_pass(input_data, weight_matrices):\n        \"\"\"Simple forward pass through the network\"\"\"\n        current = input_data\n        for weights in weight_matrices:\n            current = current @ weights.T\n            # Apply ReLU activation\n            current = np.maximum(0, current)\n        return current\n    \n    # Test original network\n    original_output = forward_pass(X, original_matrices)\n    original_capacity = np.linalg.matrix_rank(original_output)\n    \n    # Test improved network\n    improved_output = forward_pass(X, improved_matrices)\n    improved_capacity = np.linalg.matrix_rank(improved_output)\n    \n    print(f\"Original network output rank: {original_capacity}\")\n    print(f\"Improved network output rank: {improved_capacity}\")\n    print(f\"Capacity improvement: {improved_capacity - original_capacity}\")\n    \n    # Test with different input sizes\n    print(f\"\\nCapacity test with different input sizes:\")\n    for n_samples_test in [50, 100, 200]:\n        X_test = np.random.randn(n_samples_test, input_size)\n        \n        original_output_test = forward_pass(X_test, original_matrices)\n        improved_output_test = forward_pass(X_test, improved_matrices)\n        \n        original_rank = np.linalg.matrix_rank(original_output_test)\n        improved_rank = np.linalg.matrix_rank(improved_output_test)\n        \n        print(f\"  {n_samples_test} samples: Original rank {original_rank}, Improved rank {improved_rank}\")\n    \n    return original_capacity, improved_capacity\n\n# Apply comprehensive analysis\nanalysis_results = comprehensive_neural_network_analysis(weight_matrices, layer_names)\n\n# Improve weights\nimproved_matrices, improvements = improve_network_weights(weight_matrices, layer_names, analysis_results)\n\n# Test network capacity\noriginal_capacity, improved_capacity = test_network_capacity(weight_matrices, improved_matrices, layer_names)\n\n# Summary\nprint(f\"\\n=== Summary ===\")\nprint(f\"Total improvements across all layers: {sum(improvements):.3f}\")\nprint(f\"Network capacity improvement: {improved_capacity - original_capacity}\")\n\n# Visualize improvements\nprint(f\"\\nImprovements by layer:\")\nfor i, (layer_name, improvement) in enumerate(zip(layer_names, improvements)):\n    print(f\"  {layer_name}: {improvement:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nIn this comprehensive chapter on linear independence and basis, we have covered:\n\n### Key Concepts\n- **Linear Independence**: Understanding when vectors are independent and how to test for it\n- **Basis and Dimension**: Finding bases for vector spaces and understanding coordinate systems\n- **Change of Basis**: Transforming between different coordinate representations\n- **Gram-Schmidt Process**: Creating orthogonal bases from independent vectors\n\n### Mathematical Foundations\n- **Determinant and Rank**: Using matrix properties to test independence\n- **Eigenvalues and Eigenvectors**: Understanding matrix structure\n- **Orthogonality**: Creating perpendicular vector sets\n- **Projection**: Decomposing vectors into orthogonal components\n\n### Machine Learning Applications\n- **Feature Selection**: Identifying and removing redundant features\n- **Dimensionality Reduction**: Using PCA to find optimal representations\n- **Neural Networks**: Ensuring weight matrices have good conditioning\n- **Data Analysis**: Understanding data structure through linear algebra\n\n### Practical Skills\n- **Python Implementation**: Writing robust algorithms for independence testing\n- **Numerical Stability**: Handling floating-point arithmetic carefully\n- **Verification**: Testing implementations with multiple methods\n- **Error Analysis**: Understanding and quantifying approximation errors\n\n### Advanced Topics\n- **QR Decomposition**: Using matrix factorizations for independence testing\n- **SVD Analysis**: Understanding data structure through singular values\n- **Condition Numbers**: Measuring numerical stability\n- **Regularization**: Promoting independence in machine learning models\n\nThe concepts and techniques learned in this chapter are fundamental to understanding linear algebra in the context of machine learning and data science. They provide the mathematical foundation for many advanced algorithms and help us design better models and understand data structure.\n\n## Next Steps\n\nIn the next chapter, we'll explore matrix decompositions, which are powerful tools for understanding matrix structure, solving systems of equations, and implementing efficient algorithms. Matrix decompositions build upon the concepts of linear independence and basis that we've developed here."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}