{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Matrix Decompositions\n\n[![Chapter](https://img.shields.io/badge/Chapter-7-blue.svg)]()\n[![Topic](https://img.shields.io/badge/Topic-Matrix_Decompositions-green.svg)]()\n[![Difficulty](https://img.shields.io/badge/Difficulty-Advanced-red.svg)]()\n\n## Introduction\n\nMatrix decompositions are fundamental tools in linear algebra that break down complex matrices into simpler, more manageable components. These decompositions reveal the underlying structure of matrices and enable efficient algorithms for solving systems of equations, understanding data patterns, and implementing machine learning algorithms.\n\n**Mathematical Foundation:**\nMatrix decompositions express a matrix A as a product of simpler matrices:\nA = B₁ × B₂ × ... × Bₖ\n\nwhere each Bᵢ has a special structure (triangular, orthogonal, diagonal, etc.) that makes certain operations computationally efficient.\n\n**Key Benefits:**\n1. **Computational Efficiency**: Decompositions enable fast algorithms for solving systems, computing inverses, and finding eigenvalues\n2. **Numerical Stability**: Some decompositions provide better numerical properties than direct methods\n3. **Structural Insight**: Decompositions reveal important properties like rank, condition number, and geometric structure\n4. **Machine Learning Applications**: Essential for dimensionality reduction, feature extraction, and model optimization\n\n**Geometric Interpretation:**\nMatrix decompositions can be viewed as coordinate transformations that reveal the \"natural\" structure of the data. For example:\n- **LU decomposition**: Breaks down a matrix into elementary row operations\n- **QR decomposition**: Orthogonalizes the columns of a matrix\n- **SVD**: Finds the principal axes of variation in the data\n- **Eigenvalue decomposition**: Diagonalizes a matrix in its eigenbasis\n\n## LU Decomposition\n\nLU decomposition factors a square matrix A into A = LU, where L is lower triangular and U is upper triangular. This decomposition is fundamental for solving systems of linear equations efficiently.\n\n### Mathematical Foundation\n\n**Definition:**\nFor a square matrix A ∈ ℝ^(n×n), the LU decomposition is:\nA = LU\n\nwhere:\n- L ∈ ℝ^(n×n) is lower triangular (Lᵢⱼ = 0 for i < j)\n- U ∈ ℝ^(n×n) is upper triangular (Uᵢⱼ = 0 for i > j)\n\n**Existence and Uniqueness:**\n- **Existence**: LU decomposition exists if and only if all leading principal minors of A are non-zero\n- **Uniqueness**: If A is invertible, the LU decomposition is unique when L has ones on the diagonal\n- **Generalization**: For any matrix, we can find A = PLU where P is a permutation matrix\n\n**Geometric Interpretation:**\nLU decomposition represents A as a sequence of elementary row operations:\n1. **L matrix**: Represents the row operations needed to eliminate entries below the diagonal\n2. **U matrix**: The resulting upper triangular form after elimination\n3. **P matrix**: Represents row exchanges needed for numerical stability\n\n**Key Properties:**\n1. **Determinant**: det(A) = det(L) × det(U) = ∏ᵢ Lᵢᵢ × ∏ᵢ Uᵢᵢ\n2. **Inverse**: A⁻¹ = U⁻¹L⁻¹ (if A is invertible)\n3. **Linear Systems**: Ax = b becomes LUx = b, solved by forward/backward substitution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nfrom scipy.linalg import lu, lu_factor, lu_solve\nimport matplotlib.pyplot as plt\n\ndef lu_decomposition_detailed(A, pivot=True):\n    \"\"\"\n    Perform detailed LU decomposition with comprehensive analysis\n    \n    Mathematical approach:\n    A = PLU where P is permutation matrix, L is lower triangular, U is upper triangular\n    \n    Parameters:\n    A: numpy array - square matrix to decompose\n    pivot: bool - whether to use partial pivoting for numerical stability\n    \n    Returns:\n    tuple - (P, L, U, analysis_results)\n    \"\"\"\n    n, m = A.shape\n    if n != m:\n        raise ValueError(\"Matrix must be square for LU decomposition\")\n    \n    # Perform LU decomposition\n    if pivot:\n        P, L, U = lu(A)\n    else:\n        # Manual LU without pivoting (for educational purposes)\n        P, L, U = lu_decomposition_manual(A)\n    \n    # Comprehensive analysis\n    analysis = analyze_lu_decomposition(A, P, L, U)\n    \n    return P, L, U, analysis\n\ndef lu_decomposition_manual(A):\n    \"\"\"\n    Manual LU decomposition without pivoting (educational implementation)\n    \n    Mathematical approach:\n    Use Gaussian elimination to construct L and U matrices\n    \"\"\"\n    n = A.shape[0]\n    A_copy = A.copy().astype(float)\n    L = np.eye(n)\n    U = np.zeros_like(A_copy)\n    \n    for k in range(n):\n        # Check if pivot is zero\n        if abs(A_copy[k, k]) < 1e-10:\n            raise ValueError(\"Zero pivot encountered - matrix may not have LU decomposition\")\n        \n        # Store diagonal element in U\n        U[k, k] = A_copy[k, k]\n        \n        # Eliminate column k\n        for i in range(k+1, n):\n            # Compute multiplier\n            multiplier = A_copy[i, k] / A_copy[k, k]\n            \n            # Store multiplier in L\n            L[i, k] = multiplier\n            \n            # Eliminate entry in A\n            for j in range(k, n):\n                A_copy[i, j] -= multiplier * A_copy[k, j]\n        \n        # Copy row k to U\n        U[k, k:] = A_copy[k, k:]\n    \n    return np.eye(n), L, U  # No permutation matrix in manual version\n\ndef analyze_lu_decomposition(A, P, L, U):\n    \"\"\"\n    Comprehensive analysis of LU decomposition\n    \n    Parameters:\n    A: numpy array - original matrix\n    P: numpy array - permutation matrix\n    L: numpy array - lower triangular matrix\n    U: numpy array - upper triangular matrix\n    \n    Returns:\n    dict - analysis results\n    \"\"\"\n    # Verify decomposition\n    A_reconstructed = P @ L @ U\n    decomposition_error = np.linalg.norm(A - A_reconstructed, 'fro')\n    \n    # Check triangular structure\n    L_upper_entries = np.triu(L, k=1)\n    U_lower_entries = np.tril(U, k=-1)\n    L_triangular_error = np.linalg.norm(L_upper_entries)\n    U_triangular_error = np.linalg.norm(U_lower_entries)\n    \n    # Check L has ones on diagonal (if applicable)\n    L_diagonal = np.diag(L)\n    L_diagonal_error = np.linalg.norm(L_diagonal - np.ones_like(L_diagonal))\n    \n    # Compute determinants\n    det_A = np.linalg.det(A)\n    det_L = np.linalg.det(L)\n    det_U = np.linalg.det(U)\n    det_P = np.linalg.det(P)\n    det_product = det_P * det_L * det_U\n    \n    # Condition numbers\n    cond_A = np.linalg.cond(A)\n    cond_L = np.linalg.cond(L)\n    cond_U = np.linalg.cond(U)\n    \n    # Growth factor (measure of numerical stability)\n    max_U_element = np.max(np.abs(U))\n    max_A_element = np.max(np.abs(A))\n    growth_factor = max_U_element / max_A_element if max_A_element > 0 else float('inf')\n    \n    return {\n        'decomposition_error': decomposition_error,\n        'L_triangular_error': L_triangular_error,\n        'U_triangular_error': U_triangular_error,\n        'L_diagonal_error': L_diagonal_error,\n        'det_A': det_A,\n        'det_L': det_L,\n        'det_U': det_U,\n        'det_P': det_P,\n        'det_product': det_product,\n        'det_preserved': abs(det_A - det_product) < 1e-10,\n        'cond_A': cond_A,\n        'cond_L': cond_L,\n        'cond_U': cond_U,\n        'growth_factor': growth_factor,\n        'numerically_stable': growth_factor < 100  # Heuristic threshold\n    }\n\ndef solve_system_lu_detailed(A, b, method='scipy'):\n    \"\"\"\n    Solve linear system Ax = b using LU decomposition with detailed analysis\n    \n    Parameters:\n    A: numpy array - coefficient matrix\n    b: numpy array - right-hand side vector\n    method: str - 'scipy' or 'manual'\n    \n    Returns:\n    tuple - (x, analysis_results)\n    \"\"\"\n    if method == 'scipy':\n        # Use scipy's optimized implementation\n        lu_factor_result = lu_factor(A)\n        x = lu_solve(lu_factor_result, b)\n        \n        # Extract L and U from lu_factor result\n        L = np.tril(lu_factor_result[0])\n        U = np.triu(lu_factor_result[0])\n        np.fill_diagonal(L, 1.0)  # Ensure L has ones on diagonal\n        \n        analysis = analyze_lu_decomposition(A, np.eye(A.shape[0]), L, U)\n        \n    else:\n        # Manual implementation\n        P, L, U, analysis = lu_decomposition_detailed(A)\n        \n        # Solve PLUx = b\n        # Step 1: Solve Py = b\n        y1 = P.T @ b\n        \n        # Step 2: Solve Ly = y1 (forward substitution)\n        y2 = np.linalg.solve(L, y1)\n        \n        # Step 3: Solve Ux = y2 (backward substitution)\n        x = np.linalg.solve(U, y2)\n    \n    # Verify solution\n    b_computed = A @ x\n    residual = np.linalg.norm(b - b_computed)\n    relative_residual = residual / np.linalg.norm(b) if np.linalg.norm(b) > 0 else residual\n    \n    analysis['solution_residual'] = residual\n    analysis['relative_residual'] = relative_residual\n    analysis['solution_accurate'] = relative_residual < 1e-10\n    \n    return x, analysis\n\ndef compare_lu_methods(A, b):\n    \"\"\"\n    Compare different LU decomposition methods\n    \n    Parameters:\n    A: numpy array - coefficient matrix\n    b: numpy array - right-hand side vector\n    \n    Returns:\n    dict - comparison results\n    \"\"\"\n    methods = ['scipy_pivot', 'manual_no_pivot']\n    results = {}\n    \n    for method in methods:\n        try:\n            if method == 'scipy_pivot':\n                x, analysis = solve_system_lu_detailed(A, b, method='scipy')\n            else:\n                x, analysis = solve_system_lu_detailed(A, b, method='manual')\n            \n            results[method] = {\n                'x': x,\n                'analysis': analysis,\n                'success': True\n            }\n            \n        except Exception as e:\n            results[method] = {\n                'error': str(e),\n                'success': False\n            }\n    \n    return results\n\n# Example: Comprehensive LU decomposition analysis\nprint(\"=== Comprehensive LU Decomposition Analysis ===\")\n\n# Test matrix\nA = np.array([[2, 1, 1], [4, -6, 0], [-2, 7, 2]], dtype=float)\nb = np.array([5, -2, 9], dtype=float)\n\nprint(f\"Matrix A:\\n{A}\")\nprint(f\"Right-hand side b: {b}\")\n\n# Perform detailed LU decomposition\nP, L, U, analysis = lu_decomposition_detailed(A)\n\nprint(f\"\\nPermutation matrix P:\\n{P}\")\nprint(f\"Lower triangular matrix L:\\n{L}\")\nprint(f\"Upper triangular matrix U:\\n{U}\")\n\n# Display analysis results\nprint(f\"\\nLU Decomposition Analysis:\")\nfor key, value in analysis.items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.6f}\")\n    else:\n        print(f\"  {key}: {value}\")\n\n# Verify decomposition\nA_reconstructed = P @ L @ U\nprint(f\"\\nVerification:\")\nprint(f\"Original A:\\n{A}\")\nprint(f\"Reconstructed A:\\n{A_reconstructed}\")\nprint(f\"Decomposition error: {analysis['decomposition_error']:.2e}\")\n\n# Solve system with detailed analysis\nx, solution_analysis = solve_system_lu_detailed(A, b)\n\nprint(f\"\\nSystem Solution:\")\nprint(f\"x = {x}\")\nprint(f\"Solution residual: {solution_analysis['solution_residual']:.2e}\")\nprint(f\"Relative residual: {solution_analysis['relative_residual']:.2e}\")\nprint(f\"Solution accurate: {solution_analysis['solution_accurate']}\")\n\n# Verify solution\nb_computed = A @ x\nprint(f\"\\nVerification:\")\nprint(f\"Original b: {b}\")\nprint(f\"Computed b: {b_computed}\")\nprint(f\"Verification error: {np.linalg.norm(b - b_computed):.2e}\")\n\n# Compare methods\nprint(f\"\\n=== Method Comparison ===\")\ncomparison = compare_lu_methods(A, b)\n\nfor method, result in comparison.items():\n    print(f\"\\n{method}:\")\n    if result['success']:\n        analysis = result['analysis']\n        print(f\"  Success: {result['success']}\")\n        print(f\"  Decomposition error: {analysis['decomposition_error']:.2e}\")\n        print(f\"  Growth factor: {analysis['growth_factor']:.2f}\")\n        print(f\"  Solution residual: {analysis['solution_residual']:.2e}\")\n    else:\n        print(f\"  Success: {result['success']}\")\n        print(f\"  Error: {result['error']}\")\n\n# Test with different matrix types\nprint(f\"\\n=== Testing Different Matrix Types ===\")\n\n# Well-conditioned matrix\nA_well = np.array([[4, 1, 0], [1, 4, 1], [0, 1, 4]], dtype=float)\nb_well = np.array([1, 2, 3], dtype=float)\n\nprint(f\"Well-conditioned matrix (condition number: {np.linalg.cond(A_well):.2f}):\")\nx_well, analysis_well = solve_system_lu_detailed(A_well, b_well)\nprint(f\"  Solution residual: {analysis_well['solution_residual']:.2e}\")\n\n# Ill-conditioned matrix\nA_ill = np.array([[1, 1], [1, 1.0001]], dtype=float)\nb_ill = np.array([2, 2.0001], dtype=float)\n\nprint(f\"Ill-conditioned matrix (condition number: {np.linalg.cond(A_ill):.2e}):\")\nx_ill, analysis_ill = solve_system_lu_detailed(A_ill, b_ill)\nprint(f\"  Solution residual: {analysis_ill['solution_residual']:.2e}\")\nprint(f\"  Growth factor: {analysis_ill['growth_factor']:.2f}\")\n\n# Singular matrix (should fail)\nA_singular = np.array([[1, 1], [1, 1]], dtype=float)\nb_singular = np.array([1, 1], dtype=float)\n\nprint(f\"Singular matrix:\")\ntry:\n    x_singular, analysis_singular = solve_system_lu_detailed(A_singular, b_singular)\n    print(f\"  Unexpected success!\")\nexcept Exception as e:\n    print(f\"  Expected failure: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solving Systems with LU Decomposition\n\n**Mathematical Foundation:**\nThe LU decomposition enables efficient solution of linear systems Ax = b through forward and backward substitution:\n\n1. **Decomposition**: A = LU\n2. **Forward Substitution**: Solve Ly = b for y\n3. **Backward Substitution**: Solve Ux = y for x\n\n**Computational Complexity:**\n- **Decomposition**: O(n³) operations (one-time cost)\n- **Forward/Backward Substitution**: O(n²) operations per right-hand side\n- **Multiple Right-hand Sides**: Very efficient after initial decomposition\n\n**Numerical Stability:**\n- **Partial Pivoting**: Exchanging rows to avoid small pivots\n- **Growth Factor**: Measure of numerical stability\n- **Condition Number**: Relationship between input and output perturbations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solve_multiple_systems_lu(A, B):\n    \"\"\"\n    Solve multiple systems AX = B using LU decomposition\n    \n    Mathematical approach:\n    After computing A = LU, solve LUx_i = b_i for each column b_i of B\n    \n    Parameters:\n    A: numpy array - coefficient matrix (n × n)\n    B: numpy array - right-hand side matrix (n × k)\n    \n    Returns:\n    numpy array - solution matrix X (n × k)\n    \"\"\"\n    n, m = A.shape\n    if n != m:\n        raise ValueError(\"Matrix A must be square\")\n    \n    k = B.shape[1] if len(B.shape) > 1 else 1\n    \n    # Perform LU decomposition once\n    lu_factor_result = lu_factor(A)\n    \n    # Solve for each right-hand side\n    if k == 1:\n        X = lu_solve(lu_factor_result, B)\n    else:\n        X = np.zeros((n, k))\n        for i in range(k):\n            X[:, i] = lu_solve(lu_factor_result, B[:, i])\n    \n    return X\n\ndef analyze_system_sensitivity(A, b, perturbation_magnitude=1e-6):\n    \"\"\"\n    Analyze sensitivity of linear system solution to perturbations\n    \n    Parameters:\n    A: numpy array - coefficient matrix\n    b: numpy array - right-hand side vector\n    perturbation_magnitude: float - magnitude of perturbations to test\n    \n    Returns:\n    dict - sensitivity analysis results\n    \"\"\"\n    # Solve original system\n    x_original, _ = solve_system_lu_detailed(A, b)\n    \n    # Test perturbations in A\n    A_perturbed = A + perturbation_magnitude * np.random.randn(*A.shape)\n    x_A_perturbed, _ = solve_system_lu_detailed(A_perturbed, b)\n    sensitivity_A = np.linalg.norm(x_original - x_A_perturbed) / perturbation_magnitude\n    \n    # Test perturbations in b\n    b_perturbed = b + perturbation_magnitude * np.random.randn(*b.shape)\n    x_b_perturbed, _ = solve_system_lu_detailed(A, b_perturbed)\n    sensitivity_b = np.linalg.norm(x_original - x_b_perturbed) / perturbation_magnitude\n    \n    # Theoretical bounds\n    cond_A = np.linalg.cond(A)\n    norm_A_inv = np.linalg.norm(np.linalg.inv(A))\n    norm_x = np.linalg.norm(x_original)\n    norm_b = np.linalg.norm(b)\n    \n    return {\n        'sensitivity_A': sensitivity_A,\n        'sensitivity_b': sensitivity_b,\n        'condition_number': cond_A,\n        'norm_A_inv': norm_A_inv,\n        'theoretical_bound_A': cond_A * norm_x / norm_A,\n        'theoretical_bound_b': cond_A / norm_A,\n        'well_conditioned': cond_A < 100\n    }\n\n# Example: Multiple systems and sensitivity analysis\nprint(\"\\n=== Multiple Systems and Sensitivity Analysis ===\")\n\n# Create multiple right-hand sides\nB = np.column_stack([\n    np.array([5, -2, 9]),\n    np.array([1, 0, 1]),\n    np.array([0, 1, 0])\n])\n\nprint(f\"Multiple right-hand sides B:\\n{B}\")\n\n# Solve all systems efficiently\nX = solve_multiple_systems_lu(A, B)\n\nprint(f\"Solution matrix X:\\n{X}\")\n\n# Verify solutions\nfor i in range(B.shape[1]):\n    b_i = B[:, i]\n    x_i = X[:, i]\n    residual = np.linalg.norm(A @ x_i - b_i)\n    print(f\"System {i+1} residual: {residual:.2e}\")\n\n# Sensitivity analysis\nsensitivity_results = analyze_system_sensitivity(A, b)\n\nprint(f\"\\nSensitivity Analysis:\")\nfor key, value in sensitivity_results.items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.6f}\")\n    else:\n        print(f\"  {key}: {value}\")\n\n# Compare with theoretical bounds\nprint(f\"\\nSensitivity Comparison:\")\nprint(f\"  Observed sensitivity to A: {sensitivity_results['sensitivity_A']:.2e}\")\nprint(f\"  Theoretical bound for A: {sensitivity_results['theoretical_bound_A']:.2e}\")\nprint(f\"  Observed sensitivity to b: {sensitivity_results['sensitivity_b']:.2e}\")\nprint(f\"  Theoretical bound for b: {sensitivity_results['theoretical_bound_b']:.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## QR Decomposition\n\nQR decomposition factors a matrix A into A = QR, where Q is orthogonal and R is upper triangular. This decomposition is fundamental for least squares problems, eigenvalue computation, and numerical stability.\n\n### Mathematical Foundation\n\n**Definition:**\nFor a matrix A ∈ ℝ^(m×n) with m ≥ n, the QR decomposition is:\nA = QR\n\nwhere:\n- Q ∈ ℝ^(m×m) is orthogonal (QᵀQ = I)\n- R ∈ ℝ^(m×n) is upper triangular (Rᵢⱼ = 0 for i > j)\n\n**Existence and Uniqueness:**\n- **Existence**: QR decomposition always exists for any matrix A\n- **Uniqueness**: If A has full column rank, the decomposition is unique when R has positive diagonal entries\n- **Reduced Form**: For m > n, we can write A = Q₁R₁ where Q₁ ∈ ℝ^(m×n) has orthonormal columns\n\n**Geometric Interpretation:**\nQR decomposition represents A as:\n1. **Q matrix**: Orthonormal basis for the column space of A\n2. **R matrix**: Coordinates of A's columns in the Q basis\n3. **Gram-Schmidt Connection**: QR decomposition is essentially Gram-Schmidt orthogonalization applied to A's columns\n\n**Key Properties:**\n1. **Orthogonality**: QᵀQ = I (Q preserves lengths and angles)\n2. **Upper Triangular**: R is upper triangular, enabling efficient back-substitution\n3. **Rank Preservation**: rank(A) = rank(R) = number of non-zero diagonal elements of R\n4. **Least Squares**: QR decomposition provides numerically stable solution to least squares problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def qr_decomposition_detailed(A, method='numpy'):\n    \"\"\"\n    Perform detailed QR decomposition with comprehensive analysis\n    \n    Mathematical approach:\n    A = QR where Q is orthogonal and R is upper triangular\n    \n    Parameters:\n    A: numpy array - matrix to decompose\n    method: str - 'numpy', 'gram_schmidt', or 'householder'\n    \n    Returns:\n    tuple - (Q, R, analysis_results)\n    \"\"\"\n    m, n = A.shape\n    \n    if method == 'numpy':\n        # Use numpy's optimized implementation\n        Q, R = np.linalg.qr(A)\n        \n    elif method == 'gram_schmidt':\n        # Manual Gram-Schmidt implementation\n        Q, R = qr_gram_schmidt(A)\n        \n    elif method == 'householder':\n        # Manual Householder implementation\n        Q, R = qr_householder(A)\n        \n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n    \n    # Comprehensive analysis\n    analysis = analyze_qr_decomposition(A, Q, R)\n    \n    return Q, R, analysis\n\ndef qr_gram_schmidt(A):\n    \"\"\"\n    QR decomposition using Gram-Schmidt orthogonalization\n    \n    Mathematical approach:\n    Apply Gram-Schmidt to columns of A to obtain Q, then compute R = Q^T A\n    \"\"\"\n    m, n = A.shape\n    Q = np.zeros((m, n))\n    R = np.zeros((n, n))\n    \n    for j in range(n):\n        # Start with column j of A\n        v = A[:, j].copy()\n        \n        # Subtract projections onto previous orthogonal vectors\n        for i in range(j):\n            R[i, j] = np.dot(Q[:, i], A[:, j])\n            v = v - R[i, j] * Q[:, i]\n        \n        # Normalize to get next column of Q\n        R[j, j] = np.linalg.norm(v)\n        if R[j, j] > 1e-10:\n            Q[:, j] = v / R[j, j]\n        else:\n            # Handle zero or near-zero vectors\n            Q[:, j] = np.zeros(m)\n            R[j, j] = 0\n    \n    return Q, R\n\ndef qr_householder(A):\n    \"\"\"\n    QR decomposition using Householder reflections\n    \n    Mathematical approach:\n    Use Householder matrices to zero out subdiagonal elements\n    \"\"\"\n    m, n = A.shape\n    A_copy = A.copy().astype(float)\n    Q = np.eye(m)\n    \n    for j in range(min(m-1, n)):\n        # Extract column j from row j onwards\n        x = A_copy[j:, j]\n        \n        # Compute Householder vector\n        norm_x = np.linalg.norm(x)\n        if norm_x > 1e-10:\n            # Choose sign to avoid cancellation\n            if x[0] >= 0:\n                x[0] += norm_x\n            else:\n                x[0] -= norm_x\n            \n            # Normalize Householder vector\n            u = x / np.linalg.norm(x)\n            \n            # Apply Householder reflection to A\n            A_copy[j:, j:] = A_copy[j:, j:] - 2 * np.outer(u, u.T @ A_copy[j:, j:])\n            \n            # Update Q matrix\n            Q[j:, :] = Q[j:, :] - 2 * np.outer(u, u.T @ Q[j:, :])\n    \n    R = A_copy\n    return Q.T, R\n\ndef analyze_qr_decomposition(A, Q, R):\n    \"\"\"\n    Comprehensive analysis of QR decomposition\n    \n    Parameters:\n    A: numpy array - original matrix\n    Q: numpy array - orthogonal matrix\n    R: numpy array - upper triangular matrix\n    \n    Returns:\n    dict - analysis results\n    \"\"\"\n    # Verify decomposition\n    A_reconstructed = Q @ R\n    decomposition_error = np.linalg.norm(A - A_reconstructed, 'fro')\n    \n    # Check orthogonality of Q\n    Q_orthogonal_error = np.linalg.norm(Q.T @ Q - np.eye(Q.shape[1]), 'fro')\n    \n    # Check upper triangular structure of R\n    R_lower_entries = np.tril(R, k=-1)\n    R_triangular_error = np.linalg.norm(R_lower_entries)\n    \n    # Check diagonal positivity of R (for uniqueness)\n    R_diagonal = np.diag(R)\n    negative_diagonal_count = np.sum(R_diagonal < -1e-10)\n    \n    # Rank analysis\n    rank_A = np.linalg.matrix_rank(A)\n    rank_R = np.sum(np.abs(R_diagonal) > 1e-10)\n    \n    # Condition numbers\n    cond_A = np.linalg.cond(A)\n    cond_R = np.linalg.cond(R)\n    \n    # Norm preservation\n    norm_A = np.linalg.norm(A, 'fro')\n    norm_QR = np.linalg.norm(Q @ R, 'fro')\n    norm_preservation_error = abs(norm_A - norm_QR)\n    \n    return {\n        'decomposition_error': decomposition_error,\n        'Q_orthogonal_error': Q_orthogonal_error,\n        'R_triangular_error': R_triangular_error,\n        'negative_diagonal_count': negative_diagonal_count,\n        'rank_A': rank_A,\n        'rank_R': rank_R,\n        'rank_preserved': rank_A == rank_R,\n        'cond_A': cond_A,\n        'cond_R': cond_R,\n        'norm_preservation_error': norm_preservation_error,\n        'numerically_stable': decomposition_error < 1e-10 and Q_orthogonal_error < 1e-10\n    }\n\ndef solve_least_squares_qr_detailed(A, b, method='numpy'):\n    \"\"\"\n    Solve least squares problem using QR decomposition with detailed analysis\n    \n    Mathematical approach:\n    min ||Ax - b||₂ is solved by QR decomposition:\n    1. A = QR\n    2. min ||QRx - b||₂ = min ||Rx - Q^T b||₂\n    3. Solve Rx = Q^T b by back-substitution\n    \n    Parameters:\n    A: numpy array - coefficient matrix\n    b: numpy array - right-hand side vector\n    method: str - QR decomposition method\n    \n    Returns:\n    tuple - (x, analysis_results)\n    \"\"\"\n    m, n = A.shape\n    \n    # Perform QR decomposition\n    Q, R, qr_analysis = qr_decomposition_detailed(A, method=method)\n    \n    # Solve least squares problem\n    # Step 1: Compute Q^T b\n    Qtb = Q.T @ b\n    \n    # Step 2: Solve Rx = Q^T b\n    # Use only the first n rows of R and Q^T b\n    R_n = R[:n, :n]\n    Qtb_n = Qtb[:n]\n    \n    try:\n        x = np.linalg.solve(R_n, Qtb_n)\n        solution_exists = True\n    except np.linalg.LinAlgError:\n        # Handle rank-deficient case\n        x = np.linalg.lstsq(R_n, Qtb_n, rcond=None)[0]\n        solution_exists = False\n    \n    # Analysis\n    residual = A @ x - b\n    residual_norm = np.linalg.norm(residual)\n    relative_residual = residual_norm / np.linalg.norm(b) if np.linalg.norm(b) > 0 else residual_norm\n    \n    # Check if solution is unique\n    rank_A = qr_analysis['rank_A']\n    solution_unique = rank_A == n\n    \n    # Compare with numpy's least squares\n    x_numpy = np.linalg.lstsq(A, b, rcond=None)[0]\n    numpy_residual = A @ x_numpy - b\n    numpy_residual_norm = np.linalg.norm(numpy_residual)\n    \n    analysis = {\n        'x': x,\n        'residual_norm': residual_norm,\n        'relative_residual': relative_residual,\n        'solution_exists': solution_exists,\n        'solution_unique': solution_unique,\n        'rank_A': rank_A,\n        'numpy_solution': x_numpy,\n        'numpy_residual_norm': numpy_residual_norm,\n        'solution_difference': np.linalg.norm(x - x_numpy),\n        'qr_analysis': qr_analysis\n    }\n    \n    return x, analysis\n\ndef compare_qr_methods(A, b):\n    \"\"\"\n    Compare different QR decomposition methods for least squares\n    \n    Parameters:\n    A: numpy array - coefficient matrix\n    b: numpy array - right-hand side vector\n    \n    Returns:\n    dict - comparison results\n    \"\"\"\n    methods = ['numpy', 'gram_schmidt', 'householder']\n    results = {}\n    \n    for method in methods:\n        try:\n            x, analysis = solve_least_squares_qr_detailed(A, b, method=method)\n            results[method] = {\n                'x': x,\n                'analysis': analysis,\n                'success': True\n            }\n        except Exception as e:\n            results[method] = {\n                'error': str(e),\n                'success': False\n            }\n    \n    return results\n\n# Example: Comprehensive QR decomposition analysis\nprint(\"\\n=== Comprehensive QR Decomposition Analysis ===\")\n\n# Test matrix (overdetermined system)\nA = np.array([[1, 1], [1, 2], [1, 3], [1, 4]], dtype=float)\nb = np.array([2, 3, 4, 5], dtype=float)\n\nprint(f\"Matrix A:\\n{A}\")\nprint(f\"Right-hand side b: {b}\")\nprint(f\"System shape: {A.shape}\")\n\n# Perform detailed QR decomposition\nQ, R, analysis = qr_decomposition_detailed(A)\n\nprint(f\"\\nOrthogonal matrix Q:\\n{Q}\")\nprint(f\"Upper triangular matrix R:\\n{R}\")\n\n# Display analysis results\nprint(f\"\\nQR Decomposition Analysis:\")\nfor key, value in analysis.items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.6f}\")\n    else:\n        print(f\"  {key}: {value}\")\n\n# Verify decomposition\nA_reconstructed = Q @ R\nprint(f\"\\nVerification:\")\nprint(f\"Original A:\\n{A}\")\nprint(f\"Reconstructed A:\\n{A_reconstructed}\")\nprint(f\"Decomposition error: {analysis['decomposition_error']:.2e}\")\n\n# Verify orthogonality\nQ_orthogonal = Q.T @ Q\nprint(f\"\\nOrthogonality check (Q^T @ Q):\")\nprint(Q_orthogonal)\nprint(f\"Orthogonality error: {analysis['Q_orthogonal_error']:.2e}\")\n\n# Solve least squares with detailed analysis\nx, ls_analysis = solve_least_squares_qr_detailed(A, b)\n\nprint(f\"\\nLeast Squares Solution:\")\nprint(f\"x = {x}\")\nprint(f\"Residual norm: {ls_analysis['residual_norm']:.6f}\")\nprint(f\"Relative residual: {ls_analysis['relative_residual']:.6f}\")\nprint(f\"Solution unique: {ls_analysis['solution_unique']}\")\n\n# Compare with numpy\nprint(f\"\\nComparison with numpy:\")\nprint(f\"QR solution: {x}\")\nprint(f\"Numpy solution: {ls_analysis['numpy_solution']}\")\nprint(f\"Solution difference: {ls_analysis['solution_difference']:.2e}\")\n\n# Compare methods\nprint(f\"\\n=== Method Comparison ===\")\ncomparison = compare_qr_methods(A, b)\n\nfor method, result in comparison.items():\n    print(f\"\\n{method}:\")\n    if result['success']:\n        analysis = result['analysis']\n        print(f\"  Success: {result['success']}\")\n        print(f\"  Decomposition error: {analysis['qr_analysis']['decomposition_error']:.2e}\")\n        print(f\"  Residual norm: {analysis['residual_norm']:.6f}\")\n        print(f\"  Solution difference from numpy: {analysis['solution_difference']:.2e}\")\n    else:\n        print(f\"  Success: {result['success']}\")\n        print(f\"  Error: {result['error']}\")\n\n# Test with different matrix types\nprint(f\"\\n=== Testing Different Matrix Types ===\")\n\n# Well-conditioned overdetermined system\nA_well = np.array([[1, 0], [0, 1], [1, 1]], dtype=float)\nb_well = np.array([1, 2, 3], dtype=float)\n\nprint(f\"Well-conditioned system:\")\nx_well, analysis_well = solve_least_squares_qr_detailed(A_well, b_well)\nprint(f\"  Residual norm: {analysis_well['residual_norm']:.6f}\")\nprint(f\"  Solution unique: {analysis_well['solution_unique']}\")\n\n# Ill-conditioned system\nA_ill = np.array([[1, 1], [1, 1.0001], [1, 1.0002]], dtype=float)\nb_ill = np.array([2, 2.0001, 2.0002], dtype=float)\n\nprint(f\"Ill-conditioned system:\")\nx_ill, analysis_ill = solve_least_squares_qr_detailed(A_ill, b_ill)\nprint(f\"  Residual norm: {analysis_ill['residual_norm']:.6f}\")\nprint(f\"  Condition number: {analysis_ill['qr_analysis']['cond_A']:.2e}\")\n\n# Rank-deficient system\nA_rank_def = np.array([[1, 1], [1, 1], [1, 1]], dtype=float)\nb_rank_def = np.array([1, 2, 3], dtype=float)\n\nprint(f\"Rank-deficient system:\")\nx_rank_def, analysis_rank_def = solve_least_squares_qr_detailed(A_rank_def, b_rank_def)\nprint(f\"  Residual norm: {analysis_rank_def['residual_norm']:.6f}\")\nprint(f\"  Rank: {analysis_rank_def['rank_A']}\")\nprint(f\"  Solution unique: {analysis_rank_def['solution_unique']}\")\n\n# Test with square system (should give exact solution)\nprint(f\"\\n=== Square System Test ===\")\nA_square = np.array([[2, 1], [1, 3]], dtype=float)\nb_square = np.array([5, 6], dtype=float)\n\nprint(f\"Square system:\")\nx_square, analysis_square = solve_least_squares_qr_detailed(A_square, b_square)\nprint(f\"  Solution: {x_square}\")\nprint(f\"  Residual norm: {analysis_square['residual_norm']:.6f}\")\nprint(f\"  Should be exact: {analysis_square['residual_norm'] < 1e-10}\")\n\n# Verify exact solution\nx_exact = np.linalg.solve(A_square, b_square)\nprint(f\"  Exact solution: {x_exact}\")\nprint(f\"  Difference: {np.linalg.norm(x_square - x_exact):.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Least Squares Applications\n\n**Mathematical Foundation:**\nQR decomposition provides a numerically stable method for solving least squares problems:\n\nmin ||Ax - b||₂\n\nThe solution is obtained by:\n1. **Decomposition**: A = QR\n2. **Transformation**: ||Ax - b||₂ = ||QRx - b||₂ = ||Rx - Qᵀb||₂\n3. **Solution**: Solve Rx = Qᵀb by back-substitution\n\n**Key Advantages:**\n1. **Numerical Stability**: QR decomposition is more stable than normal equations\n2. **Rank Deficiency**: Handles rank-deficient matrices gracefully\n3. **Multiple Right-hand Sides**: Efficient for solving multiple least squares problems\n4. **Conditioning**: Preserves conditioning better than other methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solve_multiple_least_squares_qr(A, B):\n    \"\"\"\n    Solve multiple least squares problems AX ≈ B using QR decomposition\n    \n    Mathematical approach:\n    After computing A = QR, solve Rx_i = Q^T b_i for each column b_i of B\n    \n    Parameters:\n    A: numpy array - coefficient matrix (m × n)\n    B: numpy array - right-hand side matrix (m × k)\n    \n    Returns:\n    numpy array - solution matrix X (n × k)\n    \"\"\"\n    m, n = A.shape\n    k = B.shape[1] if len(B.shape) > 1 else 1\n    \n    # Perform QR decomposition once\n    Q, R = np.linalg.qr(A)\n    \n    # Solve for each right-hand side\n    if k == 1:\n        Qtb = Q.T @ B\n        X = np.linalg.solve(R[:n, :n], Qtb[:n])\n    else:\n        Qtb = Q.T @ B\n        X = np.zeros((n, k))\n        for i in range(k):\n            X[:, i] = np.linalg.solve(R[:n, :n], Qtb[:n, i])\n    \n    return X\n\ndef analyze_least_squares_sensitivity(A, b, perturbation_magnitude=1e-6):\n    \"\"\"\n    Analyze sensitivity of least squares solution to perturbations\n    \n    Parameters:\n    A: numpy array - coefficient matrix\n    b: numpy array - right-hand side vector\n    perturbation_magnitude: float - magnitude of perturbations to test\n    \n    Returns:\n    dict - sensitivity analysis results\n    \"\"\"\n    # Solve original problem\n    x_original, _ = solve_least_squares_qr_detailed(A, b)\n    \n    # Test perturbations in A\n    A_perturbed = A + perturbation_magnitude * np.random.randn(*A.shape)\n    x_A_perturbed, _ = solve_least_squares_qr_detailed(A_perturbed, b)\n    sensitivity_A = np.linalg.norm(x_original - x_A_perturbed) / perturbation_magnitude\n    \n    # Test perturbations in b\n    b_perturbed = b + perturbation_magnitude * np.random.randn(*b.shape)\n    x_b_perturbed, _ = solve_least_squares_qr_detailed(A, b_perturbed)\n    sensitivity_b = np.linalg.norm(x_original - x_b_perturbed) / perturbation_magnitude\n    \n    # Theoretical bounds\n    cond_A = np.linalg.cond(A)\n    norm_A_pinv = np.linalg.norm(np.linalg.pinv(A))\n    norm_x = np.linalg.norm(x_original)\n    norm_b = np.linalg.norm(b)\n    \n    return {\n        'sensitivity_A': sensitivity_A,\n        'sensitivity_b': sensitivity_b,\n        'condition_number': cond_A,\n        'norm_A_pinv': norm_A_pinv,\n        'theoretical_bound_A': cond_A * norm_x / norm_A,\n        'theoretical_bound_b': cond_A / norm_A,\n        'well_conditioned': cond_A < 100\n    }\n\n# Example: Multiple least squares and sensitivity analysis\nprint(\"\\n=== Multiple Least Squares and Sensitivity Analysis ===\")\n\n# Create multiple right-hand sides\nB = np.column_stack([\n    np.array([2, 3, 4, 5]),\n    np.array([1, 2, 3, 4]),\n    np.array([0, 1, 2, 3])\n])\n\nprint(f\"Multiple right-hand sides B:\\n{B}\")\n\n# Solve all least squares problems efficiently\nX = solve_multiple_least_squares_qr(A, B)\n\nprint(f\"Solution matrix X:\\n{X}\")\n\n# Verify solutions\nfor i in range(B.shape[1]):\n    b_i = B[:, i]\n    x_i = X[:, i]\n    residual = np.linalg.norm(A @ x_i - b_i)\n    print(f\"System {i+1} residual: {residual:.6f}\")\n\n# Sensitivity analysis\nsensitivity_results = analyze_least_squares_sensitivity(A, b)\n\nprint(f\"\\nSensitivity Analysis:\")\nfor key, value in sensitivity_results.items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.6f}\")\n    else:\n        print(f\"  {key}: {value}\")\n\n# Compare with theoretical bounds\nprint(f\"\\nSensitivity Comparison:\")\nprint(f\"  Observed sensitivity to A: {sensitivity_results['sensitivity_A']:.2e}\")\nprint(f\"  Theoretical bound for A: {sensitivity_results['theoretical_bound_A']:.2e}\")\nprint(f\"  Observed sensitivity to b: {sensitivity_results['sensitivity_b']:.2e}\")\nprint(f\"  Theoretical bound for b: {sensitivity_results['theoretical_bound_b']:.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Singular Value Decomposition (SVD)\n\nSVD decomposes a matrix A into A = UΣVᵀ, where U and V are orthogonal and Σ is diagonal.\n\n### Mathematical Definition\nA = UΣVᵀ where:\n- U is orthogonal (left singular vectors)\n- Σ is diagonal (singular values)\n- V is orthogonal (right singular vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def svd_decomposition_example():\n    \"\"\"Demonstrate SVD decomposition\"\"\"\n    A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    print(\"Matrix A:\")\n    print(A)\n    \n    # Perform SVD\n    U, S, Vt = np.linalg.svd(A)\n    \n    print(\"\\nLeft singular vectors U:\")\n    print(U)\n    print(\"\\nSingular values S:\")\n    print(S)\n    print(\"\\nRight singular vectors V^T:\")\n    print(Vt)\n    \n    # Verify orthogonality\n    U_orthogonal = U.T @ U\n    V_orthogonal = Vt @ Vt.T\n    print(f\"\\nU orthogonality error: {np.linalg.norm(U_orthogonal - np.eye(U.shape[0])):.2e}\")\n    print(f\"V orthogonality error: {np.linalg.norm(V_orthogonal - np.eye(Vt.shape[1])):.2e}\")\n    \n    # Reconstruct matrix\n    Sigma = np.zeros_like(A, dtype=float)\n    Sigma[:len(S), :len(S)] = np.diag(S)\n    A_reconstructed = U @ Sigma @ Vt\n    \n    print(\"\\nA reconstructed (U @ Σ @ V^T):\")\n    print(A_reconstructed)\n    print(f\"Decomposition error: {np.linalg.norm(A - A_reconstructed):.2e}\")\n    \n    return U, S, Vt\n\nU, S, Vt = svd_decomposition_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Low-Rank Approximation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def low_rank_approximation(A, k):\n    \"\"\"Compute rank-k approximation of matrix A using SVD\"\"\"\n    U, S, Vt = np.linalg.svd(A)\n    \n    # Keep only k singular values\n    U_k = U[:, :k]\n    S_k = S[:k]\n    Vt_k = Vt[:k, :]\n    \n    # Reconstruct rank-k approximation\n    A_k = U_k @ np.diag(S_k) @ Vt_k\n    \n    return A_k, U_k, S_k, Vt_k\n\n# Example: Image compression simulation\nnp.random.seed(42)\n# Create a \"low-rank\" matrix (simulating an image)\nn, m = 50, 50\nA_original = np.random.randn(n, m)\n# Make it approximately low-rank by adding some structure\nA_original = A_original @ A_original.T / m\n\nprint(f\"Original matrix shape: {A_original.shape}\")\nprint(f\"Original rank: {np.linalg.matrix_rank(A_original)}\")\n\n# Compute different rank approximations\nranks = [5, 10, 20]\nfor k in ranks:\n    A_k, U_k, S_k, Vt_k = low_rank_approximation(A_original, k)\n    \n    # Calculate compression ratio\n    original_size = A_original.size\n    compressed_size = U_k.size + S_k.size + Vt_k.size\n    compression_ratio = original_size / compressed_size\n    \n    # Calculate reconstruction error\n    error = np.linalg.norm(A_original - A_k) / np.linalg.norm(A_original)\n    \n    print(f\"\\nRank {k} approximation:\")\n    print(f\"  Compression ratio: {compression_ratio:.2f}x\")\n    print(f\"  Relative error: {error:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cholesky Decomposition\n\nCholesky decomposition factors a positive definite matrix A into A = LLᵀ, where L is lower triangular.\n\n### Mathematical Definition\nA = LLᵀ where:\n- L is lower triangular\n- A is symmetric positive definite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cholesky_decomposition_example():\n    \"\"\"Demonstrate Cholesky decomposition\"\"\"\n    # Create a positive definite matrix\n    A = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]])\n    print(\"Positive definite matrix A:\")\n    print(A)\n    \n    # Verify positive definiteness\n    eigenvals = np.linalg.eigvals(A)\n    print(f\"Eigenvalues: {eigenvals}\")\n    print(f\"Is positive definite: {np.all(eigenvals > 0)}\")\n    \n    # Perform Cholesky decomposition\n    L = np.linalg.cholesky(A)\n    \n    print(\"\\nLower triangular matrix L:\")\n    print(L)\n    \n    # Verify decomposition\n    A_reconstructed = L @ L.T\n    print(\"\\nA reconstructed (L @ L^T):\")\n    print(A_reconstructed)\n    print(f\"Decomposition error: {np.linalg.norm(A - A_reconstructed):.2e}\")\n    \n    return L\n\nL = cholesky_decomposition_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solving Systems with Cholesky"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solve_with_cholesky(A, b):\n    \"\"\"Solve Ax = b using Cholesky decomposition\"\"\"\n    # Cholesky decomposition\n    L = np.linalg.cholesky(A)\n    \n    # Solve L y = b (forward substitution)\n    y = np.linalg.solve(L, b)\n    \n    # Solve L^T x = y (backward substitution)\n    x = np.linalg.solve(L.T, y)\n    \n    return x\n\n# Example: Solve system with positive definite matrix\nA = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]])\nb = np.array([1, 2, 3])\n\nprint(\"System Ax = b (A positive definite):\")\nprint(f\"A:\\n{A}\")\nprint(f\"b: {b}\")\n\nx = solve_with_cholesky(A, b)\nprint(f\"\\nSolution x: {x}\")\n\n# Verify solution\nb_computed = A @ x\nprint(f\"Computed b: {b_computed}\")\nprint(f\"Verification error: {np.linalg.norm(b - b_computed):.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigenvalue Decomposition\n\nEigenvalue decomposition factors a diagonalizable matrix A into A = PDP⁻¹, where P contains eigenvectors and D is diagonal.\n\n### Mathematical Definition\nA = PDP⁻¹ where:\n- P contains eigenvectors as columns\n- D is diagonal with eigenvalues\n- A must be diagonalizable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eigenvalue_decomposition_example():\n    \"\"\"Demonstrate eigenvalue decomposition\"\"\"\n    A = np.array([[4, -2], [1, 1]])\n    print(\"Matrix A:\")\n    print(A)\n    \n    # Perform eigenvalue decomposition\n    eigenvals, eigenvecs = np.linalg.eig(A)\n    \n    print(f\"\\nEigenvalues: {eigenvals}\")\n    print(\"\\nEigenvectors (columns):\")\n    print(eigenvecs)\n    \n    # Construct diagonal matrix\n    D = np.diag(eigenvals)\n    P = eigenvecs\n    P_inv = np.linalg.inv(P)\n    \n    print(\"\\nDiagonal matrix D:\")\n    print(D)\n    print(\"\\nEigenvector matrix P:\")\n    print(P)\n    \n    # Verify decomposition\n    A_reconstructed = P @ D @ P_inv\n    print(\"\\nA reconstructed (P @ D @ P^(-1)):\")\n    print(A_reconstructed)\n    print(f\"Decomposition error: {np.linalg.norm(A - A_reconstructed):.2e}\")\n    \n    return eigenvals, eigenvecs\n\neigenvals, eigenvecs = eigenvalue_decomposition_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applications in Machine Learning\n\n### Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pca_with_svd(data, n_components=None):\n    \"\"\"Perform PCA using SVD\"\"\"\n    # Center the data\n    data_centered = data - np.mean(data, axis=0)\n    \n    # SVD of centered data\n    U, S, Vt = np.linalg.svd(data_centered, full_matrices=False)\n    \n    if n_components is None:\n        n_components = len(S)\n    \n    # Principal components are right singular vectors\n    principal_components = Vt[:n_components, :]\n    \n    # Project data onto principal components\n    data_pca = U[:, :n_components] @ np.diag(S[:n_components])\n    \n    return data_pca, principal_components, S\n\n# Example: PCA on synthetic data\nnp.random.seed(42)\nn_samples, n_features = 100, 3\ndata = np.random.randn(n_samples, n_features)\n# Add correlation\ndata[:, 2] = 0.8 * data[:, 0] + 0.2 * np.random.randn(n_samples)\n\nprint(f\"Original data shape: {data.shape}\")\n\n# Perform PCA\ndata_pca, components, singular_values = pca_with_svd(data, n_components=2)\n\nprint(f\"PCA data shape: {data_pca.shape}\")\nprint(f\"Principal components shape: {components.shape}\")\n\n# Explained variance\nexplained_variance = singular_values**2 / (n_samples - 1)\ntotal_variance = np.sum(explained_variance)\nexplained_variance_ratio = explained_variance / total_variance\n\nprint(f\"\\nExplained variance ratio:\")\nfor i, ratio in enumerate(explained_variance_ratio[:2]):\n    print(f\"PC{i+1}: {ratio:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Matrix Factorization for Recommender Systems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def matrix_factorization(R, k, max_iter=100, learning_rate=0.01, reg=0.1):\n    \"\"\"Simple matrix factorization for recommender systems\"\"\"\n    m, n = R.shape\n    \n    # Initialize factors\n    U = np.random.randn(m, k) * 0.1\n    V = np.random.randn(n, k) * 0.1\n    \n    # Find non-zero entries\n    non_zero = R != 0\n    \n    for iteration in range(max_iter):\n        # Compute prediction\n        R_pred = U @ V.T\n        \n        # Compute error only for non-zero entries\n        error = R_pred - R\n        error[~non_zero] = 0\n        \n        # Update factors\n        U_grad = error @ V + reg * U\n        V_grad = error.T @ U + reg * V\n        \n        U -= learning_rate * U_grad\n        V -= learning_rate * V_grad\n        \n        # Compute loss\n        if iteration % 20 == 0:\n            loss = np.sum(error**2) + reg * (np.sum(U**2) + np.sum(V**2))\n            print(f\"Iteration {iteration}, Loss: {loss:.4f}\")\n    \n    return U, V\n\n# Example: Simple recommender system\nnp.random.seed(42)\n# Create rating matrix (users x items)\nn_users, n_items = 10, 15\nR = np.random.randint(1, 6, (n_users, n_items))\n# Add some missing ratings\nR[np.random.rand(n_users, n_items) < 0.3] = 0\n\nprint(\"Rating matrix (0 = missing rating):\")\nprint(R)\n\n# Perform matrix factorization\nk = 3  # Number of latent factors\nU, V = matrix_factorization(R, k, max_iter=100)\n\n# Predict missing ratings\nR_pred = U @ V.T\nprint(f\"\\nPredicted ratings:\")\nprint(R_pred.round(2))\n\n# Evaluate on non-zero entries\nnon_zero = R != 0\nmse = np.mean((R_pred[non_zero] - R[non_zero])**2)\nprint(f\"\\nMean squared error on observed ratings: {mse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n### Exercise 1: LU Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform LU decomposition on matrix\nA = np.array([[2, 1, 1], [4, -6, 0], [-2, 7, 2]])\n\n# Your code here:\n# 1. Perform LU decomposition\n# 2. Solve the system Ax = [5, -2, 9] using LU\n# 3. Verify the solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: QR Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform QR decomposition and least squares\nA = np.array([[1, 1], [1, 2], [1, 3], [1, 4]])\nb = np.array([2, 3, 4, 5])\n\n# Your code here:\n# 1. Perform QR decomposition\n# 2. Solve least squares problem\n# 3. Compare with numpy's lstsq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: SVD and Low-Rank Approximation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a matrix and perform SVD\nA = np.random.randn(10, 8)\n\n# Your code here:\n# 1. Perform SVD decomposition\n# 2. Create rank-3 approximation\n# 3. Calculate compression ratio and error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solutions\n\n### Solution 1: LU Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Perform LU decomposition\nP, L, U = lu(A)\nprint(\"LU decomposition:\")\nprint(f\"L:\\n{L}\")\nprint(f\"U:\\n{U}\")\n\n# 2. Solve system using LU\nb = np.array([5, -2, 9])\nx = lu_solve(lu_factor(A), b)\nprint(f\"\\nSolution x: {x}\")\n\n# 3. Verify solution\nb_computed = A @ x\nprint(f\"Computed b: {b_computed}\")\nprint(f\"Verification error: {np.linalg.norm(b - b_computed):.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 2: QR Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Perform QR decomposition\nQ, R = np.linalg.qr(A)\nprint(\"QR decomposition:\")\nprint(f\"Q:\\n{Q}\")\nprint(f\"R:\\n{R}\")\n\n# 2. Solve least squares\nQtb = Q.T @ b\nx = np.linalg.solve(R, Qtb)\nprint(f\"\\nLeast squares solution: {x}\")\n\n# 3. Compare with numpy\nx_numpy = np.linalg.lstsq(A, b, rcond=None)[0]\nprint(f\"Numpy solution: {x_numpy}\")\nprint(f\"Difference: {np.linalg.norm(x - x_numpy):.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 3: SVD and Low-Rank Approximation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Perform SVD\nU, S, Vt = np.linalg.svd(A)\nprint(f\"Singular values: {S}\")\n\n# 2. Create rank-3 approximation\nk = 3\nU_k = U[:, :k]\nS_k = S[:k]\nVt_k = Vt[:k, :]\nA_k = U_k @ np.diag(S_k) @ Vt_k\n\n# 3. Calculate metrics\noriginal_size = A.size\ncompressed_size = U_k.size + S_k.size + Vt_k.size\ncompression_ratio = original_size / compressed_size\nerror = np.linalg.norm(A - A_k) / np.linalg.norm(A)\n\nprint(f\"Compression ratio: {compression_ratio:.2f}x\")\nprint(f\"Relative error: {error:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nIn this chapter, we covered:\n- LU decomposition for solving linear systems\n- QR decomposition for least squares problems\n- SVD for dimensionality reduction and matrix approximation\n- Cholesky decomposition for positive definite matrices\n- Eigenvalue decomposition for diagonalizable matrices\n- Applications in PCA and recommender systems\n\nMatrix decompositions are essential tools for understanding matrix structure and implementing efficient algorithms in machine learning.\n\n## Next Steps\n\nIn the next chapter, we'll explore applications of linear algebra in machine learning, including linear regression, neural networks, and optimization."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}