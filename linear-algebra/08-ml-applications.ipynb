{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Applications in Machine Learning\n\n[![Chapter](https://img.shields.io/badge/Chapter-8-blue.svg)]()\n[![Topic](https://img.shields.io/badge/Topic-ML_Applications-green.svg)]()\n[![Difficulty](https://img.shields.io/badge/Difficulty-Advanced-red.svg)]()\n\n## Introduction\n\nLinear algebra is the mathematical foundation of machine learning, providing the theoretical framework and computational tools that enable modern AI systems. This chapter explores how fundamental linear algebra concepts are applied in various machine learning algorithms, from simple linear regression to complex neural networks and deep learning systems.\n\n**Mathematical Foundation:**\nMachine learning algorithms can be viewed as optimization problems in high-dimensional vector spaces:\n- **Feature Space**: Data points are vectors in ℝⁿ\n- **Parameter Space**: Model parameters are vectors in ℝᵖ\n- **Loss Functions**: Scalar functions mapping parameter vectors to real numbers\n- **Gradients**: Vector derivatives indicating direction of steepest descent\n\n**Key Linear Algebra Concepts in ML:**\n1. **Vector Operations**: Dot products, norms, and projections for similarity and distance\n2. **Matrix Operations**: Transformations, decompositions, and eigenvalue problems\n3. **Optimization**: Gradient descent, Hessian matrices, and convex optimization\n4. **Dimensionality Reduction**: Principal component analysis and matrix factorizations\n5. **Kernel Methods**: Inner products and feature space transformations\n\n**Geometric Interpretation:**\nMachine learning can be understood geometrically:\n- **Linear Models**: Find hyperplanes that best separate or fit data\n- **Nonlinear Models**: Transform data into higher-dimensional spaces where linear separation is possible\n- **Clustering**: Find centroids that minimize distances to data points\n- **Dimensionality Reduction**: Project data onto lower-dimensional subspaces while preserving structure\n\n## 1. Linear Regression\n\nLinear regression is the foundation of supervised learning, modeling the relationship between features and target as a linear combination with additive noise.\n\n### Mathematical Foundation\n\n**Model Definition:**\nFor input features x ∈ ℝⁿ and target y ∈ ℝ, the linear regression model is:\ny = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε = xᵀβ + ε\n\nwhere:\n- β = [β₀, β₁, ..., βₙ]ᵀ is the parameter vector (including bias term)\n- x = [1, x₁, x₂, ..., xₙ]ᵀ is the feature vector (with bias term)\n- ε ~ N(0, σ²) is the noise term\n\n**Matrix Formulation:**\nFor a dataset with m samples, we have:\nY = Xβ + ε\n\nwhere:\n- Y ∈ ℝᵐ is the target vector\n- X ∈ ℝᵐˣ⁽ⁿ⁺¹⁾ is the design matrix (with bias column)\n- β ∈ ℝⁿ⁺¹ is the parameter vector\n- ε ∈ ℝᵐ is the noise vector\n\n**Objective Function:**\nThe least squares objective is:\nmin ||Xβ - Y||₂² = min Σᵢ₌₁ᵐ (xᵢᵀβ - yᵢ)²\n\n**Geometric Interpretation:**\nLinear regression finds the projection of Y onto the column space of X, minimizing the orthogonal distance between Y and the subspace spanned by X's columns.\n\n### Normal Equation Solution\n\n**Mathematical Derivation:**\nThe normal equation is derived by setting the gradient of the objective function to zero:\n\n∇β(||Xβ - Y||₂²) = 2Xᵀ(Xβ - Y) = 0\n\nSolving for β:\nXᵀXβ = XᵀY\nβ = (XᵀX)⁻¹XᵀY\n\n**Properties:**\n1. **Uniqueness**: Solution is unique if X has full column rank\n2. **Optimality**: Global minimum of the convex objective function\n3. **Computational Cost**: O(n³) for matrix inversion, O(n²) for matrix multiplication\n4. **Numerical Stability**: Can be ill-conditioned for nearly singular XᵀX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\ndef linear_regression_comprehensive(X, y, method='normal_equation', regularization=None, lambda_reg=1.0):\n    \"\"\"\n    Comprehensive linear regression implementation with multiple solution methods\n    \n    Mathematical approaches:\n    1. Normal equation: β = (X^T X)^(-1) X^T y\n    2. QR decomposition: More numerically stable\n    3. SVD decomposition: Handles rank-deficient cases\n    4. Gradient descent: Iterative optimization\n    \n    Parameters:\n    X: numpy array - feature matrix (samples × features)\n    y: numpy array - target vector\n    method: str - solution method\n    regularization: str - 'ridge' or 'lasso' regularization\n    lambda_reg: float - regularization strength\n    \n    Returns:\n    dict - comprehensive results including parameters, predictions, and analysis\n    \"\"\"\n    n_samples, n_features = X.shape\n    \n    # Add bias term\n    X_with_bias = np.column_stack([np.ones(n_samples), X])\n    \n    # Check for multicollinearity\n    condition_number = np.linalg.cond(X_with_bias)\n    rank = np.linalg.matrix_rank(X_with_bias)\n    well_conditioned = condition_number < 1000\n    \n    results = {\n        'method': method,\n        'regularization': regularization,\n        'lambda_reg': lambda_reg,\n        'condition_number': condition_number,\n        'rank': rank,\n        'well_conditioned': well_conditioned,\n        'n_samples': n_samples,\n        'n_features': n_features\n    }\n    \n    if method == 'normal_equation':\n        if regularization == 'ridge':\n            # Ridge regression: β = (X^T X + λI)^(-1) X^T y\n            I = np.eye(X_with_bias.shape[1])\n            I[0, 0] = 0  # Don't regularize bias term\n            beta = np.linalg.inv(X_with_bias.T @ X_with_bias + lambda_reg * I) @ X_with_bias.T @ y\n        else:\n            # Standard normal equation\n            beta = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y\n            \n    elif method == 'qr':\n        # QR decomposition: more numerically stable\n        Q, R = np.linalg.qr(X_with_bias)\n        beta = np.linalg.solve(R, Q.T @ y)\n        \n    elif method == 'svd':\n        # SVD decomposition: handles rank-deficient cases\n        U, S, Vt = np.linalg.svd(X_with_bias, full_matrices=False)\n        # Use pseudo-inverse for numerical stability\n        S_inv = np.diag(1.0 / np.where(S > 1e-10, S, 0))\n        beta = Vt.T @ S_inv @ U.T @ y\n        \n    elif method == 'gradient_descent':\n        # Gradient descent implementation\n        beta = gradient_descent_linear_regression(X_with_bias, y, learning_rate=0.01, epochs=1000)\n        \n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n    \n    # Compute predictions and metrics\n    y_pred = X_with_bias @ beta\n    residuals = y - y_pred\n    \n    # Statistical analysis\n    mse = mean_squared_error(y, y_pred)\n    r2 = r2_score(y, y_pred)\n    rmse = np.sqrt(mse)\n    \n    # Residual analysis\n    residual_std = np.std(residuals)\n    residual_mean = np.mean(residuals)\n    \n    # Parameter analysis\n    parameter_std = np.std(beta[1:])  # Exclude bias term\n    parameter_norm = np.linalg.norm(beta[1:])\n    \n    # Confidence intervals (simplified)\n    if method == 'normal_equation' and regularization is None:\n        # Compute standard errors\n        residual_variance = np.sum(residuals**2) / (n_samples - n_features - 1)\n        XtX_inv = np.linalg.inv(X_with_bias.T @ X_with_bias)\n        standard_errors = np.sqrt(np.diag(XtX_inv) * residual_variance)\n        \n        # 95% confidence intervals\n        confidence_intervals = np.column_stack([\n            beta - 1.96 * standard_errors,\n            beta + 1.96 * standard_errors\n        ])\n    else:\n        confidence_intervals = None\n    \n    results.update({\n        'beta': beta,\n        'y_pred': y_pred,\n        'residuals': residuals,\n        'mse': mse,\n        'r2': r2,\n        'rmse': rmse,\n        'residual_std': residual_std,\n        'residual_mean': residual_mean,\n        'parameter_std': parameter_std,\n        'parameter_norm': parameter_norm,\n        'confidence_intervals': confidence_intervals\n    })\n    \n    return results\n\ndef gradient_descent_linear_regression(X, y, learning_rate=0.01, epochs=1000, tolerance=1e-6):\n    \"\"\"\n    Gradient descent for linear regression\n    \n    Mathematical approach:\n    β^(t+1) = β^(t) - α ∇β J(β)\n    where J(β) = (1/2m) ||Xβ - y||²\n    and ∇β J(β) = (1/m) X^T (Xβ - y)\n    \n    Parameters:\n    X: numpy array - design matrix (including bias)\n    y: numpy array - target vector\n    learning_rate: float - learning rate\n    epochs: int - maximum number of iterations\n    tolerance: float - convergence tolerance\n    \n    Returns:\n    numpy array - optimized parameter vector\n    \"\"\"\n    n_samples = X.shape[0]\n    beta = np.zeros(X.shape[1])\n    costs = []\n    \n    for epoch in range(epochs):\n        # Forward pass\n        predictions = X @ beta\n        \n        # Compute cost\n        cost = np.mean((predictions - y) ** 2) / 2\n        costs.append(cost)\n        \n        # Compute gradient\n        gradients = (1/n_samples) * X.T @ (predictions - y)\n        \n        # Update parameters\n        beta_new = beta - learning_rate * gradients\n        \n        # Check convergence\n        if np.linalg.norm(beta_new - beta) < tolerance:\n            break\n            \n        beta = beta_new\n    \n    return beta\n\ndef analyze_linear_regression_results(results, X, y):\n    \"\"\"\n    Comprehensive analysis of linear regression results\n    \n    Parameters:\n    results: dict - results from linear_regression_comprehensive\n    X: numpy array - original feature matrix\n    y: numpy array - target vector\n    \n    Returns:\n    dict - comprehensive analysis\n    \"\"\"\n    analysis = {}\n    \n    # Model performance\n    analysis['performance'] = {\n        'mse': results['mse'],\n        'r2': results['r2'],\n        'rmse': results['rmse'],\n        'explained_variance': results['r2']\n    }\n    \n    # Residual analysis\n    residuals = results['residuals']\n    analysis['residuals'] = {\n        'mean': np.mean(residuals),\n        'std': np.std(residuals),\n        'skewness': np.mean(((residuals - np.mean(residuals)) / np.std(residuals))**3),\n        'kurtosis': np.mean(((residuals - np.mean(residuals)) / np.std(residuals))**4) - 3,\n        'normality_test': np.allclose(np.mean(residuals), 0, atol=1e-10)\n    }\n    \n    # Parameter analysis\n    beta = results['beta']\n    analysis['parameters'] = {\n        'bias': beta[0],\n        'feature_weights': beta[1:],\n        'weight_magnitudes': np.abs(beta[1:]),\n        'largest_weight': np.max(np.abs(beta[1:])),\n        'smallest_weight': np.min(np.abs(beta[1:])),\n        'weight_std': np.std(beta[1:])\n    }\n    \n    # Multicollinearity analysis\n    if X.shape[1] > 1:\n        corr_matrix = np.corrcoef(X.T)\n        high_corr_pairs = []\n        for i in range(X.shape[1]):\n            for j in range(i+1, X.shape[1]):\n                if abs(corr_matrix[i, j]) > 0.8:\n                    high_corr_pairs.append((i, j, corr_matrix[i, j]))\n        \n        analysis['multicollinearity'] = {\n            'correlation_matrix': corr_matrix,\n            'high_correlation_pairs': high_corr_pairs,\n            'vif_scores': compute_vif_scores(X)\n        }\n    \n    # Numerical stability\n    analysis['numerical_stability'] = {\n        'condition_number': results['condition_number'],\n        'well_conditioned': results['well_conditioned'],\n        'rank': results['rank'],\n        'method_stability': results['method'] in ['qr', 'svd']\n    }\n    \n    return analysis\n\ndef compute_vif_scores(X):\n    \"\"\"\n    Compute Variance Inflation Factor (VIF) scores for multicollinearity detection\n    \n    VIF for feature i is computed as:\n    VIF_i = 1 / (1 - R²_i)\n    where R²_i is the coefficient of determination when feature i is regressed on all other features\n    \"\"\"\n    n_features = X.shape[1]\n    vif_scores = np.zeros(n_features)\n    \n    for i in range(n_features):\n        # Regress feature i on all other features\n        X_others = np.delete(X, i, axis=1)\n        y_feature = X[:, i]\n        \n        # Fit regression\n        beta = np.linalg.lstsq(X_others, y_feature, rcond=None)[0]\n        y_pred = X_others @ beta\n        \n        # Compute R²\n        ss_res = np.sum((y_feature - y_pred) ** 2)\n        ss_tot = np.sum((y_feature - np.mean(y_feature)) ** 2)\n        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n        \n        # Compute VIF\n        vif_scores[i] = 1 / (1 - r_squared) if r_squared < 1 else float('inf')\n    \n    return vif_scores\n\ndef compare_linear_regression_methods(X, y, methods=['normal_equation', 'qr', 'svd', 'gradient_descent']):\n    \"\"\"\n    Compare different linear regression solution methods\n    \n    Parameters:\n    X: numpy array - feature matrix\n    y: numpy array - target vector\n    methods: list - methods to compare\n    \n    Returns:\n    dict - comparison results\n    \"\"\"\n    comparison = {}\n    \n    for method in methods:\n        try:\n            results = linear_regression_comprehensive(X, y, method=method)\n            analysis = analyze_linear_regression_results(results, X, y)\n            \n            comparison[method] = {\n                'success': True,\n                'results': results,\n                'analysis': analysis\n            }\n            \n        except Exception as e:\n            comparison[method] = {\n                'success': False,\n                'error': str(e)\n            }\n    \n    return comparison\n\n# Example: Comprehensive linear regression analysis\nprint(\"=== Comprehensive Linear Regression Analysis ===\")\n\n# Generate synthetic data with known structure\nnp.random.seed(42)\nn_samples, n_features = 200, 5\n\n# Create features with some correlation\nX = np.random.randn(n_samples, n_features)\nX[:, 2] = 0.7 * X[:, 0] + 0.3 * np.random.randn(n_samples)  # Correlated features\nX[:, 4] = 0.5 * X[:, 1] + 0.5 * np.random.randn(n_samples)  # Another correlation\n\n# True parameters\ntrue_beta = np.array([2.5, -1.0, 0.8, 1.2, -0.5])\ntrue_bias = 3.0\n\n# Generate target with noise\ny = X @ true_beta + true_bias + np.random.normal(0, 0.5, n_samples)\n\nprint(f\"Data shape: {X.shape}\")\nprint(f\"True parameters: bias={true_bias}, weights={true_beta}\")\n\n# Compare different methods\nmethods = ['normal_equation', 'qr', 'svd', 'gradient_descent']\ncomparison = compare_linear_regression_methods(X, y, methods)\n\nprint(f\"\\nMethod Comparison:\")\nfor method, result in comparison.items():\n    if result['success']:\n        results = result['results']\n        analysis = result['analysis']\n        \n        print(f\"\\n{method.upper()}:\")\n        print(f\"  MSE: {results['mse']:.6f}\")\n        print(f\"  R²: {results['r2']:.6f}\")\n        print(f\"  RMSE: {results['rmse']:.6f}\")\n        print(f\"  Condition number: {results['condition_number']:.2e}\")\n        print(f\"  Well-conditioned: {results['well_conditioned']}\")\n        print(f\"  Residual mean: {analysis['residuals']['mean']:.6f}\")\n        print(f\"  Residual std: {analysis['residuals']['std']:.6f}\")\n        \n        # Compare with true parameters\n        estimated_bias = results['beta'][0]\n        estimated_weights = results['beta'][1:]\n        bias_error = abs(estimated_bias - true_bias)\n        weight_error = np.linalg.norm(estimated_weights - true_beta)\n        \n        print(f\"  Bias error: {bias_error:.6f}\")\n        print(f\"  Weight error: {weight_error:.6f}\")\n        \n    else:\n        print(f\"\\n{method.upper()}: Failed - {result['error']}\")\n\n# Test with regularization\nprint(f\"\\n=== Regularization Analysis ===\")\n\n# Test ridge regression\nridge_results = linear_regression_comprehensive(X, y, method='normal_equation', regularization='ridge', lambda_reg=0.1)\nridge_analysis = analyze_linear_regression_results(ridge_results, X, y)\n\nprint(f\"Ridge Regression (λ=0.1):\")\nprint(f\"  MSE: {ridge_results['mse']:.6f}\")\nprint(f\"  R²: {ridge_results['r2']:.6f}\")\nprint(f\"  Parameter norm: {ridge_results['parameter_norm']:.6f}\")\n\n# Compare parameter magnitudes\nstandard_weights = comparison['normal_equation']['results']['beta'][1:]\nridge_weights = ridge_results['beta'][1:]\n\nprint(f\"  Standard weights: {np.linalg.norm(standard_weights):.6f}\")\nprint(f\"  Ridge weights: {np.linalg.norm(ridge_weights):.6f}\")\nprint(f\"  Shrinkage: {np.linalg.norm(standard_weights) - np.linalg.norm(ridge_weights):.6f}\")\n\n# Multicollinearity analysis\nprint(f\"\\n=== Multicollinearity Analysis ===\")\n\nanalysis = comparison['normal_equation']['analysis']\nif 'multicollinearity' in analysis:\n    vif_scores = analysis['multicollinearity']['vif_scores']\n    high_corr_pairs = analysis['multicollinearity']['high_correlation_pairs']\n    \n    print(f\"VIF scores:\")\n    for i, vif in enumerate(vif_scores):\n        print(f\"  Feature {i+1}: {vif:.2f}\")\n    \n    print(f\"High correlation pairs:\")\n    for pair in high_corr_pairs:\n        print(f\"  Features {pair[0]+1} and {pair[1]+1}: {pair[2]:.3f}\")\n\n# Visualize results\nprint(f\"\\n=== Visualization ===\")\n\n# Plot predictions vs actual\nplt.figure(figsize=(15, 5))\n\n# Method 1: Predictions vs Actual\nplt.subplot(1, 3, 1)\nfor method in ['normal_equation', 'qr', 'svd']:\n    if comparison[method]['success']:\n        y_pred = comparison[method]['results']['y_pred']\n        plt.scatter(y, y_pred, alpha=0.6, label=method, s=20)\n\nplt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', alpha=0.5)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Predictions vs Actual')\nplt.legend()\n\n# Method 2: Residuals\nplt.subplot(1, 3, 2)\nresiduals = comparison['normal_equation']['results']['residuals']\nplt.scatter(comparison['normal_equation']['results']['y_pred'], residuals, alpha=0.6)\nplt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\n\n# Method 3: Parameter comparison\nplt.subplot(1, 3, 3)\nx_pos = np.arange(len(true_beta))\nplt.bar(x_pos - 0.2, true_beta, width=0.4, label='True', alpha=0.7)\nplt.bar(x_pos + 0.2, comparison['normal_equation']['results']['beta'][1:], width=0.4, label='Estimated', alpha=0.7)\nplt.xlabel('Feature Index')\nplt.ylabel('Weight')\nplt.title('Parameter Comparison')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Test with ill-conditioned data\nprint(f\"\\n=== Ill-conditioned Data Test ===\")\n\n# Create nearly singular design matrix\nX_ill = X.copy()\nX_ill[:, 3] = X_ill[:, 0] + 1e-8 * np.random.randn(n_samples)  # Nearly dependent\n\nill_comparison = compare_linear_regression_methods(X_ill, y, methods)\n\nprint(f\"Ill-conditioned data results:\")\nfor method, result in ill_comparison.items():\n    if result['success']:\n        results = result['results']\n        print(f\"  {method}: MSE={results['mse']:.6f}, Condition number={results['condition_number']:.2e}\")\n    else:\n        print(f\"  {method}: Failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ridge Regression (L2 Regularization)\n\n**Mathematical Foundation:**\nRidge regression adds L2 regularization to prevent overfitting and handle multicollinearity:\n\n**Objective Function:**\nmin ||Xβ - Y||₂² + λ||β||₂²\n\nwhere λ > 0 is the regularization parameter.\n\n**Solution:**\nβ = (XᵀX + λI)⁻¹XᵀY\n\n**Geometric Interpretation:**\nRidge regression shrinks parameter estimates toward zero, trading bias for variance reduction.\n\n**Key Properties:**\n1. **Bias-Variance Trade-off**: Increases bias, decreases variance\n2. **Multicollinearity**: Handles correlated features effectively\n3. **Numerical Stability**: Improves conditioning of XᵀX\n4. **Shrinkage**: All parameters are shrunk by the same factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ridge_regression_comprehensive(X, y, lambda_reg=1.0, method='normal_equation'):\n    \"\"\"\n    Comprehensive ridge regression implementation\n    \n    Mathematical approach:\n    β = (X^T X + λI)^(-1) X^T y\n    \n    Parameters:\n    X: numpy array - feature matrix\n    y: numpy array - target vector\n    lambda_reg: float - regularization parameter\n    method: str - solution method\n    \n    Returns:\n    dict - comprehensive results\n    \"\"\"\n    n_samples, n_features = X.shape\n    X_with_bias = np.column_stack([np.ones(n_samples), X])\n    \n    if method == 'normal_equation':\n        # Standard ridge solution\n        I = np.eye(X_with_bias.shape[1])\n        I[0, 0] = 0  # Don't regularize bias term\n        \n        beta = np.linalg.inv(X_with_bias.T @ X_with_bias + lambda_reg * I) @ X_with_bias.T @ y\n        \n    elif method == 'svd':\n        # SVD-based solution for numerical stability\n        U, S, Vt = np.linalg.svd(X_with_bias, full_matrices=False)\n        \n        # Ridge regularization in SVD space\n        S_ridge = S / (S**2 + lambda_reg)\n        beta = Vt.T @ np.diag(S_ridge) @ U.T @ y\n        \n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n    \n    # Compute predictions and metrics\n    y_pred = X_with_bias @ beta\n    residuals = y - y_pred\n    \n    mse = mean_squared_error(y, y_pred)\n    r2 = r2_score(y, y_pred)\n    \n    # Effective degrees of freedom\n    if method == 'normal_equation':\n        XtX = X_with_bias.T @ X_with_bias\n        I_reg = np.eye(X_with_bias.shape[1])\n        I_reg[0, 0] = 0\n        effective_df = np.trace(X_with_bias @ np.linalg.inv(XtX + lambda_reg * I_reg) @ X_with_bias.T)\n    else:\n        effective_df = np.sum(S**2 / (S**2 + lambda_reg))\n    \n    return {\n        'beta': beta,\n        'y_pred': y_pred,\n        'residuals': residuals,\n        'mse': mse,\n        'r2': r2,\n        'lambda_reg': lambda_reg,\n        'effective_df': effective_df,\n        'parameter_norm': np.linalg.norm(beta[1:])\n    }\n\ndef ridge_regression_path(X, y, lambda_range=np.logspace(-3, 3, 50)):\n    \"\"\"\n    Compute ridge regression for a range of regularization parameters\n    \n    Parameters:\n    X: numpy array - feature matrix\n    y: numpy array - target vector\n    lambda_range: array - range of lambda values\n    \n    Returns:\n    dict - results for each lambda value\n    \"\"\"\n    path_results = []\n    \n    for lambda_val in lambda_range:\n        results = ridge_regression_comprehensive(X, y, lambda_val)\n        path_results.append({\n            'lambda': lambda_val,\n            'beta': results['beta'],\n            'mse': results['mse'],\n            'r2': results['r2'],\n            'parameter_norm': results['parameter_norm'],\n            'effective_df': results['effective_df']\n        })\n    \n    return path_results\n\n# Example: Ridge regression analysis\nprint(\"\\n=== Ridge Regression Analysis ===\")\n\n# Test different lambda values\nlambda_values = np.logspace(-3, 3, 20)\nridge_path = ridge_regression_path(X, y, lambda_values)\n\n# Extract results for plotting\nlambdas = [r['lambda'] for r in ridge_path]\nmses = [r['mse'] for r in ridge_path]\nr2s = [r['r2'] for r in ridge_path]\nparam_norms = [r['parameter_norm'] for r in ridge_path]\neffective_dfs = [r['effective_df'] for r in ridge_path]\n\n# Plot ridge path\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.semilogx(lambdas, mses, 'b-', linewidth=2)\nplt.xlabel('Regularization Parameter (λ)')\nplt.ylabel('Mean Squared Error')\nplt.title('Ridge Path: MSE vs λ')\nplt.grid(True)\n\nplt.subplot(1, 3, 2)\nplt.semilogx(lambdas, r2s, 'r-', linewidth=2)\nplt.xlabel('Regularization Parameter (λ)')\nplt.ylabel('R² Score')\nplt.title('Ridge Path: R² vs λ')\nplt.grid(True)\n\nplt.subplot(1, 3, 3)\nplt.semilogx(lambdas, param_norms, 'g-', linewidth=2)\nplt.xlabel('Regularization Parameter (λ)')\nplt.ylabel('Parameter Norm')\nplt.title('Ridge Path: Parameter Norm vs λ')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Compare with standard regression\nprint(f\"Standard regression parameter norm: {np.linalg.norm(comparison['normal_equation']['results']['beta'][1:]):.6f}\")\nprint(f\"Ridge regression (λ=1.0) parameter norm: {ridge_regression_comprehensive(X, y, 1.0)['parameter_norm']:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Principal Component Analysis (PCA)\n\nPCA finds the directions of maximum variance in data by computing eigenvectors of the covariance matrix.\n\n### Manual PCA Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def manual_pca(X, n_components=2):\n    # Center the data\n    X_centered = X - np.mean(X, axis=0)\n    \n    # Compute covariance matrix\n    cov_matrix = np.cov(X_centered.T)\n    \n    # Find eigenvalues and eigenvectors\n    eigenvals, eigenvecs = np.linalg.eig(cov_matrix)\n    \n    # Sort by eigenvalue magnitude\n    sorted_indices = np.argsort(eigenvals)[::-1]\n    eigenvals_sorted = eigenvals[sorted_indices]\n    eigenvecs_sorted = eigenvecs[:, sorted_indices]\n    \n    # Project data\n    X_pca = X_centered @ eigenvecs_sorted[:, :n_components]\n    \n    return X_pca, eigenvecs_sorted[:, :n_components], eigenvals_sorted\n\n# Generate data with known structure\nnp.random.seed(42)\nn_samples = 1000\n# Create correlated features\nX = np.random.randn(n_samples, 3)\nX[:, 2] = 0.8 * X[:, 0] + 0.2 * np.random.randn(n_samples)\n\n# Apply PCA\nX_pca, components, eigenvals = manual_pca(X, n_components=2)\n\n# Visualize\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.scatter(X[:, 0], X[:, 1], alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Original Data (Features 1 vs 2)')\n\nplt.subplot(1, 3, 2)\nplt.scatter(X[:, 0], X[:, 2], alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 3')\nplt.title('Original Data (Features 1 vs 3)')\n\nplt.subplot(1, 3, 3)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA Transformed Data')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Explained variance ratio:\", eigenvals[:2] / np.sum(eigenvals))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA for Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\n\n# Load iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Apply PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Visualize\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nfor i in range(3):\n    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=f'Class {i}')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA of Iris Dataset')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance Ratio')\nplt.title('Explained Variance vs Components')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Recommender Systems\n\n### Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):\n    \"\"\"\n    R: rating matrix\n    P: user matrix\n    Q: item matrix\n    K: number of latent factors\n    \"\"\"\n    Q = Q.T\n    for step in range(steps):\n        for i in range(len(R)):\n            for j in range(len(R[i])):\n                if R[i][j] > 0:\n                    eij = R[i][j] - np.dot(P[i,:], Q[:,j])\n                    for k in range(K):\n                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n        eR = np.dot(P, Q)\n        e = 0\n        for i in range(len(R)):\n            for j in range(len(R[i])):\n                if R[i][j] > 0:\n                    e = e + pow(R[i][j] - np.dot(P[i,:], Q[:,j]), 2)\n                    for k in range(K):\n                        e = e + (beta/2) * (pow(P[i][k], 2) + pow(Q[k][j], 2))\n        if e < 0.001:\n            break\n    return P, Q.T\n\n# Example: Simple rating matrix\nR = np.array([\n    [5, 3, 0, 1],\n    [4, 0, 0, 1],\n    [1, 1, 0, 5],\n    [1, 0, 0, 4],\n    [0, 1, 5, 4]\n])\n\nN = len(R)\nM = len(R[0])\nK = 2\n\nP = np.random.rand(N, K)\nQ = np.random.rand(M, K)\n\nnP, nQ = matrix_factorization(R, P, Q, K)\nnR = np.dot(nP, nQ.T)\n\nprint(\"Original ratings:\")\nprint(R)\nprint(\"\\nPredicted ratings:\")\nprint(nR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Neural Networks\n\n### Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\nclass SimpleNeuralNetwork:\n    def __init__(self, input_size, hidden_size, output_size):\n        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n        self.b1 = np.zeros((1, hidden_size))\n        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n        self.b2 = np.zeros((1, output_size))\n    \n    def forward(self, X):\n        # Hidden layer\n        self.z1 = X @ self.W1 + self.b1\n        self.a1 = sigmoid(self.z1)\n        \n        # Output layer\n        self.z2 = self.a1 @ self.W2 + self.b2\n        self.a2 = sigmoid(self.z2)\n        \n        return self.a2\n    \n    def backward(self, X, y, learning_rate=0.1):\n        m = X.shape[0]\n        \n        # Backpropagation\n        dz2 = self.a2 - y\n        dW2 = (self.a1.T @ dz2) / m\n        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n        \n        dz1 = (dz2 @ self.W2.T) * sigmoid_derivative(self.a1)\n        dW1 = (X.T @ dz1) / m\n        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n        \n        # Update weights\n        self.W2 -= learning_rate * dW2\n        self.b2 -= learning_rate * db2\n        self.W1 -= learning_rate * dW1\n        self.b1 -= learning_rate * db1\n\n# XOR problem\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([[0], [1], [1], [0]])\n\n# Train neural network\nnn = SimpleNeuralNetwork(2, 4, 1)\n\nfor epoch in range(10000):\n    # Forward pass\n    output = nn.forward(X)\n    \n    # Backward pass\n    nn.backward(X, y, learning_rate=0.1)\n    \n    if epoch % 1000 == 0:\n        loss = np.mean((output - y) ** 2)\n        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n\n# Test\npredictions = nn.forward(X)\nprint(\"\\nPredictions:\")\nfor i in range(len(X)):\n    print(f\"Input: {X[i]}, Target: {y[i][0]}, Prediction: {predictions[i][0]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Support Vector Machines (SVM)\n\n### Linear SVM with Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def linear_svm_gradient_descent(X, y, learning_rate=0.01, epochs=1000, lambda_reg=0.1):\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0\n    \n    for epoch in range(epochs):\n        for i in range(n_samples):\n            # Hinge loss gradient\n            if y[i] * (X[i] @ w + b) < 1:\n                dw = -y[i] * X[i] + lambda_reg * w\n                db = -y[i]\n            else:\n                dw = lambda_reg * w\n                db = 0\n            \n            w -= learning_rate * dw\n            b -= learning_rate * db\n    \n    return w, b\n\n# Generate linearly separable data\nnp.random.seed(42)\nn_samples = 100\nX = np.random.randn(n_samples, 2)\ny = np.where(X[:, 0] + X[:, 1] > 0, 1, -1)\n\n# Train SVM\nw, b = linear_svm_gradient_descent(X, y)\n\n# Visualize\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Class 1')\nplt.scatter(X[y == -1, 0], X[y == -1, 1], c='blue', label='Class -1')\n\n# Decision boundary\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\nZ = np.sign(xx * w[0] + yy * w[1] + b)\nplt.contour(xx, yy, Z, levels=[0], colors='black')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Linear SVM Decision Boundary')\nplt.legend()\n\nplt.subplot(1, 2, 2)\n# Show margin\nmargin_points = np.array([[-2, 2], [2, -2]])\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', alpha=0.6)\nplt.scatter(X[y == -1, 0], X[y == -1, 1], c='blue', alpha=0.6)\nplt.plot(margin_points[:, 0], margin_points[:, 1], 'k--', alpha=0.5, label='Margin')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('SVM with Margin')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Clustering with Linear Algebra\n\n### K-Means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def kmeans(X, k, max_iters=100):\n    n_samples, n_features = X.shape\n    \n    # Initialize centroids randomly\n    centroids = X[np.random.choice(n_samples, k, replace=False)]\n    \n    for _ in range(max_iters):\n        # Assign points to nearest centroid\n        distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        \n        # Update centroids\n        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n        \n        # Check convergence\n        if np.allclose(centroids, new_centroids):\n            break\n        \n        centroids = new_centroids\n    \n    return labels, centroids\n\n# Generate clustered data\nnp.random.seed(42)\nn_samples = 300\ncenters = np.array([[0, 0], [4, 4], [8, 0]])\nX = np.vstack([\n    np.random.randn(n_samples//3, 2) + centers[0],\n    np.random.randn(n_samples//3, 2) + centers[1],\n    np.random.randn(n_samples//3, 2) + centers[2]\n])\n\n# Apply K-means\nlabels, centroids = kmeans(X, k=3)\n\n# Visualize\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X[:, 0], X[:, 1], alpha=0.6)\nplt.title('Original Data')\n\nplt.subplot(1, 2, 2)\ncolors = ['red', 'blue', 'green']\nfor i in range(3):\n    plt.scatter(X[labels == i, 0], X[labels == i, 1], c=colors[i], alpha=0.6, label=f'Cluster {i}')\nplt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=200, linewidths=3, label='Centroids')\nplt.title('K-Means Clustering')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Optimization in Machine Learning\n\n### Gradient Descent for Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent_linear_regression(X, y, learning_rate=0.01, epochs=1000):\n    n_samples, n_features = X.shape\n    X_with_bias = np.column_stack([np.ones(n_samples), X])\n    theta = np.zeros(X_with_bias.shape[1])\n    \n    costs = []\n    \n    for epoch in range(epochs):\n        # Forward pass\n        predictions = X_with_bias @ theta\n        \n        # Compute cost\n        cost = np.mean((predictions - y) ** 2)\n        costs.append(cost)\n        \n        # Compute gradients\n        gradients = (2/n_samples) * X_with_bias.T @ (predictions - y)\n        \n        # Update parameters\n        theta -= learning_rate * gradients\n        \n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}, Cost: {cost:.4f}\")\n    \n    return theta, costs\n\n# Test gradient descent\ntheta_gd, costs = gradient_descent_linear_regression(X, y)\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(costs)\nplt.xlabel('Epoch')\nplt.ylabel('Cost')\nplt.title('Gradient Descent Convergence')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.scatter(X[:, 0], y, alpha=0.6, label='Data')\nX_line = np.linspace(X[:, 0].min(), X[:, 0].max(), 100).reshape(-1, 1)\ny_line = np.column_stack([np.ones(100), X_line]) @ theta_gd\nplt.plot(X_line, y_line, 'r-', label='Gradient Descent')\nplt.xlabel('Feature 1')\nplt.ylabel('Target')\nplt.title('Linear Regression Fit')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n1. **Linear Regression**: Implement linear regression using both normal equations and gradient descent.\n2. **PCA Implementation**: Implement PCA from scratch and compare with sklearn.\n3. **Neural Network**: Build a neural network to solve the XOR problem.\n4. **SVM**: Implement linear SVM using gradient descent.\n5. **Matrix Factorization**: Implement matrix factorization for a simple recommender system.\n\n## Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Linear Regression\ndef linear_regression_normal_equations(X, y):\n    X_with_bias = np.column_stack([np.ones(X.shape[0]), X])\n    return np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y\n\ndef linear_regression_gradient_descent(X, y, learning_rate=0.01, epochs=1000):\n    n_samples = X.shape[0]\n    X_with_bias = np.column_stack([np.ones(n_samples), X])\n    theta = np.zeros(X_with_bias.shape[1])\n    \n    for _ in range(epochs):\n        predictions = X_with_bias @ theta\n        gradients = (2/n_samples) * X_with_bias.T @ (predictions - y)\n        theta -= learning_rate * gradients\n    \n    return theta\n\n# Exercise 2: PCA Implementation\ndef pca_manual(X, n_components=2):\n    X_centered = X - np.mean(X, axis=0)\n    cov_matrix = np.cov(X_centered.T)\n    eigenvals, eigenvecs = np.linalg.eig(cov_matrix)\n    sorted_indices = np.argsort(eigenvals)[::-1]\n    eigenvecs_sorted = eigenvecs[:, sorted_indices]\n    return X_centered @ eigenvecs_sorted[:, :n_components]\n\n# Exercise 3: Neural Network for XOR\ndef train_xor_network():\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    y = np.array([[0], [1], [1], [0]])\n    \n    nn = SimpleNeuralNetwork(2, 4, 1)\n    for _ in range(10000):\n        output = nn.forward(X)\n        nn.backward(X, y, learning_rate=0.1)\n    \n    return nn\n\n# Exercise 4: Linear SVM\ndef linear_svm(X, y, learning_rate=0.01, epochs=1000):\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0\n    \n    for _ in range(epochs):\n        for i in range(n_samples):\n            if y[i] * (X[i] @ w + b) < 1:\n                w -= learning_rate * (-y[i] * X[i] + 0.1 * w)\n                b -= learning_rate * (-y[i])\n            else:\n                w -= learning_rate * (0.1 * w)\n    \n    return w, b\n\n# Exercise 5: Matrix Factorization\ndef simple_matrix_factorization(R, k=2, epochs=1000, learning_rate=0.01):\n    n_users, n_items = R.shape\n    P = np.random.randn(n_users, k)\n    Q = np.random.randn(n_items, k)\n    \n    for _ in range(epochs):\n        for i in range(n_users):\n            for j in range(n_items):\n                if R[i, j] > 0:\n                    eij = R[i, j] - P[i, :] @ Q[j, :]\n                    P[i, :] += learning_rate * eij * Q[j, :]\n                    Q[j, :] += learning_rate * eij * P[i, :]\n    \n    return P, Q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n- Linear algebra is fundamental to all machine learning algorithms\n- Matrix operations enable efficient computation of gradients and predictions\n- Eigenvalue decomposition (PCA) is crucial for dimensionality reduction\n- Matrix factorization enables collaborative filtering in recommender systems\n- Neural networks rely heavily on matrix multiplication for forward and backward passes\n- Optimization algorithms use linear algebra for parameter updates\n\n## Next Chapter\n\nIn the next chapter, we'll explore numerical linear algebra, focusing on numerical stability, conditioning, and efficient algorithms for large-scale problems."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}