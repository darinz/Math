{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Eigenvalues and Eigenvectors\n\n[![Chapter](https://img.shields.io/badge/Chapter-4-blue.svg)]()\n[![Topic](https://img.shields.io/badge/Topic-Eigenvalues_Eigenvectors-green.svg)]()\n[![Difficulty](https://img.shields.io/badge/Difficulty-Intermediate-orange.svg)]()\n\n## Introduction\n\nEigenvalues and eigenvectors are fundamental concepts in linear algebra that reveal the intrinsic properties of matrices. They are crucial for understanding matrix behavior, diagonalization, and applications in machine learning like Principal Component Analysis (PCA), spectral clustering, and dimensionality reduction. Eigenvalues and eigenvectors provide a \"natural coordinate system\" for understanding how matrices transform space.\n\n### Why Eigenvalues and Eigenvectors Matter in AI/ML\n\n1. **Dimensionality Reduction**: PCA uses eigenvectors to find the most important directions in data\n2. **Spectral Clustering**: Uses eigenvectors of similarity matrices for clustering\n3. **PageRank Algorithm**: Uses eigenvectors to rank web pages\n4. **Neural Networks**: Weight matrices have eigenvalues that affect training dynamics\n5. **Optimization**: Hessian eigenvalues determine convergence properties\n6. **Signal Processing**: Fourier transforms and other spectral methods use eigenvectors\n\n## What are Eigenvalues and Eigenvectors?\n\n### Mathematical Definition\n\nFor a square matrix $A$ of size $n \\times n$, a non-zero vector $\\vec{v} \\in \\mathbb{R}^n$ is an **eigenvector** if:\n$$A\\vec{v} = \\lambda\\vec{v}$$\n\nwhere $\\lambda$ is a scalar called the **eigenvalue** corresponding to $\\vec{v}$.\n\n### Geometric Interpretation\n\nEigenvectors are special vectors that don't change direction when transformed by the matrix $A$. They only get scaled by their corresponding eigenvalue:\n\n- If $\\lambda > 0$: The eigenvector is stretched by factor $\\lambda$\n- If $\\lambda < 0$: The eigenvector is stretched by factor $|\\lambda|$ and flipped\n- If $\\lambda = 0$: The eigenvector is mapped to the zero vector\n- If $|\\lambda| > 1$: The eigenvector is expanded\n- If $|\\lambda| < 1$: The eigenvector is contracted\n\n### Fundamental Properties\n\n1. **Eigenvectors are not unique**: If $\\vec{v}$ is an eigenvector, then $c\\vec{v}$ is also an eigenvector for any scalar $c \\neq 0$\n2. **Eigenvalues are unique**: Each eigenvector corresponds to exactly one eigenvalue\n3. **Eigenvectors can be complex**: Even for real matrices, eigenvalues and eigenvectors can be complex numbers\n4. **Number of eigenvalues**: An $n \\times n$ matrix has exactly $n$ eigenvalues (counting multiplicities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Example matrix\nA = np.array([[4, -2], [1, 1]])\nprint(\"Matrix A:\")\nprint(A)\n\n# Find eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\nprint(f\"\\nEigenvalues: {eigenvalues}\")\nprint(f\"Eigenvectors:\")\nprint(eigenvectors)\n\n# Verify the eigenvalue equation: Av = λv\nfor i, (eigenvalue, eigenvector) in enumerate(zip(eigenvalues, eigenvectors.T)):\n    Av = A @ eigenvector\n    lambda_v = eigenvalue * eigenvector\n    print(f\"\\nEigenvalue {i+1}: {eigenvalue:.4f}\")\n    print(f\"Eigenvector {i+1}: {eigenvector}\")\n    print(f\"A × v: {Av}\")\n    print(f\"λ × v: {lambda_v}\")\n    print(f\"Are they equal? {np.allclose(Av, lambda_v)}\")\n    print(f\"Relative error: {np.linalg.norm(Av - lambda_v) / np.linalg.norm(lambda_v):.2e}\")\n\n# Geometric interpretation\nprint(f\"\\nGeometric interpretation:\")\nfor i, (eigenvalue, eigenvector) in enumerate(zip(eigenvalues, eigenvectors.T)):\n    if eigenvalue > 0:\n        if abs(eigenvalue) > 1:\n            print(f\"Eigenvector {i+1}: Stretched by factor {abs(eigenvalue):.2f}\")\n        else:\n            print(f\"Eigenvector {i+1}: Contracted by factor {abs(eigenvalue):.2f}\")\n    else:\n        print(f\"Eigenvector {i+1}: Flipped and scaled by factor {abs(eigenvalue):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finding Eigenvalues and Eigenvectors\n\n### Characteristic Equation\n\nThe eigenvalues are solutions to the characteristic equation:\n$$\\det(A - \\lambda I) = 0$$\n\nThis is a polynomial equation of degree $n$ in $\\lambda$, called the characteristic polynomial.\n\n### Step-by-Step Process\n\n1. **Form the matrix** $A - \\lambda I$\n2. **Compute the determinant** $\\det(A - \\lambda I)$\n3. **Set equal to zero** and solve for $\\lambda$\n4. **For each eigenvalue** $\\lambda_i$, solve $(A - \\lambda_i I)\\vec{v} = \\vec{0}$ for $\\vec{v}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_eigenvalues_manually(A):\n    \"\"\"Find eigenvalues by solving the characteristic equation\"\"\"\n    n = A.shape[0]\n    \n    # For 2x2 matrices, we can solve analytically\n    if n == 2:\n        a, b = A[0, 0], A[0, 1]\n        c, d = A[1, 0], A[1, 1]\n        \n        # Characteristic equation: λ² - (a+d)λ + (ad-bc) = 0\n        trace = a + d\n        det = a * d - b * c\n        \n        # Quadratic formula\n        discriminant = trace**2 - 4 * det\n        if discriminant >= 0:\n            lambda1 = (trace + np.sqrt(discriminant)) / 2\n            lambda2 = (trace - np.sqrt(discriminant)) / 2\n            return np.array([lambda1, lambda2])\n        else:\n            # Complex eigenvalues\n            real_part = trace / 2\n            imag_part = np.sqrt(-discriminant) / 2\n            return np.array([complex(real_part, imag_part), complex(real_part, -imag_part)])\n    \n    # For larger matrices, use numerical methods\n    return np.linalg.eigvals(A)\n\ndef find_eigenvectors_manually(A, eigenvalues):\n    \"\"\"Find eigenvectors for given eigenvalues\"\"\"\n    eigenvectors = []\n    \n    for eigenvalue in eigenvalues:\n        # Solve (A - λI)v = 0\n        B = A - eigenvalue * np.eye(A.shape[0])\n        \n        # Use SVD to find null space\n        U, S, Vt = np.linalg.svd(B)\n        \n        # Find vectors in null space (corresponding to zero singular values)\n        tol = 1e-10\n        null_space_indices = np.where(S < tol)[0]\n        \n        if len(null_space_indices) > 0:\n            # Take the first vector in the null space\n            eigenvector = Vt[null_space_indices[0]]\n            eigenvectors.append(eigenvector)\n        else:\n            # If no exact null space, take the vector with smallest singular value\n            eigenvector = Vt[-1]\n            eigenvectors.append(eigenvector)\n    \n    return np.array(eigenvectors).T\n\n# Test manual eigenvalue/eigenvector calculation\nA = np.array([[4, -2], [1, 1]])\nprint(\"Matrix A:\")\nprint(A)\n\n# Manual calculation\neigenvalues_manual = find_eigenvalues_manually(A)\neigenvectors_manual = find_eigenvectors_manually(A, eigenvalues_manual)\n\nprint(f\"\\nManual calculation:\")\nprint(f\"Eigenvalues: {eigenvalues_manual}\")\nprint(f\"Eigenvectors:\")\nprint(eigenvectors_manual)\n\n# Compare with NumPy\neigenvalues_numpy, eigenvectors_numpy = np.linalg.eig(A)\nprint(f\"\\nNumPy calculation:\")\nprint(f\"Eigenvalues: {eigenvalues_numpy}\")\nprint(f\"Eigenvectors:\")\nprint(eigenvectors_numpy)\n\n# Verify both methods give same results\nprint(f\"\\nEigenvalues match: {np.allclose(eigenvalues_manual, eigenvalues_numpy)}\")\nprint(f\"Eigenvectors match: {np.allclose(np.abs(eigenvectors_manual), np.abs(eigenvectors_numpy))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Properties of Eigenvalues and Eigenvectors\n\n### Basic Properties\n\n1. **Trace and Sum**: $\\text{tr}(A) = \\sum_{i=1}^{n} \\lambda_i$\n2. **Determinant and Product**: $\\det(A) = \\prod_{i=1}^{n} \\lambda_i$\n3. **Powers**: If $\\lambda$ is an eigenvalue of $A$, then $\\lambda^k$ is an eigenvalue of $A^k$\n4. **Inverse**: If $\\lambda$ is an eigenvalue of $A$, then $\\frac{1}{\\lambda}$ is an eigenvalue of $A^{-1}$ (if $A$ is invertible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_eigenvalue_properties(A):\n    \"\"\"Verify fundamental eigenvalue properties\"\"\"\n    eigenvalues, eigenvectors = np.linalg.eig(A)\n    \n    print(\"Eigenvalue Properties Verification:\")\n    print(f\"Matrix A:\\n{A}\")\n    print(f\"Eigenvalues: {eigenvalues}\")\n    \n    # Property 1: Trace equals sum of eigenvalues\n    trace_A = np.trace(A)\n    sum_eigenvalues = np.sum(eigenvalues)\n    print(f\"\\n1. Trace property:\")\n    print(f\"   tr(A) = {trace_A:.4f}\")\n    print(f\"   Sum of eigenvalues = {sum_eigenvalues:.4f}\")\n    print(f\"   Match: {np.isclose(trace_A, sum_eigenvalues)}\")\n    \n    # Property 2: Determinant equals product of eigenvalues\n    det_A = np.linalg.det(A)\n    prod_eigenvalues = np.prod(eigenvalues)\n    print(f\"\\n2. Determinant property:\")\n    print(f\"   det(A) = {det_A:.4f}\")\n    print(f\"   Product of eigenvalues = {prod_eigenvalues:.4f}\")\n    print(f\"   Match: {np.isclose(det_A, prod_eigenvalues)}\")\n    \n    # Property 3: Powers of eigenvalues\n    A_squared = A @ A\n    eigenvalues_squared, _ = np.linalg.eig(A_squared)\n    eigenvalues_powered = eigenvalues**2\n    print(f\"\\n3. Power property:\")\n    print(f\"   Eigenvalues of A²: {eigenvalues_squared}\")\n    print(f\"   Eigenvalues²: {eigenvalues_powered}\")\n    print(f\"   Match: {np.allclose(eigenvalues_squared, eigenvalues_powered)}\")\n    \n    # Property 4: Inverse eigenvalues (if A is invertible)\n    if np.linalg.det(A) != 0:\n        A_inv = np.linalg.inv(A)\n        eigenvalues_inv, _ = np.linalg.eig(A_inv)\n        eigenvalues_reciprocal = 1 / eigenvalues\n        print(f\"\\n4. Inverse property:\")\n        print(f\"   Eigenvalues of A⁻¹: {eigenvalues_inv}\")\n        print(f\"   1/eigenvalues: {eigenvalues_reciprocal}\")\n        print(f\"   Match: {np.allclose(eigenvalues_inv, eigenvalues_reciprocal)}\")\n    \n    return eigenvalues, eigenvectors\n\n# Test with different matrices\nA1 = np.array([[2, 1], [1, 3]])\nA2 = np.array([[4, -2], [1, 1]])\nA3 = np.array([[1, 2], [3, 4]])\n\nfor i, A in enumerate([A1, A2, A3]):\n    print(f\"\\n{'='*50}\")\n    print(f\"Matrix {i+1}:\")\n    verify_eigenvalue_properties(A)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Eigenvalues of Special Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_special_matrices():\n    \"\"\"Analyze eigenvalues of special matrix types\"\"\"\n    \n    # Identity matrix\n    I = np.eye(3)\n    eigenvals_I = np.linalg.eigvals(I)\n    print(\"Identity matrix eigenvalues:\")\n    print(f\"Matrix:\\n{I}\")\n    print(f\"Eigenvalues: {eigenvals_I}\")\n    print(f\"All eigenvalues = 1: {np.allclose(eigenvals_I, 1)}\")\n    \n    # Diagonal matrix\n    D = np.diag([1, 2, 3])\n    eigenvals_D = np.linalg.eigvals(D)\n    print(f\"\\nDiagonal matrix eigenvalues:\")\n    print(f\"Matrix:\\n{D}\")\n    print(f\"Eigenvalues: {eigenvals_D}\")\n    print(f\"Eigenvalues = diagonal elements: {np.allclose(eigenvals_D, [1, 2, 3])}\")\n    \n    # Triangular matrix\n    T = np.array([[1, 2, 3], [0, 4, 5], [0, 0, 6]])\n    eigenvals_T = np.linalg.eigvals(T)\n    print(f\"\\nTriangular matrix eigenvalues:\")\n    print(f\"Matrix:\\n{T}\")\n    print(f\"Eigenvalues: {eigenvals_T}\")\n    print(f\"Eigenvalues = diagonal elements: {np.allclose(eigenvals_T, [1, 4, 6])}\")\n    \n    # Symmetric matrix\n    S = np.array([[2, 1, 0], [1, 3, 1], [0, 1, 2]])\n    eigenvals_S, eigenvecs_S = np.linalg.eigh(S)  # Use eigh for symmetric matrices\n    print(f\"\\nSymmetric matrix eigenvalues:\")\n    print(f\"Matrix:\\n{S}\")\n    print(f\"Eigenvalues: {eigenvals_S}\")\n    print(f\"All eigenvalues real: {np.all(np.isreal(eigenvals_S))}\")\n    \n    # Orthogonal matrix (rotation)\n    theta = np.pi/4\n    R = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n    eigenvals_R = np.linalg.eigvals(R)\n    print(f\"\\nOrthogonal matrix (rotation) eigenvalues:\")\n    print(f\"Matrix:\\n{R}\")\n    print(f\"Eigenvalues: {eigenvals_R}\")\n    print(f\"Magnitude of eigenvalues = 1: {np.allclose(np.abs(eigenvals_R), 1)}\")\n    \n    # Nilpotent matrix\n    N = np.array([[0, 1], [0, 0]])\n    eigenvals_N = np.linalg.eigvals(N)\n    print(f\"\\nNilpotent matrix eigenvalues:\")\n    print(f\"Matrix:\\n{N}\")\n    print(f\"Eigenvalues: {eigenvals_N}\")\n    print(f\"All eigenvalues = 0: {np.allclose(eigenvals_N, 0)}\")\n\nanalyze_special_matrices()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagonalization\n\n### Mathematical Definition\n\nA matrix $A$ is **diagonalizable** if it can be written as:\n$$A = PDP^{-1}$$\n\nwhere:\n- $P$ is an invertible matrix whose columns are eigenvectors of $A$\n- $D$ is a diagonal matrix whose diagonal elements are the corresponding eigenvalues\n- $P^{-1}$ is the inverse of $P$\n\n### Conditions for Diagonalization\n\nA matrix is diagonalizable if and only if:\n1. It has $n$ linearly independent eigenvectors (where $n$ is the size of the matrix)\n2. The geometric multiplicity equals the algebraic multiplicity for each eigenvalue\n\n### Geometric Interpretation\n\nDiagonalization represents a change of basis to the \"eigenvector basis\" where the matrix becomes diagonal. This makes many operations much simpler:\n\n- **Powers**: $A^k = PD^kP^{-1}$\n- **Exponential**: $e^A = Pe^DP^{-1}$\n- **Functions**: $f(A) = Pf(D)P^{-1}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_diagonalization(A):\n    \"\"\"Analyze diagonalization of a matrix\"\"\"\n    eigenvalues, eigenvectors = np.linalg.eig(A)\n    \n    print(\"Diagonalization Analysis:\")\n    print(f\"Matrix A:\\n{A}\")\n    print(f\"Eigenvalues: {eigenvalues}\")\n    print(f\"Eigenvectors:\\n{eigenvectors}\")\n    \n    # Check if matrix is diagonalizable\n    det_P = np.linalg.det(eigenvectors)\n    is_diagonalizable = abs(det_P) > 1e-10\n    \n    print(f\"\\nDeterminant of eigenvector matrix: {det_P:.6f}\")\n    print(f\"Matrix is diagonalizable: {is_diagonalizable}\")\n    \n    if is_diagonalizable:\n        # Perform diagonalization\n        P = eigenvectors\n        D = np.diag(eigenvalues)\n        P_inv = np.linalg.inv(P)\n        \n        # Reconstruct A\n        A_reconstructed = P @ D @ P_inv\n        \n        print(f\"\\nDiagonalization:\")\n        print(f\"P (eigenvectors):\\n{P}\")\n        print(f\"D (eigenvalues):\\n{D}\")\n        print(f\"P⁻¹:\\n{P_inv}\")\n        print(f\"A reconstructed: P @ D @ P⁻¹\\n{A_reconstructed}\")\n        print(f\"Original A:\\n{A}\")\n        print(f\"Reconstruction successful: {np.allclose(A, A_reconstructed)}\")\n        \n        # Demonstrate power calculation\n        k = 3\n        A_power_direct = np.linalg.matrix_power(A, k)\n        A_power_diagonal = P @ np.linalg.matrix_power(D, k) @ P_inv\n        \n        print(f\"\\nPower calculation (A^{k}):\")\n        print(f\"Direct calculation:\\n{A_power_direct}\")\n        print(f\"Using diagonalization:\\n{A_power_diagonal}\")\n        print(f\"Results match: {np.allclose(A_power_direct, A_power_diagonal)}\")\n        \n        return P, D, P_inv\n    else:\n        print(\"Matrix is not diagonalizable\")\n        return None, None, None\n\n# Test diagonalization with different matrices\nA_diagonalizable = np.array([[4, -2], [1, 1]])\nA_non_diagonalizable = np.array([[1, 1], [0, 1]])  # Jordan block\n\nprint(\"=\"*60)\nprint(\"Diagonalizable matrix:\")\nP1, D1, P1_inv = analyze_diagonalization(A_diagonalizable)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Non-diagonalizable matrix:\")\nP2, D2, P2_inv = analyze_diagonalization(A_non_diagonalizable)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Power Method\n\nThe power method is an iterative algorithm to find the dominant eigenvalue and eigenvector of a matrix.\n\n### Algorithm\n\n1. Start with a random vector $\\vec{v}_0$\n2. Iterate: $\\vec{v}_{k+1} = \\frac{A\\vec{v}_k}{\\|A\\vec{v}_k\\|}$\n3. The eigenvalue is approximated by: $\\lambda \\approx \\frac{\\vec{v}_k^T A \\vec{v}_k}{\\vec{v}_k^T \\vec{v}_k}$\n\n### Convergence\n\nThe power method converges to the eigenvalue with the largest magnitude, provided:\n- The matrix has a unique dominant eigenvalue\n- The initial vector has a non-zero component in the direction of the dominant eigenvector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def power_method(A, max_iter=100, tol=1e-6, verbose=True):\n    \"\"\"Power method to find dominant eigenvalue and eigenvector\"\"\"\n    n = A.shape[0]\n    \n    # Initialize random vector\n    v = np.random.randn(n)\n    v = v / np.linalg.norm(v)\n    \n    eigenvalues_history = []\n    eigenvectors_history = []\n    \n    for i in range(max_iter):\n        v_old = v.copy()\n        \n        # Apply matrix transformation\n        v_new = A @ v\n        \n        # Normalize\n        v = v_new / np.linalg.norm(v_new)\n        \n        # Calculate eigenvalue approximation\n        eigenvalue = (v.T @ A @ v) / (v.T @ v)\n        eigenvalues_history.append(eigenvalue)\n        eigenvectors_history.append(v.copy())\n        \n        # Check convergence\n        if np.linalg.norm(v - v_old) < tol:\n            if verbose:\n                print(f\"Converged after {i+1} iterations\")\n            break\n    \n    if verbose:\n        print(f\"Final eigenvalue: {eigenvalue:.6f}\")\n        print(f\"Final eigenvector: {v}\")\n    \n    return eigenvalue, v, eigenvalues_history, eigenvectors_history\n\ndef inverse_power_method(A, shift=0, max_iter=100, tol=1e-6):\n    \"\"\"Inverse power method to find eigenvalue closest to shift\"\"\"\n    n = A.shape[0]\n    \n    # Initialize random vector\n    v = np.random.randn(n)\n    v = v / np.linalg.norm(v)\n    \n    # Shift the matrix\n    A_shifted = A - shift * np.eye(n)\n    \n    for i in range(max_iter):\n        v_old = v.copy()\n        \n        # Solve linear system (A - shift*I)v_new = v\n        v_new = np.linalg.solve(A_shifted, v)\n        \n        # Normalize\n        v = v_new / np.linalg.norm(v_new)\n        \n        # Check convergence\n        if np.linalg.norm(v - v_old) < tol:\n            break\n    \n    # Calculate eigenvalue\n    eigenvalue = (v.T @ A @ v) / (v.T @ v)\n    \n    return eigenvalue, v\n\n# Test power method\nA = np.array([[4, -2], [1, 1]])\nprint(\"Testing Power Method:\")\nprint(f\"Matrix A:\\n{A}\")\n\n# Power method\ndominant_eigenvalue, dominant_eigenvector, history_eigenvalues, history_eigenvectors = power_method(A)\n\n# Compare with exact values\nexact_eigenvalues, exact_eigenvectors = np.linalg.eig(A)\ndominant_exact_idx = np.argmax(np.abs(exact_eigenvalues))\nexact_dominant_eigenvalue = exact_eigenvalues[dominant_exact_idx]\nexact_dominant_eigenvector = exact_eigenvectors[:, dominant_exact_idx]\n\nprint(f\"\\nComparison with exact values:\")\nprint(f\"Power method eigenvalue: {dominant_eigenvalue:.6f}\")\nprint(f\"Exact dominant eigenvalue: {exact_dominant_eigenvalue:.6f}\")\nprint(f\"Relative errors: {abs(dominant_eigenvalue - exact_dominant_eigenvalue) / abs(exact_dominant_eigenvalue):.2e}\")\n\nprint(f\"\\nPower method eigenvector: {dominant_eigenvector}\")\nprint(f\"Exact dominant eigenvector: {exact_dominant_eigenvector}\")\nprint(f\"Eigenvector alignment: {abs(np.dot(dominant_eigenvector, exact_dominant_eigenvector)):.6f}\")\n\n# Test inverse power method\nsmallest_eigenvalue, smallest_eigenvector = inverse_power_method(A)\nsmallest_exact_idx = np.argmin(np.abs(exact_eigenvalues))\nexact_smallest_eigenvalue = exact_eigenvalues[smallest_exact_idx]\n\nprint(f\"\\nInverse power method:\")\nprint(f\"Smallest eigenvalue: {smallest_eigenvalue:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applications in Machine Learning\n\n### Principal Component Analysis (PCA)\n\nPCA uses eigenvectors of the covariance matrix to find the principal components (directions of maximum variance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pca_analysis():\n    \"\"\"Demonstrate PCA using eigenvalues and eigenvectors\"\"\"\n    \n    # Generate sample data\n    np.random.seed(42)\n    n_samples = 100\n    n_features = 3\n    \n    # Create correlated data\n    mean = [0, 0, 0]\n    cov = [[1, 0.8, 0.6], \n           [0.8, 1, 0.7], \n           [0.6, 0.7, 1]]\n    \n    data = np.random.multivariate_normal(mean, cov, n_samples)\n    \n    # Center the data\n    data_centered = data - np.mean(data, axis=0)\n    \n    # Compute covariance matrix\n    cov_matrix = np.cov(data_centered.T)\n    \n    print(\"PCA Analysis:\")\n    print(f\"Data shape: {data.shape}\")\n    print(f\"Covariance matrix:\\n{cov_matrix}\")\n    \n    # Eigendecomposition\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    # Sort by eigenvalues (descending)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    \n    print(f\"\\nEigenvalues (variances): {eigenvalues}\")\n    print(f\"Eigenvectors (principal components):\\n{eigenvectors}\")\n    \n    # Explained variance\n    explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\n    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n    \n    print(f\"\\nExplained variance ratio: {explained_variance_ratio}\")\n    print(f\"Cumulative explained variance: {cumulative_variance_ratio}\")\n    \n    # Project data onto principal components\n    data_pca = data_centered @ eigenvectors\n    \n    print(f\"\\nTransformed data shape: {data_pca.shape}\")\n    print(f\"Variance in each component: {np.var(data_pca, axis=0)}\")\n    \n    # Verify that variances match eigenvalues\n    print(f\"Variance verification: {np.allclose(np.var(data_pca, axis=0), eigenvalues)}\")\n    \n    return data, data_pca, eigenvectors, eigenvalues\n\ndata, data_pca, eigenvectors, eigenvalues = pca_analysis()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spectral Clustering\n\nSpectral clustering uses eigenvectors of the Laplacian matrix to perform clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def spectral_clustering_demo():\n    \"\"\"Demonstrate spectral clustering using eigenvalues/eigenvectors\"\"\"\n    \n    # Generate sample data with clear clusters\n    np.random.seed(42)\n    n_samples = 200\n    \n    # Create two clusters\n    cluster1 = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], n_samples//2)\n    cluster2 = np.random.multivariate_normal([4, 4], [[1, -0.3], [-0.3, 1]], n_samples//2)\n    \n    data = np.vstack([cluster1, cluster2])\n    \n    # Compute similarity matrix (Gaussian kernel)\n    def gaussian_kernel(X, sigma=1.0):\n        n = X.shape[0]\n        K = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                diff = X[i] - X[j]\n                K[i, j] = np.exp(-np.dot(diff, diff) / (2 * sigma**2))\n        return K\n    \n    similarity_matrix = gaussian_kernel(data, sigma=1.0)\n    \n    # Compute Laplacian matrix\n    degree_matrix = np.diag(np.sum(similarity_matrix, axis=1))\n    laplacian_matrix = degree_matrix - similarity_matrix\n    \n    # Eigendecomposition of Laplacian\n    eigenvalues, eigenvectors = np.linalg.eigh(laplacian_matrix)\n    \n    # Sort eigenvalues and eigenvectors\n    sorted_indices = np.argsort(eigenvalues)\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    \n    print(\"Spectral Clustering Analysis:\")\n    print(f\"Data shape: {data.shape}\")\n    print(f\"Number of clusters: 2\")\n    print(f\"Laplacian eigenvalues: {eigenvalues[:5]}\")  # Show first 5\n    \n    # Use second eigenvector for clustering (first is constant)\n    cluster_indicator = eigenvectors[:, 1]\n    \n    # Simple clustering based on sign\n    clusters = (cluster_indicator > 0).astype(int)\n    \n    print(f\"\\nClustering results:\")\n    print(f\"Cluster sizes: {np.bincount(clusters)}\")\n    \n    return data, clusters, eigenvalues, eigenvectors\n\ndata_clusters, clusters, eigenvalues_laplacian, eigenvectors_laplacian = spectral_clustering_demo()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PageRank Algorithm\n\nPageRank uses the dominant eigenvector of the transition matrix to rank web pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pagerank_demo():\n    \"\"\"Demonstrate PageRank algorithm using eigenvalues/eigenvectors\"\"\"\n    \n    # Simple web graph (adjacency matrix)\n    # Pages: A, B, C, D\n    # A links to B, C\n    # B links to C\n    # C links to A\n    # D links to A, C\n    \n    adjacency_matrix = np.array([\n        [0, 1, 1, 0],  # A\n        [0, 0, 1, 0],  # B\n        [1, 0, 0, 0],  # C\n        [1, 0, 1, 0]   # D\n    ])\n    \n    # Create transition matrix\n    out_degrees = np.sum(adjacency_matrix, axis=1)\n    transition_matrix = np.zeros_like(adjacency_matrix, dtype=float)\n    \n    for i in range(adjacency_matrix.shape[0]):\n        if out_degrees[i] > 0:\n            transition_matrix[i] = adjacency_matrix[i] / out_degrees[i]\n        else:\n            # Handle dangling nodes (pages with no outlinks)\n            transition_matrix[i] = 1.0 / adjacency_matrix.shape[0]\n    \n    # Add damping factor (teleportation)\n    damping = 0.85\n    n_pages = adjacency_matrix.shape[0]\n    transition_matrix = damping * transition_matrix + (1 - damping) / n_pages\n    \n    print(\"PageRank Analysis:\")\n    print(f\"Transition matrix:\\n{transition_matrix}\")\n    \n    # Find dominant eigenvector (PageRank scores)\n    eigenvalues, eigenvectors = np.linalg.eig(transition_matrix)\n    \n    # Find eigenvector corresponding to eigenvalue 1\n    dominant_idx = np.argmin(np.abs(eigenvalues - 1))\n    pagerank_scores = np.real(eigenvectors[:, dominant_idx])\n    pagerank_scores = pagerank_scores / np.sum(pagerank_scores)  # Normalize\n    \n    print(f\"\\nPageRank scores:\")\n    pages = ['A', 'B', 'C', 'D']\n    for page, score in zip(pages, pagerank_scores):\n        print(f\"Page {page}: {score:.4f}\")\n    \n    # Rank pages\n    page_rankings = np.argsort(pagerank_scores)[::-1]\n    print(f\"\\nPage rankings (highest to lowest):\")\n    for i, rank in enumerate(page_rankings):\n        print(f\"{i+1}. Page {pages[rank]} (score: {pagerank_scores[rank]:.4f})\")\n    \n    return transition_matrix, pagerank_scores\n\ntransition_matrix, pagerank_scores = pagerank_demo()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization of Eigenvalues and Eigenvectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_eigenvalues_eigenvectors():\n    \"\"\"Visualize eigenvalues and eigenvectors\"\"\"\n    \n    # Create a 2x2 matrix\n    A = np.array([[3, 1], [1, 2]])\n    eigenvalues, eigenvectors = np.linalg.eig(A)\n    \n    # Create visualization\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Plot 1: Original vectors and their transformations\n    ax1 = axes[0]\n    \n    # Unit vectors\n    e1 = np.array([1, 0])\n    e2 = np.array([0, 1])\n    \n    # Transform unit vectors\n    Ae1 = A @ e1\n    Ae2 = A @ e2\n    \n    # Plot original unit vectors\n    ax1.quiver(0, 0, e1[0], e1[1], angles='xy', scale_units='xy', scale=1, \n              color='blue', alpha=0.7, label='e₁', width=0.02)\n    ax1.quiver(0, 0, e2[0], e2[1], angles='xy', scale_units='xy', scale=1, \n              color='green', alpha=0.7, label='e₂', width=0.02)\n    \n    # Plot transformed unit vectors\n    ax1.quiver(0, 0, Ae1[0], Ae1[1], angles='xy', scale_units='xy', scale=1, \n              color='red', alpha=0.7, label='Ae₁', width=0.02)\n    ax1.quiver(0, 0, Ae2[0], Ae2[1], angles='xy', scale_units='xy', scale=1, \n              color='orange', alpha=0.7, label='Ae₂', width=0.02)\n    \n    # Plot eigenvectors\n    for i, (eigenvalue, eigenvector) in enumerate(zip(eigenvalues, eigenvectors.T)):\n        # Normalize eigenvector for visualization\n        eigenvector_norm = eigenvector / np.linalg.norm(eigenvector)\n        ax1.quiver(0, 0, eigenvector_norm[0], eigenvector_norm[1], angles='xy', scale_units='xy', scale=1, \n                  color='purple', alpha=0.9, label=f'Eigenvector {i+1} (λ={eigenvalue:.2f})', width=0.03)\n        \n        # Show eigenvalue scaling\n        scaled_eigenvector = eigenvalue * eigenvector_norm\n        ax1.quiver(0, 0, scaled_eigenvector[0], scaled_eigenvector[1], angles='xy', scale_units='xy', scale=1, \n                  color='magenta', alpha=0.7, label=f'λ{i+1}×eigenvector {i+1}', width=0.02)\n    \n    ax1.set_xlim(-4, 4)\n    ax1.set_ylim(-4, 4)\n    ax1.grid(True, alpha=0.3)\n    ax1.set_aspect('equal')\n    ax1.set_title('Matrix Transformation and Eigenvectors')\n    ax1.legend()\n    \n    # Plot 2: Eigenvalue spectrum\n    ax2 = axes[1]\n    \n    # Plot eigenvalues in complex plane\n    real_parts = np.real(eigenvalues)\n    imag_parts = np.imag(eigenvalues)\n    \n    ax2.scatter(real_parts, imag_parts, c=['red', 'blue'], s=100, alpha=0.7)\n    \n    # Add unit circle for reference\n    theta = np.linspace(0, 2*np.pi, 100)\n    circle_x = np.cos(theta)\n    circle_y = np.sin(theta)\n    ax2.plot(circle_x, circle_y, 'k--', alpha=0.3, label='Unit circle')\n    \n    # Label eigenvalues\n    for i, (real, imag) in enumerate(zip(real_parts, imag_parts)):\n        ax2.annotate(f'λ{i+1}={real:.2f}+{imag:.2f}i', \n                    (real, imag), xytext=(5, 5), textcoords='offset points')\n    \n    ax2.set_xlim(-4, 4)\n    ax2.set_ylim(-4, 4)\n    ax2.grid(True, alpha=0.3)\n    ax2.set_aspect('equal')\n    ax2.set_title('Eigenvalue Spectrum')\n    ax2.set_xlabel('Real part')\n    ax2.set_ylabel('Imaginary part')\n    ax2.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_eigenvalues_eigenvectors()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n### Exercise 1: Eigenvalue Properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify eigenvalue properties for different matrices\ndef exercise_eigenvalue_properties():\n    matrices = {\n        \"Symmetric\": np.array([[2, 1], [1, 3]]),\n        \"Skew-symmetric\": np.array([[0, 1], [-1, 0]]),\n        \"Triangular\": np.array([[1, 2], [0, 3]]),\n        \"Random\": np.random.randn(2, 2)\n    }\n    \n    for name, A in matrices.items():\n        print(f\"\\n{name} matrix:\")\n        eigenvalues, eigenvectors = np.linalg.eig(A)\n        \n        # Verify trace property\n        trace_property = np.isclose(np.trace(A), np.sum(eigenvalues))\n        print(f\"  Trace property: {trace_property}\")\n        \n        # Verify determinant property\n        det_property = np.isclose(np.linalg.det(A), np.prod(eigenvalues))\n        print(f\"  Determinant property: {det_property}\")\n        \n        # Check if eigenvalues are real\n        real_eigenvalues = np.all(np.isreal(eigenvalues))\n        print(f\"  Real eigenvalues: {real_eigenvalues}\")\n\nexercise_eigenvalue_properties()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Power Method Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement and test power method variations\ndef exercise_power_method():\n    A = np.array([[4, -2], [1, 1]])\n    \n    # Standard power method\n    lambda1, v1, _, _ = power_method(A, verbose=False)\n    \n    # Inverse power method\n    lambda2, v2 = inverse_power_method(A, shift=0)\n    \n    # Exact values\n    exact_eigenvalues, exact_eigenvectors = np.linalg.eig(A)\n    \n    print(\"Power Method Exercise:\")\n    print(f\"Matrix A:\\n{A}\")\n    print(f\"Exact eigenvalues: {exact_eigenvalues}\")\n    print(f\"Power method (largest): {lambda1:.6f}\")\n    print(f\"Inverse power method (smallest): {lambda2:.6f}\")\n    print(f\"Relative errors: {abs(lambda1 - max(exact_eigenvalues)) / abs(max(exact_eigenvalues)):.2e}, {abs(lambda2 - min(exact_eigenvalues)) / abs(min(exact_eigenvalues)):.2e}\")\n\nexercise_power_method()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Diagonalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test diagonalization with different matrices\ndef exercise_diagonalization():\n    matrices = {\n        \"Diagonalizable\": np.array([[4, -2], [1, 1]]),\n        \"Non-diagonalizable\": np.array([[1, 1], [0, 1]]),\n        \"Symmetric\": np.array([[2, 1], [1, 3]])\n    }\n    \n    for name, A in matrices.items():\n        print(f\"\\n{name} matrix:\")\n        eigenvalues, eigenvectors = np.linalg.eig(A)\n        det_P = np.linalg.det(eigenvectors)\n        \n        print(f\"  Eigenvalues: {eigenvalues}\")\n        print(f\"  Det(P): {det_P:.6f}\")\n        print(f\"  Diagonalizable: {abs(det_P) > 1e-10}\")\n        \n        if abs(det_P) > 1e-10:\n            P = eigenvectors\n            D = np.diag(eigenvalues)\n            P_inv = np.linalg.inv(P)\n            A_reconstructed = P @ D @ P_inv\n            reconstruction_success = np.allclose(A, A_reconstructed)\n            print(f\"  Reconstruction successful: {reconstruction_success}\")\n\nexercise_diagonalization()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nIn this chapter, we've covered:\n\n1. **Eigenvalue/Eigenvector Fundamentals**: Definition, geometric interpretation, and mathematical properties\n2. **Finding Eigenvalues/Eigenvectors**: Characteristic equation, manual calculation, and numerical methods\n3. **Properties**: Trace, determinant, powers, and special matrix properties\n4. **Diagonalization**: Conditions, process, and applications for simplifying matrix operations\n5. **Power Method**: Iterative algorithm for finding dominant eigenvalues and eigenvectors\n6. **AI/ML Applications**: PCA, spectral clustering, and PageRank algorithm\n7. **Visualization**: Geometric interpretation and eigenvalue spectrum analysis\n\n### Key Takeaways:\n- Eigenvalues and eigenvectors reveal the fundamental structure of matrices\n- Eigenvectors provide a natural coordinate system for understanding matrix transformations\n- Diagonalization simplifies many matrix operations and calculations\n- Eigenvalues and eigenvectors are fundamental to many machine learning algorithms\n- Understanding these concepts is crucial for advanced linear algebra and AI/ML applications\n\n### Next Steps:\nIn the next chapter, we'll explore vector spaces and subspaces, understanding the mathematical foundations of linear algebra and how they relate to eigenvalues and eigenvectors."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}