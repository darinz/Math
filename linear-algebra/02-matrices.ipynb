{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Matrices and Matrix Operations\n\n[![Chapter](https://img.shields.io/badge/Chapter-2-blue.svg)]()\n[![Topic](https://img.shields.io/badge/Topic-Matrices-green.svg)]()\n[![Difficulty](https://img.shields.io/badge/Difficulty-Beginner-brightgreen.svg)]()\n\n## Introduction\n\nMatrices are rectangular arrays of numbers that represent linear transformations and systems of linear equations. They are fundamental in machine learning for representing data, transformations, and computations. Understanding matrices is crucial for grasping linear algebra concepts and their applications in AI/ML.\n\n### Why Matrices Matter in AI/ML\n\n1. **Data Representation**: Datasets are often represented as matrices where rows are samples and columns are features\n2. **Linear Transformations**: Matrices encode how vectors are transformed in space\n3. **Neural Networks**: Weight matrices connect layers and transform activations\n4. **Optimization**: Hessian matrices, covariance matrices, and other second-order information\n5. **Dimensionality Reduction**: PCA, SVD, and other techniques work with matrices\n6. **Systems of Equations**: Linear regression, least squares, and other ML problems\n\n## What is a Matrix?\n\nA matrix is a 2D array of numbers arranged in rows and columns. An m×n matrix has m rows and n columns, representing a linear transformation from $\\mathbb{R}^n$ to $\\mathbb{R}^m$.\n\n### Mathematical Notation\n\nA matrix $A$ of size $m \\times n$ is written as:\n$$A = \\begin{bmatrix} \na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}$$\n\nWhere:\n- $a_{ij}$ is the element in the $i$-th row and $j$-th column\n- $i$ ranges from 1 to $m$ (rows)\n- $j$ ranges from 1 to $n$ (columns)\n\n### Matrix as Linear Transformation\n\nA matrix $A$ represents a linear transformation $T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ such that:\n$$T(\\vec{x}) = A\\vec{x}$$\n\nThis means:\n- Input: Vector $\\vec{x} \\in \\mathbb{R}^n$\n- Output: Vector $\\vec{y} = A\\vec{x} \\in \\mathbb{R}^m$\n- The transformation is linear: $T(c\\vec{x} + \\vec{y}) = cT(\\vec{x}) + T(\\vec{y})$\n\n### Python Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Creating matrices\nA = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\nB = np.array([[9, 8, 7],\n              [6, 5, 4],\n              [3, 2, 1]])\n\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nShape of A:\", A.shape)\nprint(\"Number of rows:\", A.shape[0])\nprint(\"Number of columns:\", A.shape[1])\nprint(\"Total elements:\", A.size)\nprint(\"Data type:\", A.dtype)\n\nprint(\"\\nMatrix B:\")\nprint(B)\n\n# Creating matrices with different methods\nzeros_matrix = np.zeros((3, 4))  # 3×4 zero matrix\nones_matrix = np.ones((2, 3))    # 2×3 matrix of ones\nrandom_matrix = np.random.randn(3, 3)  # 3×3 random matrix\nidentity_matrix = np.eye(4)      # 4×4 identity matrix\n\nprint(\"\\nZero matrix (3×4):\")\nprint(zeros_matrix)\nprint(\"\\nOnes matrix (2×3):\")\nprint(ones_matrix)\nprint(\"\\nRandom matrix (3×3):\")\nprint(random_matrix)\nprint(\"\\nIdentity matrix (4×4):\")\nprint(identity_matrix)\n\n# Matrix indexing and slicing\nprint(\"\\nFirst row of A:\", A[0, :])\nprint(\"Second column of A:\", A[:, 1])\nprint(\"Element at position (1, 2):\", A[1, 2])\nprint(\"Submatrix (first 2 rows, first 2 columns):\")\nprint(A[:2, :2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix Operations\n\n### Addition and Subtraction\n\n**Mathematical Definition:**\nMatrices are added/subtracted element-wise (must have same dimensions):\n$$(A + B)_{ij} = A_{ij} + B_{ij}$$\n$$(A - B)_{ij} = A_{ij} - B_{ij}$$\n\n**Properties:**\n- **Commutative**: $A + B = B + A$\n- **Associative**: $(A + B) + C = A + (B + C)$\n- **Identity**: $A + 0 = A$ (where 0 is the zero matrix)\n- **Inverse**: $A + (-A) = 0$\n\n**Geometric Interpretation:**\n- Matrix addition corresponds to adding the corresponding linear transformations\n- $(A + B)\\vec{x} = A\\vec{x} + B\\vec{x}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrix addition\nC = A + B\nprint(\"A + B:\")\nprint(C)\n\n# Matrix subtraction\nD = A - B\nprint(\"\\nA - B:\")\nprint(D)\n\n# Broadcasting with scalar\nE = A + 5\nprint(\"\\nA + 5:\")\nprint(E)\n\n# Verification of properties\nprint(\"\\nCommutative property (A + B == B + A):\", np.array_equal(A + B, B + A))\nprint(\"Associative property check:\", np.array_equal((A + B) + C, A + (B + C)))\n\n# Element-wise operations\nprint(\"\\nElement-wise addition (same as matrix addition):\")\nprint(np.add(A, B))\n\n# Adding matrices of different shapes (broadcasting)\nrow_vector = np.array([[1, 2, 3]])\ncolumn_vector = np.array([[1], [2], [3]])\n\nprint(\"\\nAdding row vector to each row of A:\")\nprint(A + row_vector)\nprint(\"\\nAdding column vector to each column of A:\")\nprint(A + column_vector)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scalar Multiplication\n\n**Mathematical Definition:**\nMultiplying a matrix by a scalar multiplies each element:\n$$(cA)_{ij} = c \\times A_{ij}$$\n\n**Properties:**\n- **Distributive over matrix addition**: $c(A + B) = cA + cB$\n- **Distributive over scalar addition**: $(c + d)A = cA + dA$\n- **Associative**: $(cd)A = c(dA)$\n- **Identity**: $1A = A$\n\n**Geometric Interpretation:**\n- Scalar multiplication scales the linear transformation\n- $(cA)\\vec{x} = c(A\\vec{x})$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scalar multiplication\nscaled_A = 2 * A\nprint(\"2 * A:\")\nprint(scaled_A)\n\n# Division\ndivided_A = A / 2\nprint(\"\\nA / 2:\")\nprint(divided_A)\n\n# Multiple scalar operations\nresult_chain = 2 * 3 * A\nprint(\"\\n2 * 3 * A:\")\nprint(result_chain)\n\n# Verification of properties\nprint(\"\\nDistributive over matrix addition:\", \n      np.array_equal(2 * (A + B), 2 * A + 2 * B))\nprint(\"Associative property:\", \n      np.array_equal((2 * 3) * A, 2 * (3 * A)))\n\n# Scaling effects on transformation\nscales = [0.5, 1, 2, -1]\nfor scale in scales:\n    scaled = scale * A\n    print(f\"\\n{scale} * A:\")\n    print(scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Matrix Multiplication\n\n**Mathematical Definition:**\nMatrix multiplication is defined as:\n$$(AB)_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}$$\n\nWhere $A$ is $m \\times n$ and $B$ is $n \\times p$, resulting in an $m \\times p$ matrix.\n\n**Key Points:**\n- The number of columns in $A$ must equal the number of rows in $B$\n- Matrix multiplication is **not commutative**: $AB \\neq BA$ in general\n- Matrix multiplication is **associative**: $(AB)C = A(BC)$\n- Matrix multiplication is **distributive**: $A(B + C) = AB + AC$\n\n**Geometric Interpretation:**\n- Matrix multiplication represents composition of linear transformations\n- $(AB)\\vec{x} = A(B\\vec{x})$: Apply transformation $B$ first, then $A$\n\n**Step-by-Step Process:**\n1. Take the $i$-th row of matrix $A$\n2. Take the $j$-th column of matrix $B$\n3. Compute the dot product of these vectors\n4. Place the result in position $(i, j)$ of the product matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrix multiplication\nAB = np.dot(A, B)\nprint(\"A × B:\")\nprint(AB)\n\n# Alternative syntax\nAB_alt = A @ B\nprint(\"\\nA @ B (same result):\")\nprint(AB_alt)\n\n# Manual calculation for first element\nmanual_first = sum(A[0, k] * B[k, 0] for k in range(A.shape[1]))\nprint(f\"\\nManual calculation of (AB)₁₁: {manual_first}\")\nprint(f\"NumPy result (AB)₁₁: {AB[0, 0]}\")\n\n# Verification of properties\nprint(\"\\nAssociative property check:\")\nC = np.random.randn(3, 3)\nprint(\"(AB)C == A(BC):\", np.allclose((A @ B) @ C, A @ (B @ C)))\n\nprint(\"\\nDistributive property check:\")\nprint(\"A(B + C) == AB + AC:\", np.allclose(A @ (B + C), A @ B + A @ C))\n\n# Non-commutativity demonstration\nBA = B @ A\nprint(\"\\nB × A:\")\nprint(BA)\nprint(\"A × B == B × A?\", np.array_equal(AB, BA))\n\n# Matrix-vector multiplication\nvector = np.array([1, 2, 3])\nresult = A @ vector\nprint(f\"\\nA × vector {vector}:\")\nprint(result)\n\n# Element-wise multiplication (Hadamard product)\nelement_wise = A * B\nprint(\"\\nElement-wise multiplication (A * B):\")\nprint(element_wise)\nprint(\"This is NOT matrix multiplication!\")\n\n# Matrix powers\nA_squared = A @ A\nA_cubed = A @ A @ A\nprint(\"\\nA²:\")\nprint(A_squared)\nprint(\"\\nA³:\")\nprint(A_cubed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Special Matrices\n\n### Identity Matrix\n\n**Mathematical Definition:**\nThe identity matrix $I_n$ is an $n \\times n$ matrix with 1s on the diagonal and 0s elsewhere:\n$$I_n = \\begin{bmatrix} \n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}$$\n\n**Properties:**\n- $AI = A$ and $IA = A$ for any matrix $A$ of appropriate size\n- $I$ represents the identity transformation: $I\\vec{x} = \\vec{x}$\n- $I$ is the multiplicative identity for matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identity matrix\nI = np.eye(3)\nprint(\"3×3 Identity matrix:\")\nprint(I)\n\n# Verify: AI = A\nresult = A @ I\nprint(\"\\nA × I = A:\")\nprint(result)\nprint(\"\\nIs A × I equal to A?\", np.array_equal(result, A))\n\n# Verify: IA = A\nresult2 = I @ A\nprint(\"\\nI × A = A:\")\nprint(result2)\nprint(\"\\nIs I × A equal to A?\", np.array_equal(result2, A))\n\n# Identity matrix as linear transformation\ntest_vector = np.array([1, 2, 3])\ntransformed = I @ test_vector\nprint(f\"\\nI × {test_vector} = {transformed}\")\nprint(\"Identity transformation preserves the vector:\", np.array_equal(test_vector, transformed))\n\n# Creating identity matrices of different sizes\nI_2 = np.eye(2)\nI_4 = np.eye(4)\nprint(\"\\n2×2 Identity matrix:\")\nprint(I_2)\nprint(\"\\n4×4 Identity matrix:\")\nprint(I_4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero Matrix\n\n**Mathematical Definition:**\nA zero matrix $0$ has all elements equal to zero.\n\n**Properties:**\n- $A + 0 = A$ for any matrix $A$\n- $0A = 0$ and $A0 = 0$ for any matrix $A$ of appropriate size\n- $0$ represents the zero transformation: $0\\vec{x} = \\vec{0}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero matrix\nZ = np.zeros((3, 3))\nprint(\"3×3 Zero matrix:\")\nprint(Z)\n\n# Properties verification\nprint(\"\\nA + 0 = A:\", np.array_equal(A + Z, A))\nprint(\"0 × A = 0:\", np.array_equal(Z @ A, Z))\nprint(\"A × 0 = 0:\", np.array_equal(A @ Z, Z))\n\n# Zero transformation\ntest_vector = np.array([1, 2, 3])\nzero_result = Z @ test_vector\nprint(f\"\\n0 × {test_vector} = {zero_result}\")\nprint(\"Zero transformation produces zero vector:\", np.array_equal(zero_result, np.zeros(3)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Diagonal Matrix\n\n**Mathematical Definition:**\nA diagonal matrix has non-zero elements only on the main diagonal:\n$$D = \\begin{bmatrix} \nd_1 & 0 & \\cdots & 0 \\\\\n0 & d_2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & d_n\n\\end{bmatrix}$$\n\n**Properties:**\n- Diagonal matrices commute: $D_1D_2 = D_2D_1$\n- Powers of diagonal matrices are easy to compute: $D^k = \\text{diag}(d_1^k, d_2^k, \\ldots, d_n^k)$\n- Diagonal matrices represent scaling transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagonal matrix\nD = np.diag([1, 2, 3])\nprint(\"Diagonal matrix:\")\nprint(D)\n\n# Extract diagonal from matrix\ndiagonal = np.diag(A)\nprint(\"\\nDiagonal of A:\")\nprint(diagonal)\n\n# Create diagonal matrix from vector\ndiag_from_vector = np.diag([5, 10, 15])\nprint(\"\\nDiagonal matrix from vector [5, 10, 15]:\")\nprint(diag_from_vector)\n\n# Properties of diagonal matrices\nD1 = np.diag([1, 2, 3])\nD2 = np.diag([4, 5, 6])\nprint(\"\\nD1:\")\nprint(D1)\nprint(\"\\nD2:\")\nprint(D2)\nprint(\"\\nD1 × D2:\")\nprint(D1 @ D2)\nprint(\"\\nD2 × D1:\")\nprint(D2 @ D1)\nprint(\"Diagonal matrices commute:\", np.array_equal(D1 @ D2, D2 @ D1))\n\n# Powers of diagonal matrices\nD_squared = D @ D\nD_cubed = D @ D @ D\nprint(\"\\nD²:\")\nprint(D_squared)\nprint(\"\\nD³:\")\nprint(D_cubed)\nprint(\"Manual calculation D² = diag([1², 2², 3²]):\", np.array_equal(D_squared, np.diag([1, 4, 9])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix Properties\n\n### Transpose\n\n**Mathematical Definition:**\nThe transpose of a matrix flips rows and columns:\n$$(A^T)_{ij} = A_{ji}$$\n\n**Properties:**\n- $(A^T)^T = A$ (transpose of transpose is original)\n- $(A + B)^T = A^T + B^T$\n- $(cA)^T = cA^T$\n- $(AB)^T = B^T A^T$ (important!)\n\n**Geometric Interpretation:**\n- Transpose relates to the adjoint transformation\n- For real matrices, transpose corresponds to reflecting across the main diagonal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrix transpose\nA_transpose = A.T\nprint(\"A:\")\nprint(A)\nprint(\"\\nAᵀ:\")\nprint(A_transpose)\n\n# Properties of transpose\nprint(\"\\n(Aᵀ)ᵀ = A:\", np.array_equal((A.T).T, A))\nprint(\"(A + B)ᵀ = Aᵀ + Bᵀ:\", np.array_equal((A + B).T, A.T + B.T))\nprint(\"(cA)ᵀ = cAᵀ:\", np.array_equal((2*A).T, 2*A.T))\nprint(\"(AB)ᵀ = BᵀAᵀ:\", np.array_equal((A @ B).T, B.T @ A.T))\n\n# Transpose of different matrix types\nrow_vector = np.array([[1, 2, 3]])\ncolumn_vector = row_vector.T\nprint(\"\\nRow vector:\", row_vector)\nprint(\"Column vector (transpose):\", column_vector)\nprint(\"Shape of row vector:\", row_vector.shape)\nprint(\"Shape of column vector:\", column_vector.shape)\n\n# Transpose and matrix-vector multiplication\nvector = np.array([1, 2, 3])\nresult1 = A @ vector\nresult2 = (vector @ A.T).T\nprint(f\"\\nA × {vector}:\")\nprint(result1)\nprint(f\"({vector} × Aᵀ)ᵀ:\")\nprint(result2)\nprint(\"Results are equal:\", np.array_equal(result1, result2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trace\n\n**Mathematical Definition:**\nThe trace is the sum of diagonal elements:\n$$\\text{tr}(A) = \\sum_{i=1}^{n} A_{ii}$$\n\n**Properties:**\n- $\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)$\n- $\\text{tr}(cA) = c \\cdot \\text{tr}(A)$\n- $\\text{tr}(AB) = \\text{tr}(BA)$ (cyclic property)\n- $\\text{tr}(A^T) = \\text{tr}(A)$\n\n**Applications:**\n- Trace is used in optimization (e.g., matrix calculus)\n- Trace appears in many ML algorithms (e.g., covariance matrices)\n- Trace is invariant under similarity transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrix trace\ntrace_A = np.trace(A)\nprint(\"Trace of A:\", trace_A)\n\n# Manual calculation\nmanual_trace = sum(A[i, i] for i in range(min(A.shape)))\nprint(\"Manual trace calculation:\", manual_trace)\n\n# Verification: tr(A + B) = tr(A) + tr(B)\ntrace_sum = np.trace(A + B)\ntrace_A_plus_trace_B = np.trace(A) + np.trace(B)\nprint(\"tr(A + B) = tr(A) + tr(B):\", trace_sum == trace_A_plus_trace_B)\n\n# Verification: tr(cA) = c·tr(A)\nc = 3\ntrace_scaled = np.trace(c * A)\ntrace_c_times_trace_A = c * np.trace(A)\nprint(f\"tr({c}A) = {c}·tr(A):\", trace_scaled == trace_c_times_trace_A)\n\n# Cyclic property: tr(AB) = tr(BA)\ntrace_AB = np.trace(A @ B)\ntrace_BA = np.trace(B @ A)\nprint(\"tr(AB) = tr(BA):\", trace_AB == trace_BA)\n\n# Trace of transpose\ntrace_A_transpose = np.trace(A.T)\nprint(\"tr(Aᵀ) = tr(A):\", trace_A == trace_A_transpose)\n\n# Trace in optimization context\n# Example: Trace of covariance matrix\ndata = np.random.randn(100, 3)  # 100 samples, 3 features\ncov_matrix = np.cov(data.T)\ntrace_cov = np.trace(cov_matrix)\nprint(f\"\\nTrace of covariance matrix: {trace_cov:.4f}\")\nprint(\"This represents the total variance in the data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix Types\n\n### Symmetric Matrix\n\n**Mathematical Definition:**\nA symmetric matrix satisfies $A = A^T$.\n\n**Properties:**\n- Eigenvalues are real\n- Eigenvectors can be chosen to be orthogonal\n- Symmetric matrices are diagonalizable\n- Common in statistics (covariance matrices, correlation matrices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create symmetric matrix\nS = np.array([[1, 2, 3],\n              [2, 4, 5],\n              [3, 5, 6]])\n\nprint(\"Symmetric matrix S:\")\nprint(S)\nprint(\"\\nS = Sᵀ:\", np.array_equal(S, S.T))\n\n# Verify symmetry property\nprint(\"Is S symmetric?\", np.array_equal(S, S.T))\n\n# Symmetric matrix from any matrix\nany_matrix = np.random.randn(3, 3)\nsymmetric_from_any = (any_matrix + any_matrix.T) / 2\nprint(\"\\nOriginal matrix:\")\nprint(any_matrix)\nprint(\"\\nSymmetric matrix created from it:\")\nprint(symmetric_from_any)\nprint(\"Is it symmetric?\", np.array_equal(symmetric_from_any, symmetric_from_any.T))\n\n# Covariance matrix example (always symmetric)\ndata = np.random.randn(50, 3)\ncov_matrix = np.cov(data.T)\nprint(\"\\nCovariance matrix (symmetric):\")\nprint(cov_matrix)\nprint(\"Is covariance matrix symmetric?\", np.array_equal(cov_matrix, cov_matrix.T))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Skew-Symmetric Matrix\n\n**Mathematical Definition:**\nA skew-symmetric matrix satisfies $A = -A^T$.\n\n**Properties:**\n- Diagonal elements are zero\n- Eigenvalues are purely imaginary or zero\n- Used in cross product representations and rotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create skew-symmetric matrix\nK = np.array([[0, 2, -3],\n              [-2, 0, 4],\n              [3, -4, 0]])\n\nprint(\"Skew-symmetric matrix K:\")\nprint(K)\nprint(\"\\nK = -Kᵀ:\", np.array_equal(K, -K.T))\n\n# Verify skew-symmetry\nprint(\"Is K skew-symmetric?\", np.array_equal(K, -K.T))\nprint(\"Diagonal elements are zero:\", np.allclose(np.diag(K), 0))\n\n# Skew-symmetric matrix from any matrix\nany_matrix = np.random.randn(3, 3)\nskew_from_any = (any_matrix - any_matrix.T) / 2\nprint(\"\\nOriginal matrix:\")\nprint(any_matrix)\nprint(\"\\nSkew-symmetric matrix created from it:\")\nprint(skew_from_any)\nprint(\"Is it skew-symmetric?\", np.array_equal(skew_from_any, -skew_from_any.T))\n\n# Cross product matrix (skew-symmetric)\ndef cross_product_matrix(v):\n    \"\"\"Create skew-symmetric matrix for cross product v × x\"\"\"\n    return np.array([[0, -v[2], v[1]],\n                     [v[2], 0, -v[0]],\n                     [-v[1], v[0], 0]])\n\nvector = np.array([1, 2, 3])\ncross_matrix = cross_product_matrix(vector)\nprint(f\"\\nCross product matrix for vector {vector}:\")\nprint(cross_matrix)\nprint(\"Is it skew-symmetric?\", np.array_equal(cross_matrix, -cross_matrix.T))\n\n# Verify cross product property\ntest_vector = np.array([4, 5, 6])\ncross_result1 = np.cross(vector, test_vector)\ncross_result2 = cross_matrix @ test_vector\nprint(f\"\\nCross product {vector} × {test_vector}:\")\nprint(\"Using np.cross:\", cross_result1)\nprint(\"Using matrix multiplication:\", cross_result2)\nprint(\"Results are equal:\", np.array_equal(cross_result1, cross_result2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Orthogonal Matrix\n\n**Mathematical Definition:**\nAn orthogonal matrix satisfies $Q^T Q = Q Q^T = I$.\n\n**Properties:**\n- Columns are orthonormal (orthogonal and unit length)\n- Rows are orthonormal\n- $Q^T = Q^{-1}$\n- Preserves lengths and angles: $|Q\\vec{x}| = |\\vec{x}|$\n- Used in QR decomposition, rotations, and reflections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create orthogonal matrix (rotation matrix)\ndef rotation_matrix_2d(angle):\n    \"\"\"Create 2D rotation matrix\"\"\"\n    c, s = np.cos(angle), np.sin(angle)\n    return np.array([[c, -s], [s, c]])\n\nQ = rotation_matrix_2d(np.pi/4)  # 45-degree rotation\nprint(\"Orthogonal matrix (rotation):\")\nprint(Q)\n\n# Verify orthogonality\nprint(\"\\nQᵀQ = I:\")\nprint(Q.T @ Q)\nprint(\"Is Q orthogonal?\", np.allclose(Q.T @ Q, np.eye(2)))\n\n# Verify Qᵀ = Q⁻¹\nQ_inverse = np.linalg.inv(Q)\nprint(\"\\nQ⁻¹:\")\nprint(Q_inverse)\nprint(\"Qᵀ:\")\nprint(Q.T)\nprint(\"Qᵀ = Q⁻¹:\", np.allclose(Q.T, Q_inverse))\n\n# Preserve lengths\ntest_vector = np.array([3, 4])\noriginal_length = np.linalg.norm(test_vector)\ntransformed_vector = Q @ test_vector\ntransformed_length = np.linalg.norm(transformed_vector)\nprint(f\"\\nOriginal vector: {test_vector}, length: {original_length:.4f}\")\nprint(f\"Transformed vector: {transformed_vector}, length: {transformed_length:.4f}\")\nprint(\"Length preserved:\", np.isclose(original_length, transformed_length))\n\n# Create random orthogonal matrix using QR decomposition\nrandom_matrix = np.random.randn(3, 3)\nQ_random, R = np.linalg.qr(random_matrix)\nprint(\"\\nRandom orthogonal matrix (from QR decomposition):\")\nprint(Q_random)\nprint(\"Is it orthogonal?\", np.allclose(Q_random.T @ Q_random, np.eye(3)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap visualization of matrices\ndef plot_matrix_heatmap(matrix, title, ax):\n    im = ax.imshow(matrix, cmap='viridis', aspect='auto')\n    ax.set_title(title)\n    \n    # Add text annotations\n    for i in range(matrix.shape[0]):\n        for j in range(matrix.shape[1]):\n            text = ax.text(j, i, f'{matrix[i, j]:.1f}',\n                          ha=\"center\", va=\"center\", color=\"white\")\n    \n    plt.colorbar(im, ax=ax)\n\n# Create subplots for different matrices\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\nplot_matrix_heatmap(A, 'Matrix A', axes[0, 0])\nplot_matrix_heatmap(A.T, 'Matrix Aᵀ (Transpose)', axes[0, 1])\nplot_matrix_heatmap(A @ B, 'Matrix A × B', axes[1, 0])\nplot_matrix_heatmap(A * B, 'Element-wise A * B', axes[1, 1])\n\nplt.tight_layout()\nplt.show()\n\n# 3D visualization of matrix transformation\ndef plot_matrix_transformation():\n    fig = plt.figure(figsize=(15, 5))\n    \n    # Original unit vectors\n    e1 = np.array([1, 0])\n    e2 = np.array([0, 1])\n    \n    # Transformed vectors\n    Ae1 = A[:2, :2] @ e1\n    Ae2 = A[:2, :2] @ e2\n    \n    # Plot 1: Original unit vectors\n    ax1 = fig.add_subplot(131)\n    ax1.quiver(0, 0, e1[0], e1[1], angles='xy', scale_units='xy', scale=1, color='red', label='e₁')\n    ax1.quiver(0, 0, e2[0], e2[1], angles='xy', scale_units='xy', scale=1, color='blue', label='e₂')\n    ax1.set_xlim(-1, 2)\n    ax1.set_ylim(-1, 2)\n    ax1.set_aspect('equal')\n    ax1.grid(True, alpha=0.3)\n    ax1.set_title('Original Unit Vectors')\n    ax1.legend()\n    \n    # Plot 2: Transformed vectors\n    ax2 = fig.add_subplot(132)\n    ax2.quiver(0, 0, Ae1[0], Ae1[1], angles='xy', scale_units='xy', scale=1, color='red', label='Ae₁')\n    ax2.quiver(0, 0, Ae2[0], Ae2[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Ae₂')\n    ax2.set_xlim(-1, 6)\n    ax2.set_ylim(-1, 6)\n    ax2.set_aspect('equal')\n    ax2.grid(True, alpha=0.3)\n    ax2.set_title('Transformed Vectors')\n    ax2.legend()\n    \n    # Plot 3: Unit square transformation\n    ax3 = fig.add_subplot(133)\n    # Original unit square\n    square_original = np.array([[0, 1, 1, 0, 0], [0, 0, 1, 1, 0]])\n    ax3.plot(square_original[0], square_original[1], 'b-', label='Original')\n    \n    # Transformed unit square\n    square_transformed = A[:2, :2] @ square_original\n    ax3.plot(square_transformed[0], square_transformed[1], 'r-', label='Transformed')\n    \n    ax3.set_xlim(-1, 6)\n    ax3.set_ylim(-1, 6)\n    ax3.set_aspect('equal')\n    ax3.grid(True, alpha=0.3)\n    ax3.set_title('Unit Square Transformation')\n    ax3.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_matrix_transformation()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applications in AI/ML\n\n### 1. Data Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset as matrix\n# Rows: samples, Columns: features\ndataset = np.random.randn(100, 5)  # 100 samples, 5 features\nprint(\"Dataset shape:\", dataset.shape)\nprint(\"First 5 samples:\")\nprint(dataset[:5])\n\n# Feature matrix for machine learning\nX = dataset[:, :-1]  # Features (first 4 columns)\ny = dataset[:, -1]   # Target (last column)\nprint(\"\\nFeature matrix X shape:\", X.shape)\nprint(\"Target vector y shape:\", y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Linear Transformations in Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple neural network layer\ndef neural_network_layer(input_data, weights, bias):\n    \"\"\"Forward pass through a neural network layer\"\"\"\n    return input_data @ weights + bias\n\n# Example: Single layer with 3 inputs, 2 outputs\ninput_size = 3\noutput_size = 2\nbatch_size = 4\n\n# Initialize weights and bias\nW = np.random.randn(input_size, output_size) * 0.1\nb = np.zeros(output_size)\n\n# Input data\nX = np.random.randn(batch_size, input_size)\n\n# Forward pass\noutput = neural_network_layer(X, W, b)\nprint(\"Input shape:\", X.shape)\nprint(\"Weight matrix shape:\", W.shape)\nprint(\"Bias vector shape:\", b.shape)\nprint(\"Output shape:\", output.shape)\nprint(\"\\nOutput:\")\nprint(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Covariance Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate covariance matrix\ndata = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], 1000)\ncov_matrix = np.cov(data.T)\n\nprint(\"Covariance matrix:\")\nprint(cov_matrix)\nprint(\"\\nVariance of first feature:\", cov_matrix[0, 0])\nprint(\"Variance of second feature:\", cov_matrix[1, 1])\nprint(\"Covariance between features:\", cov_matrix[0, 1])\n\n# Eigendecomposition of covariance matrix\neigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\nprint(\"\\nEigenvalues:\", eigenvalues)\nprint(\"Eigenvectors:\")\nprint(eigenvectors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n### Exercise 1: Matrix Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Given matrices A and B, compute various operations\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\n# Compute: 2A + 3B, A², A×B, B×A\nresult1 = 2*A + 3*B\nresult2 = A @ A\nresult3 = A @ B\nresult4 = B @ A\n\nprint(\"2A + 3B:\")\nprint(result1)\nprint(\"\\nA²:\")\nprint(result2)\nprint(\"\\nA×B:\")\nprint(result3)\nprint(\"\\nB×A:\")\nprint(result4)\nprint(\"\\nA×B == B×A?\", np.array_equal(result3, result4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Matrix Properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify matrix properties\nA = np.random.randn(3, 3)\nB = np.random.randn(3, 3)\nC = np.random.randn(3, 3)\n\n# Associative property: (AB)C = A(BC)\nleft_side = (A @ B) @ C\nright_side = A @ (B @ C)\nprint(\"(AB)C == A(BC):\", np.allclose(left_side, right_side))\n\n# Distributive property: A(B + C) = AB + AC\nleft_side = A @ (B + C)\nright_side = A @ B + A @ C\nprint(\"A(B + C) == AB + AC:\", np.allclose(left_side, right_side))\n\n# Transpose property: (AB)ᵀ = BᵀAᵀ\nleft_side = (A @ B).T\nright_side = B.T @ A.T\nprint(\"(AB)ᵀ == BᵀAᵀ:\", np.allclose(left_side, right_side))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Special Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and verify special matrices\nn = 4\n\n# Identity matrix\nI = np.eye(n)\nprint(\"Identity matrix:\")\nprint(I)\nprint(\"I² = I:\", np.array_equal(I @ I, I))\n\n# Diagonal matrix\nD = np.diag([1, 2, 3, 4])\nprint(\"\\nDiagonal matrix:\")\nprint(D)\nprint(\"D²:\")\nprint(D @ D)\n\n# Symmetric matrix\nS = np.random.randn(n, n)\nS = (S + S.T) / 2  # Make symmetric\nprint(\"\\nSymmetric matrix:\")\nprint(S)\nprint(\"S = Sᵀ:\", np.array_equal(S, S.T))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nIn this chapter, we've covered:\n\n1. **Matrix Fundamentals**: Definition, notation, and representation as linear transformations\n2. **Basic Operations**: Addition, scalar multiplication, and matrix multiplication with detailed properties\n3. **Special Matrices**: Identity, zero, diagonal, symmetric, skew-symmetric, and orthogonal matrices\n4. **Matrix Properties**: Transpose, trace, and their mathematical properties\n5. **Geometric Interpretation**: How matrices represent linear transformations\n6. **Visualization**: Heatmaps and transformation plots\n7. **AI/ML Applications**: Data representation, neural networks, and covariance matrices\n\n### Key Takeaways:\n- Matrices represent linear transformations between vector spaces\n- Matrix multiplication is not commutative but is associative and distributive\n- Special matrices have important properties and applications\n- Understanding matrix operations is crucial for linear algebra and machine learning\n- Matrices are fundamental for representing data and transformations in AI/ML\n\n### Next Steps:\nIn the next chapter, we'll explore linear transformations in detail, understanding how matrices transform vectors and spaces."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}