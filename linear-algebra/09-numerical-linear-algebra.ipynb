{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Numerical Linear Algebra\n\n[![Chapter](https://img.shields.io/badge/Chapter-9-blue.svg)]()\n[![Topic](https://img.shields.io/badge/Topic-Numerical_Linear_Algebra-green.svg)]()\n[![Difficulty](https://img.shields.io/badge/Difficulty-Advanced-red.svg)]()\n\n## Introduction\n\nNumerical linear algebra is the study of algorithms for performing linear algebra computations on computers with finite precision arithmetic. This field bridges the gap between theoretical linear algebra and practical computational methods, addressing issues of numerical stability, computational efficiency, and algorithmic complexity.\n\n**Mathematical Foundation:**\nNumerical linear algebra deals with the practical implementation of mathematical concepts:\n- **Finite Precision**: Computers represent real numbers with limited precision (typically 64 bits for double precision)\n- **Rounding Errors**: Accumulation of small errors in arithmetic operations\n- **Conditioning**: Sensitivity of problems to small perturbations in input data\n- **Stability**: Ability of algorithms to produce accurate results despite rounding errors\n\n**Key Challenges:**\n1. **Numerical Stability**: Ensuring algorithms don't amplify rounding errors\n2. **Computational Complexity**: Balancing accuracy with computational cost\n3. **Memory Efficiency**: Handling large-scale problems within memory constraints\n4. **Parallelization**: Exploiting modern hardware for performance\n\n**Geometric Interpretation:**\nNumerical issues can be understood geometrically:\n- **Ill-conditioned problems**: Small changes in input cause large changes in output\n- **Stable algorithms**: Preserve geometric relationships despite numerical errors\n- **Convergence**: Iterative methods approach solutions through geometric optimization\n\n**Applications in Modern Computing:**\n- **Machine Learning**: Large-scale matrix operations in neural networks\n- **Scientific Computing**: Solving partial differential equations\n- **Data Science**: Principal component analysis and dimensionality reduction\n- **Computer Graphics**: 3D transformations and rendering\n- **Signal Processing**: Filtering and spectral analysis\n\n## 1. Numerical Stability and Conditioning\n\n### Mathematical Foundation\n\n**Condition Number:**\nThe condition number measures how sensitive a problem is to perturbations in the input data. For a matrix A, the condition number is:\n\nκ(A) = ||A|| · ||A⁻¹||\n\nwhere ||·|| is a matrix norm (typically the 2-norm).\n\n**Error Analysis:**\nFor the linear system Ax = b, if we perturb b by δb, the solution changes by δx:\n\n(A + δA)(x + δx) = b + δb\n\nThe relative error bound is:\n||δx||/||x|| ≤ κ(A) · (||δA||/||A|| + ||δb||/||b||)\n\n**Backward Error Analysis:**\nInstead of asking \"how accurate is the computed solution?\", we ask \"for what perturbed problem is our computed solution exact?\"\n\n**Stability Definitions:**\n1. **Forward Stability**: Computed solution is close to exact solution\n2. **Backward Stability**: Computed solution is exact for slightly perturbed problem\n3. **Mixed Stability**: Combination of forward and backward stability\n\n### Condition Number Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg\nfrom scipy.sparse import csr_matrix, csc_matrix, lil_matrix\nfrom scipy.sparse.linalg import spsolve, eigsh\nimport seaborn as sns\n\ndef comprehensive_condition_analysis(A, name=\"Matrix\"):\n    \"\"\"\n    Comprehensive condition number analysis\n    \n    Mathematical approach:\n    κ(A) = ||A||₂ · ||A⁻¹||₂ = σ_max(A) / σ_min(A)\n    \n    Parameters:\n    A: numpy array - matrix to analyze\n    name: str - name for display\n    \n    Returns:\n    dict - comprehensive analysis results\n    \"\"\"\n    analysis = {}\n    \n    # Basic condition number\n    try:\n        cond_2 = np.linalg.cond(A, p=2)\n        cond_1 = np.linalg.cond(A, p=1)\n        cond_inf = np.linalg.cond(A, p=np.inf)\n        analysis['condition_numbers'] = {\n            '2_norm': cond_2,\n            '1_norm': cond_1,\n            'inf_norm': cond_inf\n        }\n    except np.linalg.LinAlgError:\n        analysis['condition_numbers'] = {'2_norm': np.inf, '1_norm': np.inf, 'inf_norm': np.inf}\n    \n    # SVD-based analysis\n    try:\n        U, S, Vt = np.linalg.svd(A, full_matrices=False)\n        analysis['svd'] = {\n            'singular_values': S,\n            'max_singular': S[0],\n            'min_singular': S[-1],\n            'condition_from_svd': S[0] / S[-1] if S[-1] > 0 else np.inf,\n            'rank': np.sum(S > 1e-10 * S[0])\n        }\n    except np.linalg.LinAlgError:\n        analysis['svd'] = None\n    \n    # Eigenvalue analysis (for square matrices)\n    if A.shape[0] == A.shape[1]:\n        try:\n            eigenvals = np.linalg.eigvals(A)\n            analysis['eigenvalues'] = {\n                'eigenvalues': eigenvals,\n                'max_magnitude': np.max(np.abs(eigenvals)),\n                'min_magnitude': np.min(np.abs(eigenvals)),\n                'condition_from_eigenvalues': np.max(np.abs(eigenvals)) / np.min(np.abs(eigenvals)) if np.min(np.abs(eigenvals)) > 0 else np.inf\n            }\n        except np.linalg.LinAlgError:\n            analysis['eigenvalues'] = None\n    \n    # Numerical rank\n    if analysis['svd']:\n        analysis['numerical_rank'] = analysis['svd']['rank']\n    \n    # Stability assessment\n    cond_2 = analysis['condition_numbers']['2_norm']\n    if cond_2 < 100:\n        stability = \"Excellent\"\n    elif cond_2 < 1000:\n        stability = \"Good\"\n    elif cond_2 < 1e6:\n        stability = \"Fair\"\n    elif cond_2 < 1e12:\n        stability = \"Poor\"\n    else:\n        stability = \"Very Poor\"\n    \n    analysis['stability_assessment'] = stability\n    \n    # Print results\n    print(f\"\\n=== {name} Condition Analysis ===\")\n    print(f\"Condition number (2-norm): {cond_2:.2e}\")\n    print(f\"Stability: {stability}\")\n    \n    if analysis['svd']:\n        print(f\"Singular values range: {S[0]:.2e} to {S[-1]:.2e}\")\n        print(f\"Numerical rank: {analysis['numerical_rank']}\")\n    \n    if analysis['eigenvalues']:\n        eigenvals = analysis['eigenvalues']['eigenvalues']\n        print(f\"Eigenvalues range: {np.min(np.abs(eigenvals)):.2e} to {np.max(np.abs(eigenvals)):.2e}\")\n    \n    return analysis\n\ndef perturbation_analysis(A, b, perturbation_sizes=np.logspace(-15, -1, 15)):\n    \"\"\"\n    Analyze sensitivity to perturbations\n    \n    Mathematical approach:\n    For Ax = b, perturb b by δb and solve A(x + δx) = b + δb\n    Measure ||δx||/||x|| vs ||δb||/||b||\n    \n    Parameters:\n    A: numpy array - coefficient matrix\n    b: numpy array - right-hand side\n    perturbation_sizes: array - sizes of perturbations to test\n    \n    Returns:\n    dict - perturbation analysis results\n    \"\"\"\n    # Exact solution\n    x_exact = np.linalg.solve(A, b)\n    \n    # Test different perturbation sizes\n    relative_errors = []\n    amplification_factors = []\n    \n    for eps in perturbation_sizes:\n        # Perturb b\n        b_perturbed = b + eps * np.random.randn(*b.shape)\n        \n        # Solve perturbed system\n        try:\n            x_perturbed = np.linalg.solve(A, b_perturbed)\n            \n            # Compute relative errors\n            rel_error_b = np.linalg.norm(b_perturbed - b) / np.linalg.norm(b)\n            rel_error_x = np.linalg.norm(x_perturbed - x_exact) / np.linalg.norm(x_exact)\n            \n            relative_errors.append(rel_error_x)\n            amplification_factors.append(rel_error_x / rel_error_b if rel_error_b > 0 else np.inf)\n            \n        except np.linalg.LinAlgError:\n            relative_errors.append(np.inf)\n            amplification_factors.append(np.inf)\n    \n    # Theoretical bound\n    cond_A = np.linalg.cond(A)\n    theoretical_bound = cond_A * perturbation_sizes\n    \n    analysis = {\n        'perturbation_sizes': perturbation_sizes,\n        'relative_errors': relative_errors,\n        'amplification_factors': amplification_factors,\n        'theoretical_bound': theoretical_bound,\n        'condition_number': cond_A,\n        'max_amplification': np.max(amplification_factors) if not np.any(np.isinf(amplification_factors)) else np.inf\n    }\n    \n    return analysis\n\ndef backward_error_analysis(A, b, x_computed):\n    \"\"\"\n    Perform backward error analysis\n    \n    Mathematical approach:\n    Find smallest ||δA|| and ||δb|| such that (A + δA)x_computed = b + δb\n    \n    Parameters:\n    A: numpy array - original coefficient matrix\n    b: numpy array - original right-hand side\n    x_computed: numpy array - computed solution\n    \n    Returns:\n    dict - backward error analysis\n    \"\"\"\n    # Compute residual\n    residual = b - A @ x_computed\n    \n    # Backward error for right-hand side\n    backward_error_b = np.linalg.norm(residual) / np.linalg.norm(b)\n    \n    # Backward error for matrix (simplified)\n    # In practice, this requires more sophisticated analysis\n    backward_error_A = backward_error_b  # Simplified approximation\n    \n    # Forward error bound\n    cond_A = np.linalg.cond(A)\n    forward_error_bound = cond_A * backward_error_b\n    \n    # Actual forward error\n    x_exact = np.linalg.solve(A, b)\n    actual_forward_error = np.linalg.norm(x_computed - x_exact) / np.linalg.norm(x_exact)\n    \n    analysis = {\n        'backward_error_b': backward_error_b,\n        'backward_error_A': backward_error_A,\n        'forward_error_bound': forward_error_bound,\n        'actual_forward_error': actual_forward_error,\n        'residual_norm': np.linalg.norm(residual),\n        'condition_number': cond_A\n    }\n    \n    return analysis\n\ndef demonstrate_numerical_instability():\n    \"\"\"\n    Comprehensive demonstration of numerical instability\n    \n    Mathematical examples:\n    1. Hilbert matrix: Classic ill-conditioned matrix\n    2. Vandermonde matrix: Polynomial interpolation matrix\n    3. Random matrices: Statistical analysis\n    4. Structured matrices: Toeplitz, circulant, etc.\n    \"\"\"\n    \n    # 1. Hilbert matrix (classic example)\n    def hilbert_matrix(n):\n        \"\"\"Create n×n Hilbert matrix H[i,j] = 1/(i+j+1)\"\"\"\n        H = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                H[i, j] = 1.0 / (i + j + 1)\n        return H\n    \n    # 2. Vandermonde matrix\n    def vandermonde_matrix(x, n):\n        \"\"\"Create Vandermonde matrix V[i,j] = x[i]^j\"\"\"\n        V = np.zeros((len(x), n))\n        for i in range(len(x)):\n            for j in range(n):\n                V[i, j] = x[i]**j\n        return V\n    \n    # 3. Toeplitz matrix\n    def toeplitz_matrix(c, r):\n        \"\"\"Create Toeplitz matrix with first column c and first row r\"\"\"\n        n = len(c)\n        T = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                if i >= j:\n                    T[i, j] = c[i - j]\n                else:\n                    T[i, j] = r[j - i]\n        return T\n    \n    # Test different matrix types\n    matrix_types = {\n        'Identity': np.eye(5),\n        'Random': np.random.randn(5, 5),\n        'Hilbert (5×5)': hilbert_matrix(5),\n        'Hilbert (10×10)': hilbert_matrix(10),\n        'Vandermonde': vandermonde_matrix(np.linspace(0, 1, 5), 5),\n        'Toeplitz': toeplitz_matrix([1, 0.5, 0.25, 0.125, 0.0625], [1, 0.5, 0.25, 0.125, 0.0625])\n    }\n    \n    # Analyze each matrix\n    analyses = {}\n    for name, A in matrix_types.items():\n        analyses[name] = comprehensive_condition_analysis(A, name)\n    \n    # Test perturbation sensitivity\n    print(f\"\\n=== Perturbation Sensitivity Analysis ===\")\n    \n    # Use Hilbert matrix as example\n    H = hilbert_matrix(10)\n    b = np.ones(10)\n    x_exact = np.linalg.solve(H, b)\n    \n    perturbation_analysis_result = perturbation_analysis(H, b)\n    \n    # Visualize results\n    plt.figure(figsize=(15, 10))\n    \n    # Plot 1: Condition numbers comparison\n    plt.subplot(2, 3, 1)\n    names = list(analyses.keys())\n    cond_numbers = [analyses[name]['condition_numbers']['2_norm'] for name in names]\n    \n    plt.bar(range(len(names)), cond_numbers)\n    plt.xticks(range(len(names)), names, rotation=45, ha='right')\n    plt.ylabel('Condition Number')\n    plt.title('Condition Numbers Comparison')\n    plt.yscale('log')\n    \n    # Plot 2: Perturbation analysis\n    plt.subplot(2, 3, 2)\n    eps = perturbation_analysis_result['perturbation_sizes']\n    rel_errors = perturbation_analysis_result['relative_errors']\n    theoretical = perturbation_analysis_result['theoretical_bound']\n    \n    plt.loglog(eps, rel_errors, 'bo-', label='Actual Error')\n    plt.loglog(eps, theoretical, 'r--', label='Theoretical Bound')\n    plt.xlabel('Perturbation Size')\n    plt.ylabel('Relative Error in Solution')\n    plt.title('Perturbation Sensitivity')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot 3: Amplification factors\n    plt.subplot(2, 3, 3)\n    amplification = perturbation_analysis_result['amplification_factors']\n    plt.semilogx(eps, amplification, 'go-')\n    plt.axhline(y=perturbation_analysis_result['condition_number'], color='r', linestyle='--', label='Condition Number')\n    plt.xlabel('Perturbation Size')\n    plt.ylabel('Error Amplification Factor')\n    plt.title('Error Amplification')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot 4: Singular values for Hilbert matrix\n    plt.subplot(2, 3, 4)\n    S = analyses['Hilbert (10×10)']['svd']['singular_values']\n    plt.semilogy(range(1, len(S)+1), S, 'mo-')\n    plt.xlabel('Index')\n    plt.ylabel('Singular Value')\n    plt.title('Hilbert Matrix Singular Values')\n    plt.grid(True)\n    \n    # Plot 5: Eigenvalue distribution for random matrix\n    plt.subplot(2, 3, 5)\n    eigenvals = analyses['Random']['eigenvalues']['eigenvalues']\n    plt.scatter(eigenvals.real, eigenvals.imag, alpha=0.6)\n    plt.xlabel('Real Part')\n    plt.ylabel('Imaginary Part')\n    plt.title('Random Matrix Eigenvalues')\n    plt.grid(True)\n    plt.axis('equal')\n    \n    # Plot 6: Stability assessment\n    plt.subplot(2, 3, 6)\n    stability_levels = ['Excellent', 'Good', 'Fair', 'Poor', 'Very Poor']\n    stability_counts = {level: 0 for level in stability_levels}\n    \n    for name, analysis in analyses.items():\n        stability = analysis['stability_assessment']\n        stability_counts[stability] += 1\n    \n    plt.bar(stability_levels, [stability_counts[level] for level in stability_levels])\n    plt.ylabel('Number of Matrices')\n    plt.title('Stability Assessment')\n    plt.xticks(rotation=45)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Test solving linear systems with different methods\n    print(f\"\\n=== Linear System Solution Comparison ===\")\n    \n    # Test with Hilbert matrix\n    H = hilbert_matrix(8)\n    b = np.ones(8)\n    x_true = np.linalg.solve(H, b)\n    \n    # Different solution methods\n    methods = {\n        'Direct LU': lambda A, b: np.linalg.solve(A, b),\n        'QR Decomposition': lambda A, b: np.linalg.solve(A.T @ A, A.T @ b),\n        'SVD Pseudo-inverse': lambda A, b: np.linalg.pinv(A) @ b,\n        'Tikhonov Regularization': lambda A, b: np.linalg.solve(A.T @ A + 1e-6 * np.eye(A.shape[1]), A.T @ b)\n    }\n    \n    results = {}\n    for method_name, method_func in methods.items():\n        try:\n            x_computed = method_func(H, b)\n            error = np.linalg.norm(x_computed - x_true) / np.linalg.norm(x_true)\n            backward_analysis = backward_error_analysis(H, b, x_computed)\n            \n            results[method_name] = {\n                'error': error,\n                'backward_error': backward_analysis['backward_error_b'],\n                'residual': backward_analysis['residual_norm']\n            }\n            \n            print(f\"{method_name}:\")\n            print(f\"  Relative error: {error:.2e}\")\n            print(f\"  Backward error: {backward_analysis['backward_error_b']:.2e}\")\n            print(f\"  Residual norm: {backward_analysis['residual_norm']:.2e}\")\n            \n        except Exception as e:\n            print(f\"{method_name}: Failed - {e}\")\n    \n    return analyses, perturbation_analysis_result, results\n\n# Run comprehensive numerical stability demonstration\nprint(\"=== Comprehensive Numerical Stability Analysis ===\")\nanalyses, perturbation_result, solution_results = demonstrate_numerical_instability()\n\n# Additional analysis: Growth of condition number with matrix size\nprint(f\"\\n=== Condition Number Growth Analysis ===\")\n\nsizes = [5, 10, 15, 20, 25]\nhilbert_conditions = []\nvandermonde_conditions = []\n\nfor n in sizes:\n    # Hilbert matrix\n    H = np.array([[1.0/(i+j+1) for j in range(n)] for i in range(n)])\n    hilbert_conditions.append(np.linalg.cond(H))\n    \n    # Vandermonde matrix\n    x = np.linspace(0, 1, n)\n    V = np.array([[x[i]**j for j in range(n)] for i in range(n)])\n    vandermonde_conditions.append(np.linalg.cond(V))\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.semilogy(sizes, hilbert_conditions, 'bo-', label='Hilbert Matrix')\nplt.xlabel('Matrix Size')\nplt.ylabel('Condition Number')\nplt.title('Condition Number Growth: Hilbert Matrix')\nplt.grid(True)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.semilogy(sizes, vandermonde_conditions, 'ro-', label='Vandermonde Matrix')\nplt.xlabel('Matrix Size')\nplt.ylabel('Condition Number')\nplt.title('Condition Number Growth: Vandermonde Matrix')\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Hilbert matrix condition numbers: {hilbert_conditions}\")\nprint(f\"Vandermonde matrix condition numbers: {vandermonde_conditions}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numerical Stability Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def advanced_numerical_stability_examples():\n    \"\"\"\n    Advanced examples of numerical stability issues and solutions\n    \n    Mathematical concepts:\n    1. Catastrophic cancellation\n    2. Loss of significance\n    3. Algorithm stability\n    4. Preconditioning\n    \"\"\"\n    \n    # 1. Catastrophic cancellation example\n    print(\"=== Catastrophic Cancellation Example ===\")\n    \n    def quadratic_formula_unstable(a, b, c):\n        \"\"\"Unstable quadratic formula implementation\"\"\"\n        x1 = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\n        x2 = (-b - np.sqrt(b**2 - 4*a*c)) / (2*a)\n        return x1, x2\n    \n    def quadratic_formula_stable(a, b, c):\n        \"\"\"Stable quadratic formula implementation\"\"\"\n        # Use Vieta's formula to avoid cancellation\n        if b >= 0:\n            x1 = (-b - np.sqrt(b**2 - 4*a*c)) / (2*a)\n            x2 = c / (a * x1)\n        else:\n            x1 = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\n            x2 = c / (a * x1)\n        return x1, x2\n    \n    # Test with nearly equal roots\n    a, b, c = 1, -1000.001, 1000\n    \n    x1_unstable, x2_unstable = quadratic_formula_unstable(a, b, c)\n    x1_stable, x2_stable = quadratic_formula_stable(a, b, c)\n    \n    print(f\"Unstable: x1 = {x1_unstable:.10f}, x2 = {x2_unstable:.10f}\")\n    print(f\"Stable: x1 = {x1_stable:.10f}, x2 = {x2_stable:.10f}\")\n    print(f\"Product (should be c/a = {c/a}):\")\n    print(f\"  Unstable: {x1_unstable * x2_unstable:.10f}\")\n    print(f\"  Stable: {x1_stable * x2_stable:.10f}\")\n    \n    # 2. Loss of significance in matrix operations\n    print(f\"\\n=== Loss of Significance in Matrix Operations ===\")\n    \n    def matrix_conditioning_example():\n        \"\"\"Demonstrate matrix conditioning and its effects\"\"\"\n        \n        # Create a poorly conditioned matrix\n        n = 5\n        A = np.random.randn(n, n)\n        # Make it nearly singular\n        A[:, -1] = A[:, 0] + 1e-10 * np.random.randn(n)\n        \n        # Create right-hand side\n        b = np.ones(n)\n        \n        # Solve with different methods\n        methods = {\n            'Direct': lambda: np.linalg.solve(A, b),\n            'QR': lambda: np.linalg.solve(A.T @ A, A.T @ b),\n            'SVD': lambda: np.linalg.pinv(A) @ b,\n            'Regularized': lambda: np.linalg.solve(A.T @ A + 1e-8 * np.eye(n), A.T @ b)\n        }\n        \n        results = {}\n        for name, method in methods.items():\n            try:\n                x = method()\n                residual = np.linalg.norm(A @ x - b)\n                results[name] = {'x': x, 'residual': residual}\n                print(f\"{name}: residual = {residual:.2e}\")\n            except Exception as e:\n                print(f\"{name}: Failed - {e}\")\n        \n        return results\n    \n    matrix_results = matrix_conditioning_example()\n    \n    # 3. Algorithm stability comparison\n    print(f\"\\n=== Algorithm Stability Comparison ===\")\n    \n    def compare_linear_system_solvers():\n        \"\"\"Compare different linear system solvers for stability\"\"\"\n        \n        # Create test problems\n        problems = {\n            'Well-conditioned': np.random.randn(10, 10),\n            'Ill-conditioned': np.array([[1, 1], [1, 1.0001]]),\n            'Singular': np.array([[1, 1], [1, 1]])\n        }\n        \n        solvers = {\n            'LU': lambda A, b: np.linalg.solve(A, b),\n            'QR': lambda A, b: np.linalg.solve(A.T @ A, A.T @ b),\n            'SVD': lambda A, b: np.linalg.pinv(A) @ b,\n            'Tikhonov': lambda A, b: np.linalg.solve(A.T @ A + 1e-6 * np.eye(A.shape[1]), A.T @ b)\n        }\n        \n        for problem_name, A in problems.items():\n            print(f\"\\n{problem_name} problem:\")\n            print(f\"Condition number: {np.linalg.cond(A):.2e}\")\n            \n            b = np.ones(A.shape[0])\n            \n            for solver_name, solver in solvers.items():\n                try:\n                    x = solver(A, b)\n                    residual = np.linalg.norm(A @ x - b)\n                    print(f\"  {solver_name}: residual = {residual:.2e}\")\n                except Exception as e:\n                    print(f\"  {solver_name}: Failed - {e}\")\n    \n    compare_linear_system_solvers()\n    \n    # 4. Preconditioning example\n    print(f\"\\n=== Preconditioning Example ===\")\n    \n    def preconditioning_demo():\n        \"\"\"Demonstrate the effect of preconditioning\"\"\"\n        \n        # Create a poorly conditioned system\n        n = 100\n        A = np.random.randn(n, n)\n        A = A.T @ A + 1e-6 * np.eye(n)  # Make it symmetric positive definite but ill-conditioned\n        \n        b = np.random.randn(n)\n        \n        # Without preconditioning\n        x_direct = np.linalg.solve(A, b)\n        \n        # With diagonal preconditioning\n        D = np.diag(np.sqrt(np.diag(A)))\n        D_inv = np.diag(1.0 / np.sqrt(np.diag(A)))\n        \n        A_precond = D_inv @ A @ D_inv\n        b_precond = D_inv @ b\n        \n        x_precond_raw = np.linalg.solve(A_precond, b_precond)\n        x_precond = D_inv @ x_precond_raw\n        \n        # Compare residuals\n        residual_direct = np.linalg.norm(A @ x_direct - b)\n        residual_precond = np.linalg.norm(A @ x_precond - b)\n        \n        print(f\"Condition number (original): {np.linalg.cond(A):.2e}\")\n        print(f\"Condition number (preconditioned): {np.linalg.cond(A_precond):.2e}\")\n        print(f\"Residual (direct): {residual_direct:.2e}\")\n        print(f\"Residual (preconditioned): {residual_precond:.2e}\")\n        \n        return A, A_precond, x_direct, x_precond\n    \n    A_orig, A_precond, x_direct, x_precond = preconditioning_demo()\n    \n    # Visualize the effect of preconditioning\n    plt.figure(figsize=(12, 4))\n    \n    # Original matrix spectrum\n    eigenvals_orig = np.linalg.eigvals(A_orig)\n    plt.subplot(1, 3, 1)\n    plt.hist(eigenvals_orig, bins=20, alpha=0.7, label='Original')\n    plt.xlabel('Eigenvalue')\n    plt.ylabel('Frequency')\n    plt.title('Original Matrix Spectrum')\n    plt.legend()\n    \n    # Preconditioned matrix spectrum\n    eigenvals_precond = np.linalg.eigvals(A_precond)\n    plt.subplot(1, 3, 2)\n    plt.hist(eigenvals_precond, bins=20, alpha=0.7, label='Preconditioned', color='orange')\n    plt.xlabel('Eigenvalue')\n    plt.ylabel('Frequency')\n    plt.title('Preconditioned Matrix Spectrum')\n    plt.legend()\n    \n    # Condition number comparison\n    plt.subplot(1, 3, 3)\n    cond_orig = np.linalg.cond(A_orig)\n    cond_precond = np.linalg.cond(A_precond)\n    plt.bar(['Original', 'Preconditioned'], [cond_orig, cond_precond])\n    plt.ylabel('Condition Number')\n    plt.title('Condition Number Comparison')\n    plt.yscale('log')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Run advanced numerical stability examples\nadvanced_numerical_stability_examples()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Iterative Methods for Linear Systems\n\n### Jacobi Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jacobi_iteration(A, b, x0=None, max_iter=1000, tol=1e-6):\n    \"\"\"Solve Ax = b using Jacobi iteration\"\"\"\n    n = len(b)\n    if x0 is None:\n        x0 = np.zeros(n)\n    \n    x = x0.copy()\n    \n    # Extract diagonal and off-diagonal parts\n    D = np.diag(np.diag(A))\n    L_plus_U = A - D\n    \n    for iteration in range(max_iter):\n        x_new = np.linalg.solve(D, b - L_plus_U @ x)\n        \n        # Check convergence\n        if np.linalg.norm(x_new - x) < tol:\n            print(f\"Converged after {iteration + 1} iterations\")\n            break\n        \n        x = x_new\n    \n    return x\n\n# Test Jacobi method\nA = np.array([[4, -1, 0], [-1, 4, -1], [0, -1, 4]])\nb = np.array([1, 5, 0])\n\nx_jacobi = jacobi_iteration(A, b)\nx_exact = np.linalg.solve(A, b)\n\nprint(\"Jacobi solution:\", x_jacobi)\nprint(\"Exact solution:\", x_exact)\nprint(\"Error:\", np.linalg.norm(x_jacobi - x_exact))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gauss-Seidel Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gauss_seidel_iteration(A, b, x0=None, max_iter=1000, tol=1e-6):\n    \"\"\"Solve Ax = b using Gauss-Seidel iteration\"\"\"\n    n = len(b)\n    if x0 is None:\n        x0 = np.zeros(n)\n    \n    x = x0.copy()\n    \n    for iteration in range(max_iter):\n        x_old = x.copy()\n        \n        for i in range(n):\n            # Forward substitution\n            sum_val = 0\n            for j in range(n):\n                if i != j:\n                    sum_val += A[i, j] * x[j]\n            x[i] = (b[i] - sum_val) / A[i, i]\n        \n        # Check convergence\n        if np.linalg.norm(x - x_old) < tol:\n            print(f\"Converged after {iteration + 1} iterations\")\n            break\n    \n    return x\n\n# Test Gauss-Seidel\nx_gauss_seidel = gauss_seidel_iteration(A, b)\nprint(\"Gauss-Seidel solution:\", x_gauss_seidel)\nprint(\"Error:\", np.linalg.norm(x_gauss_seidel - x_exact))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conjugate Gradient Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conjugate_gradient(A, b, x0=None, max_iter=1000, tol=1e-6):\n    \"\"\"Solve Ax = b using Conjugate Gradient method (for symmetric positive definite A)\"\"\"\n    n = len(b)\n    if x0 is None:\n        x0 = np.zeros(n)\n    \n    x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    \n    for iteration in range(max_iter):\n        Ap = A @ p\n        alpha = np.dot(r, r) / np.dot(p, Ap)\n        x = x + alpha * p\n        r_new = r - alpha * Ap\n        \n        # Check convergence\n        if np.linalg.norm(r_new) < tol:\n            print(f\"Converged after {iteration + 1} iterations\")\n            break\n        \n        beta = np.dot(r_new, r_new) / np.dot(r, r)\n        p = r_new + beta * p\n        r = r_new\n    \n    return x\n\n# Test Conjugate Gradient\nx_cg = conjugate_gradient(A, b)\nprint(\"Conjugate Gradient solution:\", x_cg)\nprint(\"Error:\", np.linalg.norm(x_cg - x_exact))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sparse Matrix Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix\nfrom scipy.sparse.linalg import spsolve, eigsh\n\ndef sparse_matrix_example():\n    \"\"\"Demonstrate sparse matrix operations\"\"\"\n    \n    # Create a sparse matrix (tridiagonal)\n    n = 1000\n    diagonals = [np.ones(n), -0.5 * np.ones(n-1), -0.5 * np.ones(n-1)]\n    A_dense = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[1], -1)\n    \n    # Convert to sparse format\n    A_sparse = csr_matrix(A_dense)\n    \n    print(f\"Dense matrix size: {A_dense.nbytes / 1024:.1f} KB\")\n    print(f\"Sparse matrix size: {A_sparse.data.nbytes / 1024:.1f} KB\")\n    print(f\"Compression ratio: {A_dense.nbytes / A_sparse.data.nbytes:.1f}x\")\n    \n    # Solve linear system\n    b = np.ones(n)\n    \n    # Dense solve\n    import time\n    start_time = time.time()\n    x_dense = np.linalg.solve(A_dense, b)\n    dense_time = time.time() - start_time\n    \n    # Sparse solve\n    start_time = time.time()\n    x_sparse = spsolve(A_sparse, b)\n    sparse_time = time.time() - start_time\n    \n    print(f\"\\nDense solve time: {dense_time:.4f} seconds\")\n    print(f\"Sparse solve time: {sparse_time:.4f} seconds\")\n    print(f\"Speedup: {dense_time / sparse_time:.1f}x\")\n    \n    # Check accuracy\n    error = np.linalg.norm(x_dense - x_sparse)\n    print(f\"Solution error: {error:.2e}\")\n    \n    return A_sparse, x_sparse\n\nA_sparse, x_sparse = sparse_matrix_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Eigenvalue Problems\n\n### Power Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def power_iteration(A, max_iter=1000, tol=1e-6):\n    \"\"\"Find the largest eigenvalue and eigenvector using power iteration\"\"\"\n    n = A.shape[0]\n    v = np.random.randn(n)\n    v = v / np.linalg.norm(v)\n    \n    for iteration in range(max_iter):\n        v_new = A @ v\n        v_new = v_new / np.linalg.norm(v_new)\n        \n        # Estimate eigenvalue\n        eigenvalue = np.dot(v_new, A @ v_new)\n        \n        # Check convergence\n        if np.linalg.norm(v_new - v) < tol:\n            print(f\"Converged after {iteration + 1} iterations\")\n            break\n        \n        v = v_new\n    \n    return eigenvalue, v\n\n# Test power iteration\nA = np.random.randn(5, 5)\nA = A + A.T  # Make symmetric\neigenvalue, eigenvector = power_iteration(A)\n\n# Compare with numpy\neigenvals_np, eigenvecs_np = np.linalg.eig(A)\nmax_eigenval_idx = np.argmax(np.abs(eigenvals_np))\n\nprint(f\"Power iteration eigenvalue: {eigenvalue:.6f}\")\nprint(f\"Numpy largest eigenvalue: {eigenvals_np[max_eigenval_idx]:.6f}\")\nprint(f\"Eigenvalue error: {abs(eigenvalue - eigenvals_np[max_eigenval_idx]):.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inverse Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inverse_iteration(A, sigma, max_iter=1000, tol=1e-6):\n    \"\"\"Find eigenvalue closest to sigma using inverse iteration\"\"\"\n    n = A.shape[0]\n    v = np.random.randn(n)\n    v = v / np.linalg.norm(v)\n    \n    # Shift matrix\n    A_shifted = A - sigma * np.eye(n)\n    \n    for iteration in range(max_iter):\n        # Solve linear system\n        v_new = np.linalg.solve(A_shifted, v)\n        v_new = v_new / np.linalg.norm(v_new)\n        \n        # Estimate eigenvalue\n        eigenvalue = np.dot(v_new, A @ v_new)\n        \n        # Check convergence\n        if np.linalg.norm(v_new - v) < tol:\n            print(f\"Converged after {iteration + 1} iterations\")\n            break\n        \n        v = v_new\n    \n    return eigenvalue, v\n\n# Test inverse iteration\nsigma = 2.0  # Target eigenvalue\neigenvalue_inv, eigenvector_inv = inverse_iteration(A, sigma)\nprint(f\"Inverse iteration eigenvalue: {eigenvalue_inv:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. QR Algorithm for Eigenvalues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def qr_algorithm(A, max_iter=100, tol=1e-6):\n    \"\"\"Find all eigenvalues using QR algorithm\"\"\"\n    n = A.shape[0]\n    A_k = A.copy()\n    \n    for iteration in range(max_iter):\n        # QR decomposition\n        Q, R = np.linalg.qr(A_k)\n        \n        # Update A\n        A_new = R @ Q\n        \n        # Check convergence (off-diagonal elements)\n        off_diag_norm = np.linalg.norm(A_new - np.diag(np.diag(A_new)))\n        \n        if off_diag_norm < tol:\n            print(f\"Converged after {iteration + 1} iterations\")\n            break\n        \n        A_k = A_new\n    \n    eigenvalues = np.diag(A_k)\n    return eigenvalues\n\n# Test QR algorithm\neigenvalues_qr = qr_algorithm(A)\neigenvalues_np = np.linalg.eigvals(A)\n\nprint(\"QR algorithm eigenvalues:\", eigenvalues_qr)\nprint(\"Numpy eigenvalues:\", eigenvalues_np)\nprint(\"Maximum error:\", np.max(np.abs(eigenvalues_qr - eigenvalues_np)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Singular Value Decomposition (SVD) for Large Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def truncated_svd(A, k):\n    \"\"\"Compute truncated SVD for large matrices\"\"\"\n    from scipy.sparse.linalg import svds\n    \n    if A.shape[0] * A.shape[1] > 1e6:  # Use sparse SVD for large matrices\n        U, S, Vt = svds(A, k=k)\n    else:\n        U, S, Vt = np.linalg.svd(A, full_matrices=False)\n        U = U[:, :k]\n        S = S[:k]\n        Vt = Vt[:k, :]\n    \n    return U, S, Vt\n\n# Example: Image compression using truncated SVD\ndef compress_image_svd(image, k):\n    \"\"\"Compress image using truncated SVD\"\"\"\n    U, S, Vt = truncated_svd(image, k)\n    compressed = U @ np.diag(S) @ Vt\n    \n    # Calculate compression ratio\n    original_size = image.size\n    compressed_size = U.size + S.size + Vt.size\n    compression_ratio = original_size / compressed_size\n    \n    return compressed, compression_ratio\n\n# Test with a simple image\nimage = np.random.rand(100, 100)\ncompressed_10, ratio_10 = compress_image_svd(image, 10)\ncompressed_50, ratio_50 = compress_image_svd(image, 50)\n\nprint(f\"Compression ratio (k=10): {ratio_10:.1f}x\")\nprint(f\"Compression ratio (k=50): {ratio_50:.1f}x\")\nprint(f\"Error (k=10): {np.linalg.norm(image - compressed_10):.4f}\")\nprint(f\"Error (k=50): {np.linalg.norm(image - compressed_50):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def performance_comparison():\n    \"\"\"Compare performance of different linear algebra operations\"\"\"\n    import time\n    \n    sizes = [100, 500, 1000, 2000]\n    times = {'dense_solve': [], 'sparse_solve': [], 'eigenvalues': [], 'svd': []}\n    \n    for n in sizes:\n        print(f\"\\nTesting size {n}x{n}\")\n        \n        # Generate test matrix\n        A = np.random.randn(n, n)\n        A = A + A.T  # Make symmetric\n        b = np.random.randn(n)\n        \n        # Dense solve\n        start_time = time.time()\n        x = np.linalg.solve(A, b)\n        times['dense_solve'].append(time.time() - start_time)\n        \n        # Sparse solve\n        A_sparse = csr_matrix(A)\n        start_time = time.time()\n        x_sparse = spsolve(A_sparse, b)\n        times['sparse_solve'].append(time.time() - start_time)\n        \n        # Eigenvalues\n        start_time = time.time()\n        eigenvals = np.linalg.eigvals(A)\n        times['eigenvalues'].append(time.time() - start_time)\n        \n        # SVD\n        start_time = time.time()\n        U, S, Vt = np.linalg.svd(A, full_matrices=False)\n        times['svd'].append(time.time() - start_time)\n    \n    # Plot results\n    plt.figure(figsize=(12, 8))\n    \n    for i, (operation, time_list) in enumerate(times.items()):\n        plt.subplot(2, 2, i+1)\n        plt.loglog(sizes, time_list, 'bo-', label=operation)\n        plt.xlabel('Matrix Size')\n        plt.ylabel('Time (seconds)')\n        plt.title(f'{operation.replace(\"_\", \" \").title()}')\n        plt.grid(True)\n        plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return times\n\ntimes = performance_comparison()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n1. **Condition Number**: Compute condition numbers for different matrix types and analyze their sensitivity.\n2. **Iterative Methods**: Implement and compare Jacobi, Gauss-Seidel, and Conjugate Gradient methods.\n3. **Sparse Matrices**: Create a sparse matrix and compare dense vs sparse operations.\n4. **Eigenvalue Methods**: Implement power iteration and inverse iteration for finding eigenvalues.\n5. **Performance Analysis**: Profile different linear algebra operations for various matrix sizes.\n\n## Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Condition Number Analysis\ndef analyze_condition_numbers():\n    matrices = {\n        'Identity': np.eye(5),\n        'Random': np.random.randn(5, 5),\n        'Hilbert': linalg.hilbert(5),\n        'Vandermonde': np.vander(np.arange(1, 6))\n    }\n    \n    for name, A in matrices.items():\n        cond = condition_number(A)\n        print(f\"{name} matrix condition number: {cond:.2e}\")\n\n# Exercise 2: Iterative Methods Comparison\ndef compare_iterative_methods(A, b):\n    x_exact = np.linalg.solve(A, b)\n    \n    x_jacobi = jacobi_iteration(A, b)\n    x_gauss_seidel = gauss_seidel_iteration(A, b)\n    x_cg = conjugate_gradient(A, b)\n    \n    errors = {\n        'Jacobi': np.linalg.norm(x_jacobi - x_exact),\n        'Gauss-Seidel': np.linalg.norm(x_gauss_seidel - x_exact),\n        'Conjugate Gradient': np.linalg.norm(x_cg - x_exact)\n    }\n    \n    for method, error in errors.items():\n        print(f\"{method} error: {error:.2e}\")\n\n# Exercise 3: Sparse vs Dense\ndef sparse_vs_dense_comparison(n=1000):\n    # Create tridiagonal matrix\n    diagonals = [np.ones(n), -0.5 * np.ones(n-1), -0.5 * np.ones(n-1)]\n    A_dense = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[1], -1)\n    A_sparse = csr_matrix(A_dense)\n    b = np.ones(n)\n    \n    # Compare solve times\n    import time\n    \n    start_time = time.time()\n    x_dense = np.linalg.solve(A_dense, b)\n    dense_time = time.time() - start_time\n    \n    start_time = time.time()\n    x_sparse = spsolve(A_sparse, b)\n    sparse_time = time.time() - start_time\n    \n    print(f\"Dense solve time: {dense_time:.4f}s\")\n    print(f\"Sparse solve time: {sparse_time:.4f}s\")\n    print(f\"Speedup: {dense_time / sparse_time:.1f}x\")\n\n# Exercise 4: Eigenvalue Methods\ndef eigenvalue_methods_comparison(A):\n    # Power iteration\n    eigenval_power, eigenvec_power = power_iteration(A)\n    \n    # Inverse iteration (target largest eigenvalue)\n    eigenvals_np = np.linalg.eigvals(A)\n    target = np.max(np.abs(eigenvals_np))\n    eigenval_inv, eigenvec_inv = inverse_iteration(A, target)\n    \n    # QR algorithm\n    eigenvals_qr = qr_algorithm(A)\n    \n    print(f\"Power iteration: {eigenval_power:.6f}\")\n    print(f\"Inverse iteration: {eigenval_inv:.6f}\")\n    print(f\"QR algorithm (max): {np.max(eigenvals_qr):.6f}\")\n    print(f\"Numpy (max): {np.max(eigenvals_np):.6f}\")\n\n# Exercise 5: Performance Analysis\ndef performance_analysis():\n    sizes = [100, 500, 1000]\n    operations = ['solve', 'eigenvalues', 'svd', 'inverse']\n    \n    for size in sizes:\n        print(f\"\\nMatrix size: {size}x{size}\")\n        A = np.random.randn(size, size)\n        \n        for operation in operations:\n            start_time = time.time()\n            if operation == 'solve':\n                b = np.random.randn(size)\n                x = np.linalg.solve(A, b)\n            elif operation == 'eigenvalues':\n                eigenvals = np.linalg.eigvals(A)\n            elif operation == 'svd':\n                U, S, Vt = np.linalg.svd(A, full_matrices=False)\n            elif operation == 'inverse':\n                A_inv = np.linalg.inv(A)\n            \n            elapsed_time = time.time() - start_time\n            print(f\"  {operation}: {elapsed_time:.4f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n- Numerical stability is crucial for reliable computations\n- Condition number measures problem sensitivity to perturbations\n- Iterative methods are efficient for large, sparse systems\n- Sparse matrix formats save memory and computation time\n- Different eigenvalue algorithms have different strengths\n- Performance varies significantly with matrix size and structure\n\n## Next Chapter\n\nIn the final chapter, we'll provide a comprehensive summary of all concepts covered and present practice problems to reinforce understanding."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}